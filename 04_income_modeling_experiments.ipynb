{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6932f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 04_MODELING_EXPERIMENTS.IPYNB\n",
    "# L&T Finance Pearl Challenge - Farmer Income Prediction\n",
    "# Objective: Achieve MAPE < 18% with focus on income range balance\n",
    "# =============================================================================\n",
    "\n",
    "# CHUNK 1: INITIAL SETUP & IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"🚀 L&T Finance Pearl Challenge - Modeling Experiments\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"📅 Execution started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"🎯 Objective: Achieve MAPE < 18% with income range optimization\")\n",
    "print(f\"🎲 Random State: {RANDOM_STATE}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Core ML Libraries\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score, StratifiedKFold, KFold, \n",
    "    GridSearchCV, RandomizedSearchCV\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_percentage_error, mean_absolute_error, \n",
    "    mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"✅ Core ML libraries imported successfully\")\n",
    "\n",
    "# Advanced ML Libraries  \n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    print(\"✅ LightGBM imported successfully\")\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"❌ LightGBM not available\")\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"✅ XGBoost imported successfully\") \n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"❌ XGBoost not available\")\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    print(\"✅ CatBoost imported successfully\")\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"❌ CatBoost not available\")\n",
    "    CATBOOST_AVAILABLE = False\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "try:\n",
    "    import optuna\n",
    "    print(\"✅ Optuna imported successfully\")\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    # Suppress optuna logs for cleaner output\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "except ImportError:\n",
    "    print(\"❌ Optuna not available - using GridSearchCV fallback\")\n",
    "    OPTUNA_AVAILABLE = False\n",
    "\n",
    "print(\"\\n📋 Library Availability Summary:\")\n",
    "print(f\"   🔹 LightGBM: {'✅' if LIGHTGBM_AVAILABLE else '❌'}\")\n",
    "print(f\"   🔹 XGBoost: {'✅' if XGBOOST_AVAILABLE else '❌'}\")\n",
    "print(f\"   🔹 CatBoost: {'✅' if CATBOOST_AVAILABLE else '❌'}\")\n",
    "print(f\"   🔹 Optuna: {'✅' if OPTUNA_AVAILABLE else '❌'}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b78f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 2: DIRECTORY SETUP & DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "# Set up project directories\n",
    "BASE_DIR = Path(\"../\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "ENGINEERED_DIR = DATA_DIR / \"feature_engineered\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"📁 Directory Structure:\")\n",
    "print(f\"   🔹 Base Directory: {BASE_DIR}\")\n",
    "print(f\"   🔹 Data Directory: {ENGINEERED_DIR}\")\n",
    "print(f\"   🔹 Models Directory: {MODELS_DIR}\")\n",
    "print(f\"   🔹 Results Directory: {RESULTS_DIR}\")\n",
    "\n",
    "# Verify required files exist\n",
    "required_files = [\n",
    "    \"X_train_eng.npy\", \"X_val_eng.npy\", \"X_test_eng.npy\",\n",
    "    \"y_train_eng.npy\", \"y_val_eng.npy\", \"feature_metadata.json\"\n",
    "]\n",
    "\n",
    "print(\"\\n🔍 Checking required files:\")\n",
    "missing_files = []\n",
    "for file in required_files:\n",
    "    file_path = ENGINEERED_DIR / file\n",
    "    if file_path.exists():\n",
    "        print(f\"   ✅ {file}\")\n",
    "    else:\n",
    "        print(f\"   ❌ {file} - MISSING!\")\n",
    "        missing_files.append(file)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n❌ ERROR: Missing files: {missing_files}\")\n",
    "    print(\"Please run preprocessing notebooks first!\")\n",
    "    raise FileNotFoundError(\"Required preprocessed files not found\")\n",
    "\n",
    "print(\"\\n📊 Loading preprocessed datasets...\")\n",
    "\n",
    "# Load training and validation data\n",
    "print(\"   🔄 Loading X_train...\")\n",
    "X_train = np.load(ENGINEERED_DIR / \"X_train_eng.npy\").astype(np.float32)\n",
    "print(\"   🔄 Loading X_val...\")\n",
    "X_val = np.load(ENGINEERED_DIR / \"X_val_eng.npy\").astype(np.float32)\n",
    "print(\"   🔄 Loading X_test...\")\n",
    "X_test = np.load(ENGINEERED_DIR / \"X_test_eng.npy\").astype(np.float32)\n",
    "print(\"   🔄 Loading y_train...\")\n",
    "y_train = np.load(ENGINEERED_DIR / \"y_train_eng.npy\").astype(np.float32)\n",
    "print(\"   🔄 Loading y_val...\")\n",
    "y_val = np.load(ENGINEERED_DIR / \"y_val_eng.npy\").astype(np.float32)\n",
    "\n",
    "# Load feature metadata\n",
    "print(\"   🔄 Loading feature metadata...\")\n",
    "with open(ENGINEERED_DIR / \"feature_metadata.json\", 'r') as f:\n",
    "    feature_metadata = json.load(f)\n",
    "\n",
    "feature_names = feature_metadata['final_feature_names']\n",
    "\n",
    "# Memory usage check\n",
    "def get_memory_usage():\n",
    "    \"\"\"Calculate memory usage of loaded arrays\"\"\"\n",
    "    arrays = [X_train, X_val, X_test, y_train, y_val]\n",
    "    total_memory = sum(arr.nbytes for arr in arrays) / (1024**2)  # MB\n",
    "    return total_memory\n",
    "\n",
    "memory_mb = get_memory_usage()\n",
    "\n",
    "print(f\"\\n✅ Data loading completed successfully!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"📊 DATASET SUMMARY:\")\n",
    "print(f\"   🔹 X_train shape: {X_train.shape}\")\n",
    "print(f\"   🔹 X_val shape: {X_val.shape}\")\n",
    "print(f\"   🔹 X_test shape: {X_test.shape}\")\n",
    "print(f\"   🔹 y_train shape: {y_train.shape}\")\n",
    "print(f\"   🔹 y_val shape: {y_val.shape}\")\n",
    "print(f\"   🔹 Feature count: {len(feature_names)}\")\n",
    "print(f\"   🔹 Total memory usage: {memory_mb:.1f} MB\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff89b04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 3: MANDATORY LOG TRANSFORMATION & INCOME RANGE SETUP\n",
    "# =============================================================================\n",
    "\n",
    "print(\"🎯 CRITICAL: Applying mandatory log1p transformation\")\n",
    "print(\"   📈 Problem: 2-5L income range shows 41.43% MAPE (vs target <18%)\")\n",
    "print(\"   💡 Solution: Log transformation + range-specific modeling\")\n",
    "\n",
    "# MANDATORY: Apply log1p transformation to target variables\n",
    "print(\"\\n🔄 Applying log1p transformation...\")\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log = np.log1p(y_val)\n",
    "\n",
    "print(f\"   ✅ y_train_log: {y_train_log.shape} | Range: [{y_train_log.min():.3f}, {y_train_log.max():.3f}]\")\n",
    "print(f\"   ✅ y_val_log: {y_val_log.shape} | Range: [{y_val_log.min():.3f}, {y_val_log.max():.3f}]\")\n",
    "\n",
    "# Create income range masks for specialized analysis\n",
    "print(\"\\n🎯 Creating income range masks...\")\n",
    "income_ranges = {\n",
    "    'low': (y_train >= 200000) & (y_train < 500000),    # 2-5 Lakhs\n",
    "    'mid': (y_train >= 500000) & (y_train < 1000000),   # 5-10 Lakhs  \n",
    "    'high': y_train >= 1000000                          # 10+ Lakhs\n",
    "}\n",
    "\n",
    "income_ranges_val = {\n",
    "    'low': (y_val >= 200000) & (y_val < 500000),\n",
    "    'mid': (y_val >= 500000) & (y_val < 1000000),\n",
    "    'high': y_val >= 1000000\n",
    "}\n",
    "\n",
    "# Analyze income distribution\n",
    "print(\"\\n📊 INCOME RANGE ANALYSIS:\")\n",
    "print(\"Training Set:\")\n",
    "for range_name, mask in income_ranges.items():\n",
    "    count = mask.sum()\n",
    "    percentage = (count / len(y_train)) * 100\n",
    "    avg_income = y_train[mask].mean() if count > 0 else 0\n",
    "    print(f\"   🔹 {range_name.upper():>4} (2-5L/5-10L/10L+): {count:>6,} samples ({percentage:>5.1f}%) | Avg: ₹{avg_income:>8,.0f}\")\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "for range_name, mask in income_ranges_val.items():\n",
    "    count = mask.sum()\n",
    "    percentage = (count / len(y_val)) * 100\n",
    "    avg_income = y_val[mask].mean() if count > 0 else 0\n",
    "    print(f\"   🔹 {range_name.upper():>4} (2-5L/5-10L/10L+): {count:>6,} samples ({percentage:>5.1f}%) | Avg: ₹{avg_income:>8,.0f}\")\n",
    "\n",
    "# Create sample weights (3x weight for 2-5L range)\n",
    "print(\"\\n⚖️  Creating sample weights for range balancing...\")\n",
    "sample_weights = np.ones(len(y_train_log))\n",
    "sample_weights[income_ranges['low']] = 3.0  # 3x weight for 2-5L range\n",
    "\n",
    "print(f\"   ✅ Sample weights created: {len(sample_weights)} weights\")\n",
    "print(f\"   🔹 Standard weight (5-10L, 10L+): 1.0 ({(sample_weights == 1.0).sum():,} samples)\")\n",
    "print(f\"   🔹 Enhanced weight (2-5L): 3.0 ({(sample_weights == 3.0).sum():,} samples)\")\n",
    "print(f\"   🔹 Total weighted samples equivalent: {sample_weights.sum():,.0f}\")\n",
    "\n",
    "# Target variable statistics in both scales\n",
    "print(\"\\n📈 TARGET VARIABLE STATISTICS:\")\n",
    "print(\"Original Scale (₹):\")\n",
    "print(f\"   🔹 Training   | Mean: ₹{y_train.mean():>8,.0f} | Std: ₹{y_train.std():>8,.0f} | Range: [₹{y_train.min():>7,.0f}, ₹{y_train.max():>9,.0f}]\")\n",
    "print(f\"   🔹 Validation | Mean: ₹{y_val.mean():>8,.0f} | Std: ₹{y_val.std():>8,.0f} | Range: [₹{y_val.min():>7,.0f}, ₹{y_val.max():>9,.0f}]\")\n",
    "\n",
    "print(\"\\nLog-Transformed Scale:\")\n",
    "print(f\"   🔹 Training   | Mean: {y_train_log.mean():>6.3f} | Std: {y_train_log.std():>6.3f} | Range: [{y_train_log.min():>6.3f}, {y_train_log.max():>6.3f}]\")\n",
    "print(f\"   🔹 Validation | Mean: {y_val_log.mean():>6.3f} | Std: {y_val_log.std():>6.3f} | Range: [{y_val_log.min():>6.3f}, {y_val_log.max():>6.3f}]\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed72d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 4: ENHANCED EVALUATION FRAMEWORK\n",
    "# =============================================================================\n",
    "\n",
    "print(\"📊 Setting up enhanced evaluation framework with income range analysis\")\n",
    "\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Absolute Percentage Error with safety checks\"\"\"\n",
    "    # Avoid division by zero\n",
    "    mask = y_true != 0\n",
    "    if not mask.any():\n",
    "        return np.inf\n",
    "    \n",
    "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "    return mape\n",
    "\n",
    "def evaluate_with_range_analysis(y_true_log, y_pred_log, income_ranges_mask=None, \n",
    "                                dataset_name=\"Dataset\", return_details=False):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation with income range breakdown\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true_log: True values in log space\n",
    "    - y_pred_log: Predicted values in log space  \n",
    "    - income_ranges_mask: Dict with range masks for original scale\n",
    "    - dataset_name: Name for reporting\n",
    "    - return_details: Whether to return detailed metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert from log space to original scale\n",
    "    y_true_orig = np.expm1(y_true_log)\n",
    "    y_pred_orig = np.expm1(y_pred_log)\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_mape = calculate_mape(y_true_orig, y_pred_orig)\n",
    "    overall_mae = mean_absolute_error(y_true_orig, y_pred_orig)\n",
    "    overall_rmse = np.sqrt(mean_squared_error(y_true_orig, y_pred_orig))\n",
    "    overall_r2 = r2_score(y_true_orig, y_pred_orig)\n",
    "    \n",
    "    print(f\"\\n📊 {dataset_name} PERFORMANCE ANALYSIS:\")\n",
    "    print(f\"   🎯 Overall MAPE: {overall_mape:.2f}%\")\n",
    "    print(f\"   📏 Overall MAE:  ₹{overall_mae:,.0f}\")\n",
    "    print(f\"   📐 Overall RMSE: ₹{overall_rmse:,.0f}\")\n",
    "    print(f\"   📈 Overall R²:   {overall_r2:.4f}\")\n",
    "    \n",
    "    # Range-specific analysis if masks provided\n",
    "    range_metrics = {}\n",
    "    if income_ranges_mask is not None:\n",
    "        print(f\"\\n🎯 INCOME RANGE BREAKDOWN:\")\n",
    "        \n",
    "        range_names = {'low': '2-5L', 'mid': '5-10L', 'high': '10L+'}\n",
    "        \n",
    "        for range_key, mask in income_ranges_mask.items():\n",
    "            if mask.sum() > 0:\n",
    "                range_mape = calculate_mape(y_true_orig[mask], y_pred_orig[mask])\n",
    "                range_mae = mean_absolute_error(y_true_orig[mask], y_pred_orig[mask])\n",
    "                range_count = mask.sum()\n",
    "                \n",
    "                range_metrics[range_key] = {\n",
    "                    'mape': range_mape,\n",
    "                    'mae': range_mae,\n",
    "                    'count': range_count\n",
    "                }\n",
    "                \n",
    "                status = \"✅\" if range_mape < 20 else \"⚠️\" if range_mape < 25 else \"❌\"\n",
    "                print(f\"   {status} {range_names[range_key]:>4}: {range_mape:>6.2f}% MAPE | ₹{range_mae:>8,.0f} MAE | {range_count:>6,} samples\")\n",
    "            else:\n",
    "                print(f\"   ⭕ {range_names[range_key]:>4}: No samples in range\")\n",
    "    \n",
    "    if return_details:\n",
    "        return {\n",
    "            'overall_mape': overall_mape,\n",
    "            'overall_mae': overall_mae, \n",
    "            'overall_rmse': overall_rmse,\n",
    "            'overall_r2': overall_r2,\n",
    "            'range_metrics': range_metrics\n",
    "        }\n",
    "    else:\n",
    "        return overall_mape\n",
    "\n",
    "# Cross-validation setup with stratification\n",
    "def create_stratified_cv(y_train, n_splits=5):\n",
    "    \"\"\"Create stratified CV splits based on income ranges\"\"\"\n",
    "    \n",
    "    # Create income bins for stratification\n",
    "    income_bins = np.digitize(y_train, bins=[200000, 500000, 1000000, 2000000])\n",
    "    \n",
    "    # Use StratifiedKFold with income bins\n",
    "    cv_splitter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    print(f\"✅ Created {n_splits}-fold stratified CV based on income ranges\")\n",
    "    print(f\"   🎯 Bin distribution: {np.bincount(income_bins)}\")\n",
    "    \n",
    "    return cv_splitter, income_bins\n",
    "\n",
    "# Model performance tracking\n",
    "class ModelTracker:\n",
    "    \"\"\"Track model performance across experiments\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "        \n",
    "    def add_result(self, model_name, overall_mape, range_metrics=None, \n",
    "                  training_time=0, memory_usage=0, hyperparams=None):\n",
    "        \"\"\"Add model result to tracking\"\"\"\n",
    "        \n",
    "        result = {\n",
    "            'model_name': model_name,\n",
    "            'overall_mape': overall_mape,\n",
    "            'training_time': training_time,\n",
    "            'memory_usage': memory_usage,\n",
    "            'hyperparams': hyperparams or {},\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "        \n",
    "        if range_metrics:\n",
    "            result.update({\n",
    "                'low_mape': range_metrics.get('low', {}).get('mape', np.nan),\n",
    "                'mid_mape': range_metrics.get('mid', {}).get('mape', np.nan), \n",
    "                'high_mape': range_metrics.get('high', {}).get('mape', np.nan)\n",
    "            })\n",
    "            \n",
    "        self.results.append(result)\n",
    "        \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get performance summary table\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to display\")\n",
    "            return None\n",
    "            \n",
    "        df = pd.DataFrame(self.results)\n",
    "        return df.sort_values('overall_mape')\n",
    "    \n",
    "    def get_best_model(self):\n",
    "        \"\"\"Get best performing model\"\"\"\n",
    "        if not self.results:\n",
    "            return None\n",
    "        return min(self.results, key=lambda x: x['overall_mape'])\n",
    "\n",
    "# Initialize model tracker\n",
    "model_tracker = ModelTracker()\n",
    "\n",
    "# Visualization functions\n",
    "def plot_predictions_vs_actual(y_true, y_pred, title=\"Predictions vs Actual\", \n",
    "                              income_ranges_mask=None):\n",
    "    \"\"\"Plot predictions vs actual values with range highlighting\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Convert to millions for better readability\n",
    "    y_true_millions = y_true / 1000000\n",
    "    y_pred_millions = y_pred / 1000000\n",
    "    \n",
    "    if income_ranges_mask is not None:\n",
    "        # Plot different ranges in different colors\n",
    "        colors = {'low': 'red', 'mid': 'blue', 'high': 'green'}\n",
    "        labels = {'low': '2-5L', 'mid': '5-10L', 'high': '10L+'}\n",
    "        \n",
    "        for range_key, mask in income_ranges_mask.items():\n",
    "            if mask.sum() > 0:\n",
    "                plt.scatter(y_true_millions[mask], y_pred_millions[mask], \n",
    "                          alpha=0.6, c=colors[range_key], label=labels[range_key], s=20)\n",
    "    else:\n",
    "        plt.scatter(y_true_millions, y_pred_millions, alpha=0.6, s=20)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    max_val = max(y_true_millions.max(), y_pred_millions.max())\n",
    "    min_val = min(y_true_millions.min(), y_pred_millions.min())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Actual Income (₹ Millions)')\n",
    "    plt.ylabel('Predicted Income (₹ Millions)')\n",
    "    plt.title(title)\n",
    "    \n",
    "    if income_ranges_mask is not None:\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Initialize CV splitter\n",
    "cv_splitter, income_bins = create_stratified_cv(y_train, n_splits=5)\n",
    "\n",
    "print(\"✅ Enhanced evaluation framework setup complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea90ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 5: BASELINE MODELS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"📈 Training baseline models to establish performance benchmarks\")\n",
    "print(\"🎯 All models will use log-transformed targets with sample weighting\")\n",
    "\n",
    "# Simple baseline predictions\n",
    "print(\"\\n🔵 SIMPLE BASELINES:\")\n",
    "\n",
    "# Mean baseline\n",
    "mean_baseline_log = np.full_like(y_val_log, y_train_log.mean())\n",
    "mean_mape = evaluate_with_range_analysis(y_val_log, mean_baseline_log, \n",
    "                                        income_ranges_val, \"Mean Baseline\")\n",
    "model_tracker.add_result(\"Mean Baseline\", mean_mape)\n",
    "\n",
    "# Median baseline  \n",
    "median_baseline_log = np.full_like(y_val_log, np.median(y_train_log))\n",
    "median_mape = evaluate_with_range_analysis(y_val_log, median_baseline_log,\n",
    "                                          income_ranges_val, \"Median Baseline\")\n",
    "model_tracker.add_result(\"Median Baseline\", median_mape)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Linear models with regularization\n",
    "print(\"🔵 LINEAR MODEL BASELINES:\")\n",
    "\n",
    "# Ridge Regression\n",
    "print(\"\\n📊 Training Ridge Regression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "ridge_model = Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
    "ridge_model.fit(X_train, y_train_log, sample_weight=sample_weights)\n",
    "\n",
    "ridge_pred_log = ridge_model.predict(X_val)\n",
    "ridge_time = time.time() - start_time\n",
    "\n",
    "ridge_metrics = evaluate_with_range_analysis(y_val_log, ridge_pred_log, \n",
    "                                           income_ranges_val, \"Ridge Regression\", \n",
    "                                           return_details=True)\n",
    "model_tracker.add_result(\"Ridge Regression\", ridge_metrics['overall_mape'], \n",
    "                        ridge_metrics['range_metrics'], ridge_time)\n",
    "\n",
    "# Lasso Regression\n",
    "print(\"\\n📊 Training Lasso Regression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lasso_model = Lasso(alpha=0.1, random_state=RANDOM_STATE, max_iter=2000)\n",
    "lasso_model.fit(X_train, y_train_log, sample_weight=sample_weights)\n",
    "\n",
    "lasso_pred_log = lasso_model.predict(X_val)\n",
    "lasso_time = time.time() - start_time\n",
    "\n",
    "lasso_metrics = evaluate_with_range_analysis(y_val_log, lasso_pred_log,\n",
    "                                           income_ranges_val, \"Lasso Regression\",\n",
    "                                           return_details=True)\n",
    "model_tracker.add_result(\"Lasso Regression\", lasso_metrics['overall_mape'],\n",
    "                        lasso_metrics['range_metrics'], lasso_time)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Tree-based baselines\n",
    "print(\"🔵 TREE-BASED BASELINES:\")\n",
    "\n",
    "# Random Forest baseline\n",
    "print(\"\\n📊 Training Random Forest Baseline...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_baseline = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10, \n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_baseline.fit(X_train, y_train_log, sample_weight=sample_weights)\n",
    "rf_pred_log = rf_baseline.predict(X_val)\n",
    "rf_time = time.time() - start_time\n",
    "\n",
    "rf_metrics = evaluate_with_range_analysis(y_val_log, rf_pred_log,\n",
    "                                        income_ranges_val, \"Random Forest Baseline\",\n",
    "                                        return_details=True)\n",
    "model_tracker.add_result(\"Random Forest Baseline\", rf_metrics['overall_mape'],\n",
    "                        rf_metrics['range_metrics'], rf_time)\n",
    "\n",
    "# Extra Trees baseline\n",
    "print(\"\\n📊 Training Extra Trees Baseline...\")\n",
    "start_time = time.time()\n",
    "\n",
    "et_baseline = ExtraTreesRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5, \n",
    "    min_samples_leaf=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "et_baseline.fit(X_train, y_train_log, sample_weight=sample_weights)\n",
    "et_pred_log = et_baseline.predict(X_val)\n",
    "et_time = time.time() - start_time\n",
    "\n",
    "et_metrics = evaluate_with_range_analysis(y_val_log, et_pred_log,\n",
    "                                        income_ranges_val, \"Extra Trees Baseline\", \n",
    "                                        return_details=True)\n",
    "model_tracker.add_result(\"Extra Trees Baseline\", et_metrics['overall_mape'],\n",
    "                        et_metrics['range_metrics'], et_time)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Gradient boosting baselines (if available)\n",
    "if XGBOOST_AVAILABLE:\n",
    "    print(\"🔵 GRADIENT BOOSTING BASELINE:\")\n",
    "    print(\"\\n📊 Training XGBoost Baseline...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    xgb_baseline = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    xgb_baseline.fit(X_train, y_train_log, sample_weight=sample_weights)\n",
    "    xgb_pred_log = xgb_baseline.predict(X_val)\n",
    "    xgb_time = time.time() - start_time\n",
    "    \n",
    "    xgb_metrics = evaluate_with_range_analysis(y_val_log, xgb_pred_log,\n",
    "                                             income_ranges_val, \"XGBoost Baseline\",\n",
    "                                             return_details=True)\n",
    "    model_tracker.add_result(\"XGBoost Baseline\", xgb_metrics['overall_mape'],\n",
    "                            xgb_metrics['range_metrics'], xgb_time)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Baseline results summary\n",
    "print(\"📊 BASELINE MODELS SUMMARY:\")\n",
    "baseline_summary = model_tracker.get_summary()\n",
    "if baseline_summary is not None:\n",
    "    print(\"\\nTop 5 Baseline Models:\")\n",
    "    display_cols = ['model_name', 'overall_mape', 'low_mape', 'mid_mape', 'high_mape', 'training_time']\n",
    "    available_cols = [col for col in display_cols if col in baseline_summary.columns]\n",
    "    print(baseline_summary[available_cols].head().to_string(index=False, float_format='%.2f'))\n",
    "\n",
    "# Best baseline identification\n",
    "best_baseline = model_tracker.get_best_model()\n",
    "if best_baseline:\n",
    "    print(f\"\\n🏆 BEST BASELINE MODEL: {best_baseline['model_name']}\")\n",
    "    print(f\"   🎯 Overall MAPE: {best_baseline['overall_mape']:.2f}%\")\n",
    "    if 'low_mape' in best_baseline:\n",
    "        print(f\"   🔴 2-5L MAPE: {best_baseline['low_mape']:.2f}%\") \n",
    "        print(f\"   🔵 5-10L MAPE: {best_baseline['mid_mape']:.2f}%\")\n",
    "        print(f\"   🟢 10L+ MAPE: {best_baseline['high_mape']:.2f}%\")\n",
    "\n",
    "print(\"\\n✅ Baseline models training complete!\")\n",
    "print(\"🎯 Target for advanced models: Beat best baseline performance\")\n",
    "print(\"🔥 Critical focus: Reduce 2-5L range MAPE significantly\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6980f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 6: ADVANCED MODEL 1 - LIGHTGBM WITH OPTUNA\n",
    "# =============================================================================\n",
    "\n",
    "if LIGHTGBM_AVAILABLE and OPTUNA_AVAILABLE:\n",
    "    print(\"🚀 Training LightGBM with Optuna hyperparameter optimization\")\n",
    "    print(\"🎯 Focus: Minimize overall MAPE with 2-5L range penalty\")\n",
    "    \n",
    "    def objective_lightgbm(trial):\n",
    "        \"\"\"Optuna objective function for LightGBM optimization\"\"\"\n",
    "        \n",
    "        # Define hyperparameter search space\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mae',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "            'verbosity': -1,\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # Add GPU support if available\n",
    "        try:\n",
    "            params['device'] = 'gpu'\n",
    "            params['gpu_platform_id'] = 0\n",
    "            params['gpu_device_id'] = 0\n",
    "        except:\n",
    "            params['device'] = 'cpu'\n",
    "        \n",
    "        # Cross-validation with income range evaluation\n",
    "        cv_scores = []\n",
    "        cv_range_penalties = []\n",
    "        \n",
    "        for train_idx, val_idx in cv_splitter.split(X_train, income_bins):\n",
    "            X_cv_train, X_cv_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_cv_train, y_cv_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "            weights_cv_train = sample_weights[train_idx]\n",
    "            \n",
    "            # Create LightGBM datasets\n",
    "            train_data = lgb.Dataset(X_cv_train, label=y_cv_train, weight=weights_cv_train)\n",
    "            \n",
    "            # Train model\n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train_data,\n",
    "                num_boost_round=1000,\n",
    "                valid_sets=[train_data],\n",
    "                callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
    "            )\n",
    "            \n",
    "            # Predict and evaluate\n",
    "            y_pred_log = model.predict(X_cv_val, num_iteration=model.best_iteration)\n",
    "            \n",
    "            # Convert to original scale for MAPE calculation\n",
    "            y_true_orig = np.expm1(y_cv_val)\n",
    "            y_pred_orig = np.expm1(y_pred_log)\n",
    "            \n",
    "            # Calculate overall MAPE\n",
    "            overall_mape = calculate_mape(y_true_orig, y_pred_orig)\n",
    "            cv_scores.append(overall_mape)\n",
    "            \n",
    "            # Calculate 2-5L range penalty (if samples exist)\n",
    "            low_mask = (y_true_orig >= 200000) & (y_true_orig < 500000)\n",
    "            if low_mask.sum() > 0:\n",
    "                low_mape = calculate_mape(y_true_orig[low_mask], y_pred_orig[low_mask])\n",
    "                # Penalty: heavily weight 2-5L performance\n",
    "                range_penalty = max(0, low_mape - 25) * 2  # Penalty if >25% MAPE\n",
    "                cv_range_penalties.append(range_penalty)\n",
    "            else:\n",
    "                cv_range_penalties.append(0)\n",
    "        \n",
    "        # Final objective: overall MAPE + range penalty\n",
    "        mean_mape = np.mean(cv_scores)\n",
    "        mean_penalty = np.mean(cv_range_penalties)\n",
    "        objective_value = mean_mape + mean_penalty\n",
    "        \n",
    "        return objective_value\n",
    "    \n",
    "    # Run Optuna optimization\n",
    "    print(\"\\n🔄 Starting Optuna optimization (Target: 200 trials)...\")\n",
    "    print(\"   💡 Objective: Minimize (Overall MAPE + 2-5L Range Penalty)\")\n",
    "    \n",
    "    study_lgb = optuna.create_study(direction='minimize', study_name='lightgbm_optimization')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    study_lgb.optimize(objective_lightgbm, n_trials=200, timeout=1800)  # 30 min timeout\n",
    "    optimization_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n✅ Optuna optimization completed in {optimization_time/60:.1f} minutes\")\n",
    "    print(f\"🏆 Best objective value: {study_lgb.best_value:.3f}\")\n",
    "    print(f\"🎯 Best trial number: {study_lgb.best_trial.number}\")\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    print(\"\\n🔄 Training final LightGBM model with best parameters...\")\n",
    "    \n",
    "    best_params = study_lgb.best_params.copy()\n",
    "    best_params.update({\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mae',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1\n",
    "    })\n",
    "    \n",
    "    # Add GPU support\n",
    "    try:\n",
    "        best_params['device'] = 'gpu'\n",
    "        best_params['gpu_platform_id'] = 0\n",
    "        best_params['gpu_device_id'] = 0\n",
    "        print(\"   ⚡ Using GPU acceleration\")\n",
    "    except:\n",
    "        best_params['device'] = 'cpu'\n",
    "        print(\"   🖥️  Using CPU training\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train_log, weight=sample_weights)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val_log, reference=train_data)\n",
    "    \n",
    "    # Train final model\n",
    "    start_time = time.time()\n",
    "    lgb_final = lgb.train(\n",
    "        best_params,\n",
    "        train_data,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'val'],\n",
    "        callbacks=[lgb.early_stopping(150), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Generate predictions\n",
    "    lgb_pred_log = lgb_final.predict(X_val, num_iteration=lgb_final.best_iteration)\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    print(f\"\\n📊 Final LightGBM model training completed in {training_time:.1f} seconds\")\n",
    "    lgb_metrics = evaluate_with_range_analysis(y_val_log, lgb_pred_log,\n",
    "                                             income_ranges_val, \"LightGBM Optimized\",\n",
    "                                             return_details=True)\n",
    "    \n",
    "    # Track results\n",
    "    model_tracker.add_result(\"LightGBM Optimized\", lgb_metrics['overall_mape'],\n",
    "                            lgb_metrics['range_metrics'], training_time,\n",
    "                            hyperparams=best_params)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    print(\"\\n📊 LightGBM Feature Importance (Top 15):\")\n",
    "    feature_importance = lgb_final.feature_importance(importance_type='gain')\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(importance_df.head(15).to_string(index=False))\n",
    "    \n",
    "    # Save model\n",
    "    model_path = MODELS_DIR / \"lightgbm_optimized.pkl\"\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'model': lgb_final,\n",
    "            'params': best_params,\n",
    "            'feature_names': feature_names,\n",
    "            'cv_score': study_lgb.best_value,\n",
    "            'feature_importance': importance_df\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"\\n💾 LightGBM model saved to: {model_path}\")\n",
    "    \n",
    "    # Visualization of hyperparameter optimization\n",
    "    print(\"\\n📈 Optimization Analysis:\")\n",
    "    print(f\"   🔹 Total trials: {len(study_lgb.trials)}\")\n",
    "    print(f\"   🔹 Best trial: {study_lgb.best_trial.number}\")\n",
    "    print(f\"   🔹 Best parameters:\")\n",
    "    for param, value in study_lgb.best_params.items():\n",
    "        print(f\"      • {param}: {value}\")\n",
    "    \n",
    "    print(\"\\n✅ LightGBM optimization and training complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ LightGBM or Optuna not available - skipping LightGBM optimization\")\n",
    "    lgb_metrics = None\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c57eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 7: ADVANCED MODEL 2 - XGBOOST WITH OPTUNA\n",
    "# =============================================================================\n",
    "\n",
    "if XGBOOST_AVAILABLE and OPTUNA_AVAILABLE:\n",
    "    print(\"🚀 Training XGBoost with Optuna hyperparameter optimization\")\n",
    "    print(\"🎯 Focus: Minimize overall MAPE with enhanced 2-5L range penalty\")\n",
    "    \n",
    "    def objective_xgboost(trial):\n",
    "        \"\"\"Optuna objective function for XGBoost optimization\"\"\"\n",
    "        \n",
    "        # Define hyperparameter search space\n",
    "        params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'mae',\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 2000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.6, 1.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': -1,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        # Add GPU support if available\n",
    "        try:\n",
    "            params['tree_method'] = 'gpu_hist'\n",
    "            params['gpu_id'] = 0\n",
    "        except:\n",
    "            params['tree_method'] = 'hist'\n",
    "        \n",
    "        # Cross-validation with income range evaluation\n",
    "        cv_scores = []\n",
    "        cv_range_penalties = []\n",
    "        \n",
    "        for train_idx, val_idx in cv_splitter.split(X_train, income_bins):\n",
    "            X_cv_train, X_cv_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_cv_train, y_cv_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "            weights_cv_train = sample_weights[train_idx]\n",
    "            \n",
    "            # Train model\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "            model.fit(\n",
    "                X_cv_train, y_cv_train,\n",
    "                sample_weight=weights_cv_train,\n",
    "                eval_set=[(X_cv_train, y_cv_train)],\n",
    "                early_stopping_rounds=100,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Predict and evaluate\n",
    "            y_pred_log = model.predict(X_cv_val)\n",
    "            \n",
    "            # Convert to original scale for MAPE calculation\n",
    "            y_true_orig = np.expm1(y_cv_val)\n",
    "            y_pred_orig = np.expm1(y_pred_log)\n",
    "            \n",
    "            # Calculate overall MAPE\n",
    "            overall_mape = calculate_mape(y_true_orig, y_pred_orig)\n",
    "            cv_scores.append(overall_mape)\n",
    "            \n",
    "            # Calculate 2-5L range penalty (enhanced)\n",
    "            low_mask = (y_true_orig >= 200000) & (y_true_orig < 500000)\n",
    "            if low_mask.sum() > 0:\n",
    "                low_mape = calculate_mape(y_true_orig[low_mask], y_pred_orig[low_mask])\n",
    "                # Enhanced penalty: more aggressive for 2-5L range\n",
    "                range_penalty = max(0, low_mape - 20) * 3  # Penalty if >20% MAPE\n",
    "                cv_range_penalties.append(range_penalty)\n",
    "            else:\n",
    "                cv_range_penalties.append(0)\n",
    "        \n",
    "        # Final objective: overall MAPE + enhanced range penalty\n",
    "        mean_mape = np.mean(cv_scores)\n",
    "        mean_penalty = np.mean(cv_range_penalties)\n",
    "        objective_value = mean_mape + mean_penalty\n",
    "        \n",
    "        return objective_value\n",
    "    \n",
    "    # Run Optuna optimization\n",
    "    print(\"\\n🔄 Starting Optuna optimization (Target: 200 trials)...\")\n",
    "    print(\"   💡 Objective: Minimize (Overall MAPE + Enhanced 2-5L Range Penalty)\")\n",
    "    \n",
    "    study_xgb = optuna.create_study(direction='minimize', study_name='xgboost_optimization')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    study_xgb.optimize(objective_xgboost, n_trials=200, timeout=1800)  # 30 min timeout\n",
    "    optimization_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n✅ Optuna optimization completed in {optimization_time/60:.1f} minutes\")\n",
    "    print(f\"🏆 Best objective value: {study_xgb.best_value:.3f}\")\n",
    "    print(f\"🎯 Best trial number: {study_xgb.best_trial.number}\")\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    print(\"\\n🔄 Training final XGBoost model with best parameters...\")\n",
    "    \n",
    "    best_params = study_xgb.best_params.copy()\n",
    "    best_params.update({\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'mae',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': 0\n",
    "    })\n",
    "    \n",
    "    # Add GPU support\n",
    "    try:\n",
    "        best_params['tree_method'] = 'gpu_hist'\n",
    "        best_params['gpu_id'] = 0\n",
    "        print(\"   ⚡ Using GPU acceleration\")\n",
    "    except:\n",
    "        best_params['tree_method'] = 'hist'\n",
    "        print(\"   🖥️  Using CPU training\")\n",
    "    \n",
    "    # Train final model\n",
    "    start_time = time.time()\n",
    "    xgb_final = xgb.XGBRegressor(**best_params)\n",
    "    xgb_final.fit(\n",
    "        X_train, y_train_log,\n",
    "        sample_weight=sample_weights,\n",
    "        eval_set=[(X_train, y_train_log), (X_val, y_val_log)],\n",
    "        early_stopping_rounds=150,\n",
    "        verbose=100\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Generate predictions\n",
    "    xgb_pred_log = xgb_final.predict(X_val)\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    print(f\"\\n📊 Final XGBoost model training completed in {training_time:.1f} seconds\")\n",
    "    xgb_metrics = evaluate_with_range_analysis(y_val_log, xgb_pred_log,\n",
    "                                             income_ranges_val, \"XGBoost Optimized\",\n",
    "                                             return_details=True)\n",
    "    \n",
    "    # Track results\n",
    "    model_tracker.add_result(\"XGBoost Optimized\", xgb_metrics['overall_mape'],\n",
    "                            xgb_metrics['range_metrics'], training_time,\n",
    "                            hyperparams=best_params)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    print(\"\\n📊 XGBoost Feature Importance (Top 15):\")\n",
    "    feature_importance = xgb_final.feature_importances_\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(importance_df.head(15).to_string(index=False))\n",
    "    \n",
    "    # Training progress visualization\n",
    "    if hasattr(xgb_final, 'evals_result_'):\n",
    "        results = xgb_final.evals_result_\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(results['validation_0']['mae'], label='Training MAE')\n",
    "        plt.plot(results['validation_1']['mae'], label='Validation MAE')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.title('XGBoost Training Progress')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.bar(range(len(importance_df.head(10))), importance_df.head(10)['importance'])\n",
    "        plt.xlabel('Feature Rank')\n",
    "        plt.ylabel('Importance')\n",
    "        plt.title('Top 10 Feature Importances')\n",
    "        plt.xticks(range(10), range(1, 11))\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Save model\n",
    "    model_path = MODELS_DIR / \"xgboost_optimized.pkl\"\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'model': xgb_final,\n",
    "            'params': best_params,\n",
    "            'feature_names': feature_names,\n",
    "            'cv_score': study_xgb.best_value,\n",
    "            'feature_importance': importance_df\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"\\n💾 XGBoost model saved to: {model_path}\")\n",
    "    \n",
    "    # Optimization analysis\n",
    "    print(\"\\n📈 Optimization Analysis:\")\n",
    "    print(f\"   🔹 Total trials: {len(study_xgb.trials)}\")\n",
    "    print(f\"   🔹 Best trial: {study_xgb.best_trial.number}\")\n",
    "    print(f\"   🔹 Best parameters:\")\n",
    "    for param, value in study_xgb.best_params.items():\n",
    "        print(f\"      • {param}: {value}\")\n",
    "    \n",
    "    # Compare with previous best\n",
    "    current_best = model_tracker.get_best_model()\n",
    "    if current_best and current_best['model_name'] == 'XGBoost Optimized':\n",
    "        print(f\"\\n🏆 XGBoost is now the best model!\")\n",
    "    \n",
    "    print(\"\\n✅ XGBoost optimization and training complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ XGBoost or Optuna not available - skipping XGBoost optimization\")\n",
    "    xgb_metrics = None\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf839629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 8: ADVANCED MODEL 3 - CATBOOST WITH OPTUNA\n",
    "# =============================================================================\n",
    "\n",
    "if CATBOOST_AVAILABLE and OPTUNA_AVAILABLE:\n",
    "    print(\"🚀 Training CatBoost with Optuna hyperparameter optimization\")\n",
    "    print(\"🎯 Focus: Leverage CatBoost's built-in overfitting detection + 2-5L penalty\")\n",
    "    \n",
    "    def objective_catboost(trial):\n",
    "        \"\"\"Optuna objective function for CatBoost optimization\"\"\"\n",
    "        \n",
    "        # Define hyperparameter search space\n",
    "        params = {\n",
    "            'loss_function': 'MAE',\n",
    "            'iterations': trial.suggest_int('iterations', 500, 3000),\n",
    "            'depth': trial.suggest_int('depth', 4, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 30),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.6, 1.0),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 50),\n",
    "            'max_leaves': trial.suggest_int('max_leaves', 16, 512),\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'verbose': False,\n",
    "            'thread_count': -1\n",
    "        }\n",
    "        \n",
    "        # Add GPU support if available\n",
    "        try:\n",
    "            params['task_type'] = 'GPU'\n",
    "            params['gpu_ram_part'] = 0.5\n",
    "        except:\n",
    "            params['task_type'] = 'CPU'\n",
    "        \n",
    "        # Cross-validation with income range evaluation\n",
    "        cv_scores = []\n",
    "        cv_range_penalties = []\n",
    "        \n",
    "        for train_idx, val_idx in cv_splitter.split(X_train, income_bins):\n",
    "            X_cv_train, X_cv_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_cv_train, y_cv_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "            weights_cv_train = sample_weights[train_idx]\n",
    "            \n",
    "            # Create CatBoost pools\n",
    "            train_pool = cb.Pool(X_cv_train, y_cv_train, weight=weights_cv_train)\n",
    "            val_pool = cb.Pool(X_cv_val, y_cv_val)\n",
    "            \n",
    "            # Train model with built-in overfitting detection\n",
    "            model = cb.CatBoostRegressor(**params)\n",
    "            model.fit(\n",
    "                train_pool,\n",
    "                eval_set=val_pool,\n",
    "                early_stopping_rounds=100,\n",
    "                use_best_model=True,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Predict and evaluate\n",
    "            y_pred_log = model.predict(X_cv_val)\n",
    "            \n",
    "            # Convert to original scale for MAPE calculation\n",
    "            y_true_orig = np.expm1(y_cv_val)\n",
    "            y_pred_orig = np.expm1(y_pred_log)\n",
    "            \n",
    "            # Calculate overall MAPE\n",
    "            overall_mape = calculate_mape(y_true_orig, y_pred_orig)\n",
    "            cv_scores.append(overall_mape)\n",
    "            \n",
    "            # Calculate 2-5L range penalty (most aggressive)\n",
    "            low_mask = (y_true_orig >= 200000) & (y_true_orig < 500000)\n",
    "            if low_mask.sum() > 0:\n",
    "                low_mape = calculate_mape(y_true_orig[low_mask], y_pred_orig[low_mask])\n",
    "                # Most aggressive penalty for CatBoost\n",
    "                range_penalty = max(0, low_mape - 18) * 4  # Penalty if >18% MAPE\n",
    "                cv_range_penalties.append(range_penalty)\n",
    "            else:\n",
    "                cv_range_penalties.append(0)\n",
    "        \n",
    "        # Final objective: overall MAPE + most aggressive range penalty\n",
    "        mean_mape = np.mean(cv_scores)\n",
    "        mean_penalty = np.mean(cv_range_penalties)\n",
    "        objective_value = mean_mape + mean_penalty\n",
    "        \n",
    "        return objective_value\n",
    "    \n",
    "    # Run Optuna optimization\n",
    "    print(\"\\n🔄 Starting Optuna optimization (Target: 150 trials - CatBoost is slower)...\")\n",
    "    print(\"   💡 Objective: Minimize (Overall MAPE + Most Aggressive 2-5L Penalty)\")\n",
    "    print(\"   🎯 Using CatBoost's built-in overfitting detection\")\n",
    "    \n",
    "    study_cb = optuna.create_study(direction='minimize', study_name='catboost_optimization')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    study_cb.optimize(objective_catboost, n_trials=150, timeout=2400)  # 40 min timeout\n",
    "    optimization_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n✅ Optuna optimization completed in {optimization_time/60:.1f} minutes\")\n",
    "    print(f\"🏆 Best objective value: {study_cb.best_value:.3f}\")\n",
    "    print(f\"🎯 Best trial number: {study_cb.best_trial.number}\")\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    print(\"\\n🔄 Training final CatBoost model with best parameters...\")\n",
    "    \n",
    "    best_params = study_cb.best_params.copy()\n",
    "    best_params.update({\n",
    "        'loss_function': 'MAE',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': 100,  # Show progress for final training\n",
    "        'thread_count': -1\n",
    "    })\n",
    "    \n",
    "    # Add GPU support\n",
    "    try:\n",
    "        best_params['task_type'] = 'GPU'\n",
    "        best_params['gpu_ram_part'] = 0.5\n",
    "        print(\"   ⚡ Using GPU acceleration\")\n",
    "    except:\n",
    "        best_params['task_type'] = 'CPU'\n",
    "        print(\"   🖥️  Using CPU training\")\n",
    "    \n",
    "    # Create pools for training\n",
    "    train_pool = cb.Pool(X_train, y_train_log, weight=sample_weights)\n",
    "    val_pool = cb.Pool(X_val, y_val_log)\n",
    "    \n",
    "    # Train final model\n",
    "    start_time = time.time()\n",
    "    cb_final = cb.CatBoostRegressor(**best_params)\n",
    "    cb_final.fit(\n",
    "        train_pool,\n",
    "        eval_set=val_pool,\n",
    "        early_stopping_rounds=200,\n",
    "        use_best_model=True,\n",
    "        plot=False\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Generate predictions\n",
    "    cb_pred_log = cb_final.predict(X_val)\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    print(f\"\\n📊 Final CatBoost model training completed in {training_time:.1f} seconds\")\n",
    "    cb_metrics = evaluate_with_range_analysis(y_val_log, cb_pred_log,\n",
    "                                            income_ranges_val, \"CatBoost Optimized\",\n",
    "                                            return_details=True)\n",
    "    \n",
    "    # Track results\n",
    "    model_tracker.add_result(\"CatBoost Optimized\", cb_metrics['overall_mape'],\n",
    "                            cb_metrics['range_metrics'], training_time,\n",
    "                            hyperparams=best_params)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    print(\"\\n📊 CatBoost Feature Importance (Top 15):\")\n",
    "    feature_importance = cb_final.get_feature_importance()\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(importance_df.head(15).to_string(index=False))\n",
    "    \n",
    "    # Training metrics visualization\n",
    "    if hasattr(cb_final, 'get_evals_result'):\n",
    "        evals_result = cb_final.get_evals_result()\n",
    "        if evals_result:\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            # Training progress\n",
    "            plt.subplot(1, 3, 1)\n",
    "            train_scores = evals_result['learn']['MAE']\n",
    "            val_scores = evals_result['validation']['MAE']\n",
    "            iterations = range(len(train_scores))\n",
    "            \n",
    "            plt.plot(iterations, train_scores, label='Training MAE', alpha=0.8)\n",
    "            plt.plot(iterations, val_scores, label='Validation MAE', alpha=0.8)\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('MAE')\n",
    "            plt.title('CatBoost Training Progress')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Feature importance\n",
    "            plt.subplot(1, 3, 2)\n",
    "            top_features = importance_df.head(10)\n",
    "            plt.barh(range(len(top_features)), top_features['importance'])\n",
    "            plt.yticks(range(len(top_features)), \n",
    "                      [f[:20] + '...' if len(f) > 20 else f for f in top_features['feature']])\n",
    "            plt.xlabel('Importance')\n",
    "            plt.title('Top 10 Feature Importances')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Learning curve comparison\n",
    "            plt.subplot(1, 3, 3)\n",
    "            min_len = min(len(train_scores), len(val_scores))\n",
    "            diff = np.array(val_scores[:min_len]) - np.array(train_scores[:min_len])\n",
    "            plt.plot(range(min_len), diff, label='Val - Train MAE', color='red', alpha=0.7)\n",
    "            plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('MAE Difference')\n",
    "            plt.title('Overfitting Detection')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    # Model analysis\n",
    "    print(f\"\\n📈 CatBoost Model Analysis:\")\n",
    "    print(f\"   🔹 Total iterations: {cb_final.get_best_iteration()}\")\n",
    "    print(f\"   🔹 Best iteration: {cb_final.get_best_iteration()}\")\n",
    "    print(f\"   🔹 Training score: {cb_final.get_best_score()['learn']['MAE']:.4f}\")\n",
    "    print(f\"   🔹 Validation score: {cb_final.get_best_score()['validation']['MAE']:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_path = MODELS_DIR / \"catboost_optimized.cbm\"\n",
    "    cb_final.save_model(str(model_path))\n",
    "    \n",
    "    # Also save metadata\n",
    "    metadata_path = MODELS_DIR / \"catboost_optimized_metadata.pkl\"\n",
    "    with open(metadata_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'params': best_params,\n",
    "            'feature_names': feature_names,\n",
    "            'cv_score': study_cb.best_value,\n",
    "            'feature_importance': importance_df,\n",
    "            'best_iteration': cb_final.get_best_iteration()\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"\\n💾 CatBoost model saved to: {model_path}\")\n",
    "    print(f\"💾 Metadata saved to: {metadata_path}\")\n",
    "    \n",
    "    # Optimization analysis\n",
    "    print(\"\\n📈 Optimization Analysis:\")\n",
    "    print(f\"   🔹 Total trials: {len(study_cb.trials)}\")\n",
    "    print(f\"   🔹 Best trial: {study_cb.best_trial.number}\")\n",
    "    print(f\"   🔹 Best parameters:\")\n",
    "    for param, value in study_cb.best_params.items():\n",
    "        print(f\"      • {param}: {value}\")\n",
    "    \n",
    "    # Compare with previous best\n",
    "    current_best = model_tracker.get_best_model()\n",
    "    if current_best and current_best['model_name'] == 'CatBoost Optimized':\n",
    "        print(f\"\\n🏆 CatBoost is now the best model!\")\n",
    "    \n",
    "    print(\"\\n✅ CatBoost optimization and training complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ CatBoost or Optuna not available - skipping CatBoost optimization\")\n",
    "    cb_metrics = None\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62aed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 9: ADVANCED MODEL 4 - EXTRA TREES WITH OPTUNA\n",
    "# =============================================================================\n",
    "\n",
    "if OPTUNA_AVAILABLE:\n",
    "    print(\"🚀 Training Extra Trees with Optuna hyperparameter optimization\")\n",
    "    print(\"🎯 Focus: Variance reduction for 2-5L range stability + ensemble diversity\")\n",
    "    \n",
    "    def objective_extratrees(trial):\n",
    "        \"\"\"Optuna objective function for Extra Trees optimization\"\"\"\n",
    "        \n",
    "        # Define hyperparameter search space\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.3, 0.5, 0.7, 1.0]),\n",
    "            'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.01),\n",
    "            'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # Cross-validation with income range evaluation\n",
    "        cv_scores = []\n",
    "        cv_range_penalties = []\n",
    "        cv_range_stds = []  # Track variance for stability\n",
    "        \n",
    "        for train_idx, val_idx in cv_splitter.split(X_train, income_bins):\n",
    "            X_cv_train, X_cv_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_cv_train, y_cv_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "            weights_cv_train = sample_weights[train_idx]\n",
    "            \n",
    "            # Train model\n",
    "            model = ExtraTreesRegressor(**params)\n",
    "            model.fit(X_cv_train, y_cv_train, sample_weight=weights_cv_train)\n",
    "            \n",
    "            # Predict and evaluate\n",
    "            y_pred_log = model.predict(X_cv_val)\n",
    "            \n",
    "            # Convert to original scale for MAPE calculation\n",
    "            y_true_orig = np.expm1(y_cv_val)\n",
    "            y_pred_orig = np.expm1(y_pred_log)\n",
    "            \n",
    "            # Calculate overall MAPE\n",
    "            overall_mape = calculate_mape(y_true_orig, y_pred_orig)\n",
    "            cv_scores.append(overall_mape)\n",
    "            \n",
    "            # Calculate 2-5L range penalty and variance\n",
    "            low_mask = (y_true_orig >= 200000) & (y_true_orig < 500000)\n",
    "            if low_mask.sum() > 0:\n",
    "                low_mape = calculate_mape(y_true_orig[low_mask], y_pred_orig[low_mask])\n",
    "                \n",
    "                # Standard penalty for 2-5L range\n",
    "                range_penalty = max(0, low_mape - 22) * 2.5  # Penalty if >22% MAPE\n",
    "                cv_range_penalties.append(range_penalty)\n",
    "                \n",
    "                # Variance penalty - Extra Trees should reduce prediction variance\n",
    "                low_pred_std = np.std(y_pred_orig[low_mask])\n",
    "                low_true_std = np.std(y_true_orig[low_mask])\n",
    "                variance_ratio = low_pred_std / (low_true_std + 1e-8)\n",
    "                variance_penalty = max(0, variance_ratio - 1.0) * 5  # Penalize high variance\n",
    "                cv_range_stds.append(variance_penalty)\n",
    "            else:\n",
    "                cv_range_penalties.append(0)\n",
    "                cv_range_stds.append(0)\n",
    "        \n",
    "        # Final objective: MAPE + range penalty + variance penalty\n",
    "        mean_mape = np.mean(cv_scores)\n",
    "        mean_range_penalty = np.mean(cv_range_penalties)\n",
    "        mean_variance_penalty = np.mean(cv_range_stds)\n",
    "        objective_value = mean_mape + mean_range_penalty + mean_variance_penalty\n",
    "        \n",
    "        return objective_value\n",
    "    \n",
    "    # Run Optuna optimization\n",
    "    print(\"\\n🔄 Starting Optuna optimization (Target: 100 trials - faster than boosting)...\")\n",
    "    print(\"   💡 Objective: Minimize (MAPE + 2-5L Range Penalty + Variance Penalty)\")\n",
    "    print(\"   🎯 Focus on variance reduction and ensemble diversity\")\n",
    "    \n",
    "    study_et = optuna.create_study(direction='minimize', study_name='extratrees_optimization')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    study_et.optimize(objective_extratrees, n_trials=100, timeout=1200)  # 20 min timeout\n",
    "    optimization_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n✅ Optuna optimization completed in {optimization_time/60:.1f} minutes\")\n",
    "    print(f\"🏆 Best objective value: {study_et.best_value:.3f}\")\n",
    "    print(f\"🎯 Best trial number: {study_et.best_trial.number}\")\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    print(\"\\n🔄 Training final Extra Trees model with best parameters...\")\n",
    "    \n",
    "    best_params = study_et.best_params.copy()\n",
    "    best_params.update({\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1\n",
    "    })\n",
    "    \n",
    "    # Train final model\n",
    "    start_time = time.time()\n",
    "    et_final = ExtraTreesRegressor(**best_params)\n",
    "    et_final.fit(X_train, y_train_log, sample_weight=sample_weights)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Generate predictions\n",
    "    et_pred_log = et_final.predict(X_val)\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    print(f\"\\n📊 Final Extra Trees model training completed in {training_time:.1f} seconds\")\n",
    "    et_metrics = evaluate_with_range_analysis(y_val_log, et_pred_log,\n",
    "                                            income_ranges_val, \"Extra Trees Optimized\",\n",
    "                                            return_details=True)\n",
    "    \n",
    "    # Track results\n",
    "    model_tracker.add_result(\"Extra Trees Optimized\", et_metrics['overall_mape'],\n",
    "                            et_metrics['range_metrics'], training_time,\n",
    "                            hyperparams=best_params)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    print(\"\\n📊 Extra Trees Feature Importance (Top 15):\")\n",
    "    feature_importance = et_final.feature_importances_\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(importance_df.head(15).to_string(index=False))\n",
    "    \n",
    "    # Prediction variance analysis\n",
    "    print(\"\\n📊 Prediction Variance Analysis:\")\n",
    "    y_val_orig = np.expm1(y_val_log)\n",
    "    et_pred_orig = np.expm1(et_pred_log)\n",
    "    \n",
    "    for range_name, mask in income_ranges_val.items():\n",
    "        if mask.sum() > 0:\n",
    "            true_std = np.std(y_val_orig[mask])\n",
    "            pred_std = np.std(et_pred_orig[mask])\n",
    "            variance_ratio = pred_std / true_std\n",
    "            range_labels = {'low': '2-5L', 'mid': '5-10L', 'high': '10L+'}\n",
    "            print(f\"   🔹 {range_labels[range_name]:>4}: True Std=₹{true_std:>8,.0f} | Pred Std=₹{pred_std:>8,.0f} | Ratio={variance_ratio:.3f}\")\n",
    "    \n",
    "    # Ensemble diversity analysis (compare with Random Forest if available)\n",
    "    rf_available = any(result['model_name'] == 'Random Forest Baseline' for result in model_tracker.results)\n",
    "    if rf_available:\n",
    "        print(\"\\n📊 Ensemble Diversity Analysis:\")\n",
    "        # Find Random Forest predictions for comparison\n",
    "        rf_result = next((r for r in model_tracker.results if r['model_name'] == 'Random Forest Baseline'), None)\n",
    "        if rf_result:\n",
    "            print(\"   🎯 Extra Trees vs Random Forest diversity will be valuable for ensemble\")\n",
    "    \n",
    "    # Model visualization\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Feature importance\n",
    "    plt.subplot(1, 3, 1)\n",
    "    top_features = importance_df.head(10)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), \n",
    "              [f[:15] + '...' if len(f) > 15 else f for f in top_features['feature']])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Extra Trees: Top 10 Features')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Predictions vs actual by income range\n",
    "    plt.subplot(1, 3, 2)\n",
    "    colors = {'low': 'red', 'mid': 'blue', 'high': 'green'}\n",
    "    labels = {'low': '2-5L', 'mid': '5-10L', 'high': '10L+'}\n",
    "    \n",
    "    for range_name, mask in income_ranges_val.items():\n",
    "        if mask.sum() > 0:\n",
    "            y_true_millions = y_val_orig[mask] / 1000000\n",
    "            y_pred_millions = et_pred_orig[mask] / 1000000\n",
    "            plt.scatter(y_true_millions, y_pred_millions, \n",
    "                       alpha=0.6, c=colors[range_name], label=labels[range_name], s=20)\n",
    "    \n",
    "    max_val = max(y_val_orig.max(), et_pred_orig.max()) / 1000000\n",
    "    min_val = min(y_val_orig.min(), et_pred_orig.min()) / 1000000\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=2)\n",
    "    plt.xlabel('Actual Income (₹ Millions)')\n",
    "    plt.ylabel('Predicted Income (₹ Millions)')\n",
    "    plt.title('Extra Trees: Predictions vs Actual')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error distribution by range\n",
    "    plt.subplot(1, 3, 3)\n",
    "    errors_by_range = {}\n",
    "    for range_name, mask in income_ranges_val.items():\n",
    "        if mask.sum() > 0:\n",
    "            errors = np.abs(y_val_orig[mask] - et_pred_orig[mask]) / y_val_orig[mask] * 100\n",
    "            errors_by_range[labels[range_name]] = errors\n",
    "    \n",
    "    if errors_by_range:\n",
    "        plt.boxplot(errors_by_range.values(), labels=errors_by_range.keys())\n",
    "        plt.ylabel('Absolute Percentage Error (%)')\n",
    "        plt.title('Error Distribution by Income Range')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save model\n",
    "    model_path = MODELS_DIR / \"extratrees_optimized.pkl\"\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'model': et_final,\n",
    "            'params': best_params,\n",
    "            'feature_names': feature_names,\n",
    "            'cv_score': study_et.best_value,\n",
    "            'feature_importance': importance_df\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"\\n💾 Extra Trees model saved to: {model_path}\")\n",
    "    \n",
    "    # Optimization analysis\n",
    "    print(\"\\n📈 Optimization Analysis:\")\n",
    "    print(f\"   🔹 Total trials: {len(study_et.trials)}\")\n",
    "    print(f\"   🔹 Best trial: {study_et.best_trial.number}\")\n",
    "    print(f\"   🔹 Best parameters:\")\n",
    "    for param, value in study_et.best_params.items():\n",
    "        print(f\"      • {param}: {value}\")\n",
    "    \n",
    "    # Compare with previous best\n",
    "    current_best = model_tracker.get_best_model()\n",
    "    if current_best and current_best['model_name'] == 'Extra Trees Optimized':\n",
    "        print(f\"\\n🏆 Extra Trees is now the best model!\")\n",
    "    \n",
    "    print(\"\\n✅ Extra Trees optimization and training complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Optuna not available - skipping Extra Trees optimization\")\n",
    "    et_metrics = None\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423455a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 10: MODEL COMPARISON & SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"📊 Comprehensive Model Performance Comparison & Selection\")\n",
    "print(\"🎯 Objective: Identify top 3 models for ensemble and final selection\")\n",
    "\n",
    "# Get comprehensive results summary\n",
    "results_summary = model_tracker.get_summary()\n",
    "\n",
    "if results_summary is not None and len(results_summary) > 0:\n",
    "    \n",
    "    # Display comprehensive comparison table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📋 COMPLETE MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Select columns for display\n",
    "    display_columns = ['model_name', 'overall_mape', 'low_mape', 'mid_mape', 'high_mape', 'training_time']\n",
    "    available_columns = [col for col in display_columns if col in results_summary.columns]\n",
    "    \n",
    "    # Format the results for better readability\n",
    "    display_df = results_summary[available_columns].copy()\n",
    "    \n",
    "    # Round numeric columns\n",
    "    numeric_columns = ['overall_mape', 'low_mape', 'mid_mape', 'high_mape', 'training_time']\n",
    "    for col in numeric_columns:\n",
    "        if col in display_df.columns:\n",
    "            display_df[col] = display_df[col].round(2)\n",
    "    \n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    # Statistical analysis of results\n",
    "    print(f\"\\n📈 STATISTICAL ANALYSIS:\")\n",
    "    print(f\"   🔹 Total models trained: {len(results_summary)}\")\n",
    "    \n",
    "    if 'overall_mape' in results_summary.columns:\n",
    "        overall_mapes = results_summary['overall_mape'].dropna()\n",
    "        print(f\"   🔹 Best overall MAPE: {overall_mapes.min():.2f}%\")\n",
    "        print(f\"   🔹 Worst overall MAPE: {overall_mapes.max():.2f}%\")\n",
    "        print(f\"   🔹 Mean overall MAPE: {overall_mapes.mean():.2f}%\")\n",
    "        print(f\"   🔹 MAPE std deviation: {overall_mapes.std():.2f}%\")\n",
    "        \n",
    "        # Target achievement analysis\n",
    "        target_achieved = (overall_mapes < 18).sum()\n",
    "        print(f\"   🎯 Models achieving <18% MAPE: {target_achieved}/{len(overall_mapes)}\")\n",
    "    \n",
    "    # 2-5L range specific analysis\n",
    "    if 'low_mape' in results_summary.columns:\n",
    "        low_mapes = results_summary['low_mape'].dropna()\n",
    "        if len(low_mapes) > 0:\n",
    "            print(f\"\\n🔴 2-5L RANGE ANALYSIS:\")\n",
    "            print(f\"   🔹 Best 2-5L MAPE: {low_mapes.min():.2f}%\")\n",
    "            print(f\"   🔹 Worst 2-5L MAPE: {low_mapes.max():.2f}%\")\n",
    "            print(f\"   🔹 Mean 2-5L MAPE: {low_mapes.mean():.2f}%\")\n",
    "            \n",
    "            # Critical improvement tracking\n",
    "            baseline_low_mape = 41.43  # From problem context\n",
    "            best_low_mape = low_mapes.min()\n",
    "            improvement = baseline_low_mape - best_low_mape\n",
    "            improvement_pct = (improvement / baseline_low_mape) * 100\n",
    "            \n",
    "            print(f\"   🚀 Improvement from baseline: -{improvement:.2f}% ({improvement_pct:.1f}% reduction)\")\n",
    "            \n",
    "            # Target achievement for 2-5L\n",
    "            low_target_achieved = (low_mapes < 25).sum()\n",
    "            print(f\"   🎯 Models achieving <25% MAPE in 2-5L: {low_target_achieved}/{len(low_mapes)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Advanced model identification and ranking\n",
    "    print(\"🏆 TOP PERFORMING MODELS IDENTIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Filter advanced models only (exclude baselines)\n",
    "    advanced_models = results_summary[\n",
    "        ~results_summary['model_name'].str.contains('Baseline|Mean|Median', case=False, na=False)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(advanced_models) > 0:\n",
    "        print(\"Advanced Models Performance:\")\n",
    "        print(advanced_models[available_columns].to_string(index=False))\n",
    "        \n",
    "        # Multi-criteria ranking system\n",
    "        print(f\"\\n📊 MULTI-CRITERIA RANKING SYSTEM:\")\n",
    "        \n",
    "        # Ranking criteria with weights\n",
    "        ranking_df = advanced_models.copy()\n",
    "        \n",
    "        # Criteria 1: Overall MAPE (40% weight)\n",
    "        if 'overall_mape' in ranking_df.columns:\n",
    "            ranking_df['overall_rank'] = ranking_df['overall_mape'].rank()\n",
    "            print(f\"   🎯 Overall MAPE ranking (40% weight)\")\n",
    "        \n",
    "        # Criteria 2: 2-5L MAPE (35% weight) - Most critical\n",
    "        if 'low_mape' in ranking_df.columns:\n",
    "            ranking_df['low_rank'] = ranking_df['low_mape'].rank()\n",
    "            print(f\"   🔴 2-5L MAPE ranking (35% weight) - CRITICAL\")\n",
    "        \n",
    "        # Criteria 3: Training efficiency (15% weight)\n",
    "        if 'training_time' in ranking_df.columns:\n",
    "            ranking_df['time_rank'] = ranking_df['training_time'].rank()\n",
    "            print(f\"   ⏱️  Training time ranking (15% weight)\")\n",
    "        \n",
    "        # Criteria 4: Range consistency (10% weight)\n",
    "        if 'mid_mape' in ranking_df.columns and 'high_mape' in ranking_df.columns:\n",
    "            ranking_df['range_consistency'] = abs(ranking_df['mid_mape'] - ranking_df['high_mape'])\n",
    "            ranking_df['consistency_rank'] = ranking_df['range_consistency'].rank()\n",
    "            print(f\"   📏 Range consistency ranking (10% weight)\")\n",
    "        \n",
    "        # Calculate weighted composite score\n",
    "        composite_scores = []\n",
    "        for idx, row in ranking_df.iterrows():\n",
    "            score = 0\n",
    "            weights_sum = 0\n",
    "            \n",
    "            if 'overall_rank' in ranking_df.columns:\n",
    "                score += row['overall_rank'] * 0.40\n",
    "                weights_sum += 0.40\n",
    "            \n",
    "            if 'low_rank' in ranking_df.columns:\n",
    "                score += row['low_rank'] * 0.35\n",
    "                weights_sum += 0.35\n",
    "            \n",
    "            if 'time_rank' in ranking_df.columns:\n",
    "                score += row['time_rank'] * 0.15\n",
    "                weights_sum += 0.15\n",
    "            \n",
    "            if 'consistency_rank' in ranking_df.columns:\n",
    "                score += row['consistency_rank'] * 0.10\n",
    "                weights_sum += 0.10\n",
    "            \n",
    "            if weights_sum > 0:\n",
    "                composite_scores.append(score / weights_sum)\n",
    "            else:\n",
    "                composite_scores.append(float('inf'))\n",
    "        \n",
    "        ranking_df['composite_score'] = composite_scores\n",
    "        ranking_df = ranking_df.sort_values('composite_score')\n",
    "        \n",
    "        print(f\"\\n🏆 FINAL WEIGHTED RANKING:\")\n",
    "        rank_display = ranking_df[['model_name', 'overall_mape', 'low_mape', 'composite_score']].copy()\n",
    "        rank_display['rank'] = range(1, len(rank_display) + 1)\n",
    "        rank_display = rank_display[['rank', 'model_name', 'overall_mape', 'low_mape', 'composite_score']]\n",
    "        print(rank_display.to_string(index=False, float_format='%.2f'))\n",
    "        \n",
    "        # Select top 3 models for ensemble\n",
    "        top_3_models = ranking_df.head(3)\n",
    "        \n",
    "        print(f\"\\n🎖️  TOP 3 MODELS SELECTED FOR ENSEMBLE:\")\n",
    "        for i, (idx, model) in enumerate(top_3_models.iterrows(), 1):\n",
    "            print(f\"   {i}. {model['model_name']}\")\n",
    "            print(f\"      📊 Overall MAPE: {model['overall_mape']:.2f}%\")\n",
    "            if 'low_mape' in model:\n",
    "                print(f\"      🔴 2-5L MAPE: {model['low_mape']:.2f}%\")\n",
    "            if 'training_time' in model:\n",
    "                print(f\"      ⏱️  Training Time: {model['training_time']:.1f}s\")\n",
    "            print(f\"      🏆 Composite Score: {model['composite_score']:.2f}\")\n",
    "            print()\n",
    "        \n",
    "        # Best individual model analysis\n",
    "        best_model = ranking_df.iloc[0]\n",
    "        print(f\"🥇 BEST INDIVIDUAL MODEL: {best_model['model_name']}\")\n",
    "        print(f\"   🎯 Achieves overall MAPE: {best_model['overall_mape']:.2f}%\")\n",
    "        \n",
    "        if best_model['overall_mape'] < 18:\n",
    "            print(f\"   ✅ SUCCESS: Achieved target <18% MAPE!\")\n",
    "        else:\n",
    "            gap = best_model['overall_mape'] - 18\n",
    "            print(f\"   ⚠️  Gap to target: +{gap:.2f}% (ensemble may close this gap)\")\n",
    "        \n",
    "        if 'low_mape' in best_model:\n",
    "            if best_model['low_mape'] < 25:\n",
    "                print(f\"   ✅ SUCCESS: 2-5L range improved to {best_model['low_mape']:.2f}%\")\n",
    "            else:\n",
    "                gap_low = best_model['low_mape'] - 25\n",
    "                print(f\"   ⚠️  2-5L gap to target: +{gap_low:.2f}%\")\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ No advanced models found for ranking\")\n",
    "    \n",
    "    # Visualization of model comparison\n",
    "    if len(results_summary) >= 3:\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        \n",
    "        # Overall MAPE comparison\n",
    "        plt.subplot(2, 3, 1)\n",
    "        models = results_summary['model_name']\n",
    "        overall_mapes = results_summary['overall_mape']\n",
    "        \n",
    "        colors = ['red' if mape >= 18 else 'green' for mape in overall_mapes]\n",
    "        bars = plt.bar(range(len(models)), overall_mapes, color=colors, alpha=0.7)\n",
    "        plt.axhline(y=18, color='red', linestyle='--', alpha=0.8, label='Target: 18%')\n",
    "        plt.xlabel('Models')\n",
    "        plt.ylabel('Overall MAPE (%)')\n",
    "        plt.title('Overall MAPE Comparison')\n",
    "        plt.xticks(range(len(models)), [m[:10] + '...' if len(m) > 10 else m for m in models], rotation=45)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2-5L MAPE comparison\n",
    "        if 'low_mape' in results_summary.columns:\n",
    "            plt.subplot(2, 3, 2)\n",
    "            low_mapes = results_summary['low_mape'].fillna(0)\n",
    "            colors_low = ['red' if mape >= 25 else 'green' for mape in low_mapes]\n",
    "            plt.bar(range(len(models)), low_mapes, color=colors_low, alpha=0.7)\n",
    "            plt.axhline(y=25, color='red', linestyle='--', alpha=0.8, label='Target: 25%')\n",
    "            plt.xlabel('Models')\n",
    "            plt.ylabel('2-5L MAPE (%)')\n",
    "            plt.title('2-5L Range MAPE Comparison')\n",
    "            plt.xticks(range(len(models)), [m[:10] + '...' if len(m) > 10 else m for m in models], rotation=45)\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Training time comparison\n",
    "        if 'training_time' in results_summary.columns:\n",
    "            plt.subplot(2, 3, 3)\n",
    "            times = results_summary['training_time'].fillna(0)\n",
    "            plt.bar(range(len(models)), times, alpha=0.7, color='blue')\n",
    "            plt.xlabel('Models')\n",
    "            plt.ylabel('Training Time (seconds)')\n",
    "            plt.title('Training Time Comparison')\n",
    "            plt.xticks(range(len(models)), [m[:10] + '...' if len(m) > 10 else m for m in models], rotation=45)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Range performance radar chart for top 3\n",
    "        if len(advanced_models) >= 3 and all(col in advanced_models.columns for col in ['low_mape', 'mid_mape', 'high_mape']):\n",
    "            plt.subplot(2, 3, 4)\n",
    "            top_3 = advanced_models.head(3)\n",
    "            \n",
    "            ranges = ['2-5L', '5-10L', '10L+']\n",
    "            angles = np.linspace(0, 2 * np.pi, len(ranges), endpoint=False)\n",
    "            angles = np.concatenate((angles, [angles[0]]))\n",
    "            \n",
    "            for i, (idx, model) in enumerate(top_3.iterrows()):\n",
    "                values = [model['low_mape'], model['mid_mape'], model['high_mape']]\n",
    "                values = np.concatenate((values, [values[0]]))\n",
    "                plt.polar(angles, values, 'o-', linewidth=2, label=model['model_name'][:15])\n",
    "            \n",
    "            plt.thetagrids(angles[:-1] * 180/np.pi, ranges)\n",
    "            plt.title('Top 3 Models: Range Performance')\n",
    "            plt.legend()\n",
    "        \n",
    "        # Performance improvement timeline\n",
    "        plt.subplot(2, 3, 5)\n",
    "        if 'timestamp' in results_summary.columns:\n",
    "            sorted_results = results_summary.sort_values('timestamp')\n",
    "            cumulative_best = sorted_results['overall_mape'].cummin()\n",
    "            plt.plot(range(len(cumulative_best)), cumulative_best, 'b-o', alpha=0.7)\n",
    "            plt.axhline(y=18, color='red', linestyle='--', alpha=0.8, label='Target: 18%')\n",
    "            plt.xlabel('Model Sequence')\n",
    "            plt.ylabel('Best MAPE So Far (%)')\n",
    "            plt.title('Performance Improvement Timeline')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Model efficiency scatter plot\n",
    "        if 'training_time' in results_summary.columns:\n",
    "            plt.subplot(2, 3, 6)\n",
    "            plt.scatter(results_summary['training_time'], results_summary['overall_mape'], \n",
    "                       alpha=0.7, s=100, c=range(len(results_summary)), cmap='viridis')\n",
    "            \n",
    "            for i, model in enumerate(results_summary['model_name']):\n",
    "                plt.annotate(model[:8], \n",
    "                           (results_summary.iloc[i]['training_time'], results_summary.iloc[i]['overall_mape']),\n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "            \n",
    "            plt.xlabel('Training Time (seconds)')\n",
    "            plt.ylabel('Overall MAPE (%)')\n",
    "            plt.title('Model Efficiency: MAPE vs Training Time')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✅ Model comparison and selection complete!\")\n",
    "    print(\"🎯 Ready for ensemble methods with top performing models\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No model results available for comparison\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 11: BEST MODEL DEEP ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"🔍 Deep Analysis of Best Performing Model\")\n",
    "print(\"🎯 Objective: Understand model behavior, strengths, and limitations\")\n",
    "\n",
    "# Get the best model\n",
    "best_model_info = model_tracker.get_best_model()\n",
    "\n",
    "if best_model_info is not None:\n",
    "    best_model_name = best_model_info['model_name']\n",
    "    best_overall_mape = best_model_info['overall_mape']\n",
    "    \n",
    "    print(f\"\\n🏆 ANALYZING BEST MODEL: {best_model_name}\")\n",
    "    print(f\"🎯 Overall MAPE: {best_overall_mape:.2f}%\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load the best model for detailed analysis\n",
    "    best_model = None\n",
    "    best_model_path = None\n",
    "    feature_importance_df = None\n",
    "    \n",
    "    # Determine model path and load\n",
    "    if 'LightGBM' in best_model_name:\n",
    "        best_model_path = MODELS_DIR / \"lightgbm_optimized.pkl\"\n",
    "        if best_model_path.exists():\n",
    "            with open(best_model_path, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "                best_model = model_data['model']\n",
    "                feature_importance_df = model_data.get('feature_importance', None)\n",
    "            \n",
    "            # Generate predictions for analysis\n",
    "            best_pred_log = best_model.predict(X_val, num_iteration=best_model.best_iteration)\n",
    "            \n",
    "    elif 'XGBoost' in best_model_name:\n",
    "        best_model_path = MODELS_DIR / \"xgboost_optimized.pkl\"\n",
    "        if best_model_path.exists():\n",
    "            with open(best_model_path, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "                best_model = model_data['model']\n",
    "                feature_importance_df = model_data.get('feature_importance', None)\n",
    "            \n",
    "            best_pred_log = best_model.predict(X_val)\n",
    "            \n",
    "    elif 'CatBoost' in best_model_name:\n",
    "        best_model_path = MODELS_DIR / \"catboost_optimized.cbm\"\n",
    "        metadata_path = MODELS_DIR / \"catboost_optimized_metadata.pkl\"\n",
    "        \n",
    "        if best_model_path.exists():\n",
    "            best_model = cb.CatBoostRegressor()\n",
    "            best_model.load_model(str(best_model_path))\n",
    "            \n",
    "            if metadata_path.exists():\n",
    "                with open(metadata_path, 'rb') as f:\n",
    "                    metadata = pickle.load(f)\n",
    "                    feature_importance_df = metadata.get('feature_importance', None)\n",
    "            \n",
    "            best_pred_log = best_model.predict(X_val)\n",
    "            \n",
    "    elif 'Extra Trees' in best_model_name:\n",
    "        best_model_path = MODELS_DIR / \"extratrees_optimized.pkl\"\n",
    "        if best_model_path.exists():\n",
    "            with open(best_model_path, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "                best_model = model_data['model']\n",
    "                feature_importance_df = model_data.get('feature_importance', None)\n",
    "            \n",
    "            best_pred_log = best_model.predict(X_val)\n",
    "    \n",
    "    if best_model is not None:\n",
    "        # Convert predictions to original scale\n",
    "        best_pred_orig = np.expm1(best_pred_log)\n",
    "        y_val_orig = np.expm1(y_val_log)\n",
    "        \n",
    "        # 1. DETAILED PERFORMANCE BREAKDOWN\n",
    "        print(\"📊 DETAILED PERFORMANCE BREAKDOWN\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Overall metrics\n",
    "        overall_mae = mean_absolute_error(y_val_orig, best_pred_orig)\n",
    "        overall_rmse = np.sqrt(mean_squared_error(y_val_orig, best_pred_orig))\n",
    "        overall_r2 = r2_score(y_val_orig, best_pred_orig)\n",
    "        \n",
    "        print(f\"Overall Performance:\")\n",
    "        print(f\"   🎯 MAPE: {best_overall_mape:.2f}%\")\n",
    "        print(f\"   📏 MAE:  ₹{overall_mae:,.0f}\")\n",
    "        print(f\"   📐 RMSE: ₹{overall_rmse:,.0f}\")\n",
    "        print(f\"   📈 R²:   {overall_r2:.4f}\")\n",
    "        \n",
    "        # Range-specific detailed analysis\n",
    "        print(f\"\\nIncome Range Analysis:\")\n",
    "        range_labels = {'low': '2-5L', 'mid': '5-10L', 'high': '10L+'}\n",
    "        range_details = {}\n",
    "        \n",
    "        for range_key, mask in income_ranges_val.items():\n",
    "            if mask.sum() > 0:\n",
    "                range_mape = calculate_mape(y_val_orig[mask], best_pred_orig[mask])\n",
    "                range_mae = mean_absolute_error(y_val_orig[mask], best_pred_orig[mask])\n",
    "                range_rmse = np.sqrt(mean_squared_error(y_val_orig[mask], best_pred_orig[mask]))\n",
    "                range_r2 = r2_score(y_val_orig[mask], best_pred_orig[mask])\n",
    "                range_count = mask.sum()\n",
    "                \n",
    "                range_details[range_key] = {\n",
    "                    'mape': range_mape, 'mae': range_mae, 'rmse': range_rmse,\n",
    "                    'r2': range_r2, 'count': range_count\n",
    "                }\n",
    "                \n",
    "                status = \"✅\" if range_mape < 20 else \"⚠️\" if range_mape < 25 else \"❌\"\n",
    "                print(f\"   {status} {range_labels[range_key]:>4}: {range_mape:>6.2f}% MAPE | ₹{range_mae:>8,.0f} MAE | R²={range_r2:>6.3f} | {range_count:>6,} samples\")\n",
    "        \n",
    "        # 2. FEATURE IMPORTANCE ANALYSIS\n",
    "        print(f\"\\n📊 FEATURE IMPORTANCE ANALYSIS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if feature_importance_df is not None:\n",
    "            top_20_features = feature_importance_df.head(20)\n",
    "            print(\"Top 20 Most Important Features:\")\n",
    "            \n",
    "            for i, (_, row) in enumerate(top_20_features.iterrows(), 1):\n",
    "                feature_name = row['feature']\n",
    "                importance = row['importance']\n",
    "                \n",
    "                # Categorize feature type\n",
    "                if any(keyword in feature_name.lower() for keyword in ['crop', 'production', 'yield']):\n",
    "                    category = \"🌾 Agricultural\"\n",
    "                elif any(keyword in feature_name.lower() for keyword in ['weather', 'temp', 'rain']):\n",
    "                    category = \"🌤️  Weather\"\n",
    "                elif any(keyword in feature_name.lower() for keyword in ['income', 'financial', 'profit']):\n",
    "                    category = \"💰 Financial\"\n",
    "                elif any(keyword in feature_name.lower() for keyword in ['geo', 'location', 'distance']):\n",
    "                    category = \"📍 Geographic\"\n",
    "                elif any(keyword in feature_name.lower() for keyword in ['age', 'education', 'family']):\n",
    "                    category = \"👥 Demographic\"\n",
    "                else:\n",
    "                    category = \"🔧 Engineered\"\n",
    "                \n",
    "                print(f\"   {i:>2}. {category} | {feature_name[:40]:.<40} {importance:>8.3f}\")\n",
    "            \n",
    "            # Feature category analysis\n",
    "            print(f\"\\nFeature Category Distribution (Top 20):\")\n",
    "            categories = {}\n",
    "            for _, row in top_20_features.iterrows():\n",
    "                feature_name = row['feature']\n",
    "                if any(keyword in feature_name.lower() for keyword in ['crop', 'production', 'yield']):\n",
    "                    cat = \"Agricultural\"\n",
    "                elif any(keyword in feature_name.lower() for keyword in ['weather', 'temp', 'rain']):\n",
    "                    cat = \"Weather\"\n",
    "                elif any(keyword in feature_name.lower() for keyword in ['income', 'financial', 'profit']):\n",
    "                    cat = \"Financial\"\n",
    "                elif any(keyword in feature_name.lower() for keyword in ['geo', 'location', 'distance']):\n",
    "                    cat = \"Geographic\"\n",
    "                elif any(keyword in feature_name.lower() for keyword in ['age', 'education', 'family']):\n",
    "                    cat = \"Demographic\"\n",
    "                else:\n",
    "                    cat = \"Engineered\"\n",
    "                \n",
    "                categories[cat] = categories.get(cat, 0) + 1\n",
    "            \n",
    "            for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"   🔹 {cat}: {count} features\")\n",
    "        \n",
    "        # 3. ERROR ANALYSIS BY SEGMENTS\n",
    "        print(f\"\\n📊 ERROR ANALYSIS BY SEGMENTS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Calculate errors\n",
    "        errors = np.abs(y_val_orig - best_pred_orig)\n",
    "        percentage_errors = errors / y_val_orig * 100\n",
    "        \n",
    "        # Error quantile analysis\n",
    "        print(\"Error Distribution Analysis:\")\n",
    "        percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "        print(\"   📊 Percentage Error Percentiles:\")\n",
    "        for p in percentiles:\n",
    "            value = np.percentile(percentage_errors, p)\n",
    "            print(f\"      P{p:>2}: {value:>6.2f}%\")\n",
    "        \n",
    "        # High error cases analysis\n",
    "        high_error_threshold = np.percentile(percentage_errors, 95)\n",
    "        high_error_mask = percentage_errors > high_error_threshold\n",
    "        \n",
    "        print(f\"\\n🔍 High Error Cases Analysis (>{high_error_threshold:.1f}% error):\")\n",
    "        print(f\"   📊 Total high error cases: {high_error_mask.sum():,} ({high_error_mask.sum()/len(y_val_orig)*100:.1f}%)\")\n",
    "        \n",
    "        # Analyze high error cases by income range\n",
    "        for range_key, range_mask in income_ranges_val.items():\n",
    "            if range_mask.sum() > 0:\n",
    "                high_error_in_range = (high_error_mask & range_mask).sum()\n",
    "                range_total = range_mask.sum()\n",
    "                pct_high_error = high_error_in_range / range_total * 100\n",
    "                print(f\"   🔹 {range_labels[range_key]:>4}: {high_error_in_range:>4,}/{range_total:>5,} ({pct_high_error:>5.1f}%) high error cases\")\n",
    "        \n",
    "        # 4. PREDICTION PATTERN ANALYSIS\n",
    "        print(f\"\\n📊 PREDICTION PATTERN ANALYSIS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Bias analysis\n",
    "        bias = np.mean(best_pred_orig - y_val_orig)\n",
    "        abs_bias = np.mean(np.abs(best_pred_orig - y_val_orig))\n",
    "        \n",
    "        print(f\"Prediction Bias Analysis:\")\n",
    "        print(f\"   🎯 Mean Bias: ₹{bias:,.0f} ({'Over' if bias > 0 else 'Under'}prediction)\")\n",
    "        print(f\"   📏 Mean Absolute Bias: ₹{abs_bias:,.0f}\")\n",
    "        \n",
    "        # Range-specific bias\n",
    "        print(f\"\\nBias by Income Range:\")\n",
    "        for range_key, mask in income_ranges_val.items():\n",
    "            if mask.sum() > 0:\n",
    "                range_bias = np.mean(best_pred_orig[mask] - y_val_orig[mask])\n",
    "                direction = \"Over\" if range_bias > 0 else \"Under\"\n",
    "                print(f\"   🔹 {range_labels[range_key]:>4}: ₹{range_bias:>8,.0f} ({direction}prediction)\")\n",
    "        \n",
    "        # 5. MODEL STRENGTHS AND LIMITATIONS\n",
    "        print(f\"\\n📊 MODEL STRENGTHS AND LIMITATIONS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        print(\"🟢 STRENGTHS:\")\n",
    "        \n",
    "        # Identify strengths based on performance\n",
    "        if best_overall_mape < 18:\n",
    "            print(\"   ✅ Achieves target overall MAPE < 18%\")\n",
    "        \n",
    "        best_range_performance = min([range_details[k]['mape'] for k in range_details.keys()])\n",
    "        if best_range_performance < 20:\n",
    "            print(\"   ✅ Strong performance in at least one income range\")\n",
    "        \n",
    "        if overall_r2 > 0.8:\n",
    "            print(\"   ✅ High R² indicates good variance explanation\")\n",
    "        \n",
    "        if 'low' in range_details and range_details['low']['mape'] < 30:\n",
    "            print(\"   ✅ Improved 2-5L range performance from baseline\")\n",
    "        \n",
    "        # Analyze prediction consistency\n",
    "        cv_std = np.std([range_details[k]['mape'] for k in range_details.keys()])\n",
    "        if cv_std < 5:\n",
    "            print(\"   ✅ Consistent performance across income ranges\")\n",
    "        \n",
    "        print(\"\\n🔴 LIMITATIONS:\")\n",
    "        \n",
    "        # Identify limitations\n",
    "        if best_overall_mape >= 18:\n",
    "            gap = best_overall_mape - 18\n",
    "            print(f\"   ⚠️  Overall MAPE {gap:.2f}% above target\")\n",
    "        \n",
    "        if 'low' in range_details and range_details['low']['mape'] > 25:\n",
    "            gap_low = range_details['low']['mape'] - 25\n",
    "            print(f\"   ⚠️  2-5L range MAPE {gap_low:.2f}% above target\")\n",
    "        \n",
    "        if bias > 50000:  # Significant bias\n",
    "            print(f\"   ⚠️  Systematic overprediction bias of ₹{bias:,.0f}\")\n",
    "        elif bias < -50000:\n",
    "            print(f\"   ⚠️  Systematic underprediction bias of ₹{abs(bias):,.0f}\")\n",
    "        \n",
    "        high_error_rate = high_error_mask.sum() / len(y_val_orig) * 100\n",
    "        if high_error_rate > 10:\n",
    "            print(f\"   ⚠️  High error rate: {high_error_rate:.1f}% of cases have >{high_error_threshold:.1f}% error\")\n",
    "        \n",
    "        # 6. VISUALIZATION\n",
    "        plt.figure(figsize=(20, 12))\n",
    "        \n",
    "        # Predictions vs Actual by range\n",
    "        plt.subplot(2, 4, 1)\n",
    "        colors = {'low': 'red', 'mid': 'blue', 'high': 'green'}\n",
    "        for range_key, mask in income_ranges_val.items():\n",
    "            if mask.sum() > 0:\n",
    "                y_true_millions = y_val_orig[mask] / 1000000\n",
    "                y_pred_millions = best_pred_orig[mask] / 1000000\n",
    "                plt.scatter(y_true_millions, y_pred_millions, \n",
    "                           alpha=0.6, c=colors[range_key], label=range_labels[range_key], s=15)\n",
    "        \n",
    "        max_val = max(y_val_orig.max(), best_pred_orig.max()) / 1000000\n",
    "        min_val = min(y_val_orig.min(), best_pred_orig.min()) / 1000000\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=2)\n",
    "        plt.xlabel('Actual Income (₹ Millions)')\n",
    "        plt.ylabel('Predicted Income (₹ Millions)')\n",
    "        plt.title(f'{best_model_name}: Predictions vs Actual')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Error distribution\n",
    "        plt.subplot(2, 4, 2)\n",
    "        plt.hist(percentage_errors, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        plt.axvline(best_overall_mape, color='red', linestyle='--', label=f'Mean: {best_overall_mape:.1f}%')\n",
    "        plt.axvline(18, color='green', linestyle='--', label='Target: 18%')\n",
    "        plt.xlabel('Absolute Percentage Error (%)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Error Distribution')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Residuals plot\n",
    "        plt.subplot(2, 4, 3)\n",
    "        residuals = best_pred_orig - y_val_orig\n",
    "        plt.scatter(best_pred_orig/1000000, residuals/1000000, alpha=0.6, s=15)\n",
    "        plt.axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "        plt.xlabel('Predicted Income (₹ Millions)')\n",
    "        plt.ylabel('Residuals (₹ Millions)')\n",
    "        plt.title('Residuals Plot')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Feature importance\n",
    "        if feature_importance_df is not None:\n",
    "            plt.subplot(2, 4, 4)\n",
    "            top_10 = feature_importance_df.head(10)\n",
    "            plt.barh(range(len(top_10)), top_10['importance'])\n",
    "            plt.yticks(range(len(top_10)), [f[:20] + '...' if len(f) > 20 else f for f in top_10['feature']])\n",
    "            plt.xlabel('Importance')\n",
    "            plt.title('Top 10 Feature Importance')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Error by income range\n",
    "        plt.subplot(2, 4, 5)\n",
    "        range_mapes = [range_details[k]['mape'] for k in ['low', 'mid', 'high']]\n",
    "        range_names = ['2-5L', '5-10L', '10L+']\n",
    "        colors_bar = ['red' if x > 20 else 'green' for x in range_mapes]\n",
    "        bars = plt.bar(range_names, range_mapes, color=colors_bar, alpha=0.7)\n",
    "        plt.axhline(y=18, color='green', linestyle='--', alpha=0.8, label='Overall Target')\n",
    "        plt.axhline(y=25, color='orange', linestyle='--', alpha=0.8, label='2-5L Target')\n",
    "        plt.ylabel('MAPE (%)')\n",
    "        plt.title('MAPE by Income Range')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Prediction accuracy by actual income\n",
    "        plt.subplot(2, 4, 6)\n",
    "        income_bins_viz = np.linspace(y_val_orig.min(), y_val_orig.max(), 20)\n",
    "        bin_centers = (income_bins_viz[:-1] + income_bins_viz[1:]) / 2\n",
    "        bin_mapes = []\n",
    "        \n",
    "        for i in range(len(income_bins_viz)-1):\n",
    "            mask = (y_val_orig >= income_bins_viz[i]) & (y_val_orig < income_bins_viz[i+1])\n",
    "            if mask.sum() > 10:  # Enough samples\n",
    "                bin_mape = calculate_mape(y_val_orig[mask], best_pred_orig[mask])\n",
    "                bin_mapes.append(bin_mape)\n",
    "            else:\n",
    "                bin_mapes.append(np.nan)\n",
    "        \n",
    "        valid_bins = ~np.isnan(bin_mapes)\n",
    "        plt.plot(bin_centers[valid_bins]/1000000, np.array(bin_mapes)[valid_bins], 'b-o', alpha=0.7)\n",
    "        plt.axhline(y=18, color='red', linestyle='--', alpha=0.8, label='Target: 18%')\n",
    "        plt.xlabel('Actual Income (₹ Millions)')\n",
    "        plt.ylabel('MAPE (%)')\n",
    "        plt.title('Accuracy vs Income Level')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Training time vs performance comparison\n",
    "        plt.subplot(2, 4, 7)\n",
    "        if len(model_tracker.results) > 1:\n",
    "            all_times = [r.get('training_time', 0) for r in model_tracker.results]\n",
    "            all_mapes = [r['overall_mape'] for r in model_tracker.results]\n",
    "            all_names = [r['model_name'] for r in model_tracker.results]\n",
    "            \n",
    "            plt.scatter(all_times, all_mapes, alpha=0.7, s=100)\n",
    "            \n",
    "            # Highlight best model\n",
    "            best_idx = all_names.index(best_model_name)\n",
    "            plt.scatter(all_times[best_idx], all_mapes[best_idx], \n",
    "                       color='red', s=200, marker='*', label='Best Model')\n",
    "            \n",
    "            plt.xlabel('Training Time (seconds)')\n",
    "            plt.ylabel('Overall MAPE (%)')\n",
    "            plt.title('Efficiency: Performance vs Time')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Model confidence intervals (if applicable)\n",
    "        plt.subplot(2, 4, 8)\n",
    "        sorted_indices = np.argsort(y_val_orig)\n",
    "        sorted_actual = y_val_orig[sorted_indices]\n",
    "        sorted_pred = best_pred_orig[sorted_indices]\n",
    "        \n",
    "        # Calculate moving average for smoothing\n",
    "        window = len(sorted_actual) // 50\n",
    "        if window > 1:\n",
    "            smooth_actual = np.convolve(sorted_actual, np.ones(window)/window, mode='valid')\n",
    "            smooth_pred = np.convolve(sorted_pred, np.ones(window)/window, mode='valid')\n",
    "            \n",
    "            plt.plot(smooth_actual/1000000, smooth_pred/1000000, 'b-', alpha=0.8, label='Predicted')\n",
    "            plt.plot(smooth_actual/1000000, smooth_actual/1000000, 'r--', alpha=0.8, label='Perfect')\n",
    "            plt.xlabel('Actual Income (₹ Millions)')\n",
    "            plt.ylabel('Predicted Income (₹ Millions)')\n",
    "            plt.title('Prediction Smoothness')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 7. BUSINESS INSIGHTS\n",
    "        print(f\"\\n💼 BUSINESS INSIGHTS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if feature_importance_df is not None:\n",
    "            top_5_features = feature_importance_df.head(5)['feature'].tolist()\n",
    "            \n",
    "            print(\"Key Predictive Factors:\")\n",
    "            for i, feature in enumerate(top_5_features, 1):\n",
    "                if 'crop' in feature.lower() or 'production' in feature.lower():\n",
    "                    insight = \"Agricultural productivity is a key income driver\"\n",
    "                elif 'weather' in feature.lower() or 'rain' in feature.lower():\n",
    "                    insight = \"Weather patterns significantly impact farmer income\"\n",
    "                elif 'financial' in feature.lower() or 'income' in feature.lower():\n",
    "                    insight = \"Financial diversification affects income stability\"\n",
    "                elif 'geo' in feature.lower() or 'location' in feature.lower():\n",
    "                    insight = \"Geographic location influences income potential\"\n",
    "                else:\n",
    "                    insight = \"This factor contributes to income prediction\"\n",
    "                \n",
    "                print(f\"   {i}. {feature[:30]:<30} → {insight}\")\n",
    "        \n",
    "        print(f\"\\nRecommendations for Income Improvement:\")\n",
    "        \n",
    "        if 'low' in range_details and range_details['low']['mape'] > 25:\n",
    "            print(\"   🎯 Focus interventions on 2-5L income farmers (highest prediction error)\")\n",
    "        \n",
    "        print(\"   🌾 Enhance agricultural productivity programs\")\n",
    "        print(\"   🌤️  Improve weather resilience and forecasting\")\n",
    "        print(\"   💰 Promote income diversification strategies\")\n",
    "        print(\"   📍 Target location-specific interventions\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"✅ Deep analysis of {best_model_name} complete!\")\n",
    "        print(\"🎯 Model ready for ensemble combination and final predictions\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ Could not load best model from {best_model_path}\")\n",
    "        print(\"   Please ensure model files are properly saved\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No best model found for analysis\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b2f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 12: FINAL VALIDATION & TEST PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"🎯 Final Validation & Test Predictions Generation\")\n",
    "print(\"🏆 Objective: Validate best models and generate submission predictions\")\n",
    "\n",
    "# Get top 3 models for final validation\n",
    "results_summary = model_tracker.get_summary()\n",
    "if results_summary is not None and len(results_summary) > 0:\n",
    "    \n",
    "    # Filter advanced models and get top 3\n",
    "    advanced_models = results_summary[\n",
    "        ~results_summary['model_name'].str.contains('Baseline|Mean|Median', case=False, na=False)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(advanced_models) >= 3:\n",
    "        top_3_models = advanced_models.head(3)\n",
    "        model_names = top_3_models['model_name'].tolist()\n",
    "        \n",
    "        print(f\"\\n🏆 TOP 3 MODELS FOR FINAL VALIDATION:\")\n",
    "        for i, name in enumerate(model_names, 1):\n",
    "            mape = top_3_models.iloc[i-1]['overall_mape']\n",
    "            print(f\"   {i}. {name}: {mape:.2f}% MAPE\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        \n",
    "        # Load models and generate predictions\n",
    "        final_models = {}\n",
    "        final_predictions_val = {}\n",
    "        final_predictions_test = {}\n",
    "        \n",
    "        print(\"📊 LOADING MODELS AND GENERATING PREDICTIONS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            print(f\"\\n🔄 Processing {model_name}...\")\n",
    "            \n",
    "            try:\n",
    "                if 'LightGBM' in model_name:\n",
    "                    model_path = MODELS_DIR / \"lightgbm_optimized.pkl\"\n",
    "                    with open(model_path, 'rb') as f:\n",
    "                        model_data = pickle.load(f)\n",
    "                        model = model_data['model']\n",
    "                    \n",
    "                    pred_val_log = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "                    pred_test_log = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "                    \n",
    "                elif 'XGBoost' in model_name:\n",
    "                    model_path = MODELS_DIR / \"xgboost_optimized.pkl\"\n",
    "                    with open(model_path, 'rb') as f:\n",
    "                        model_data = pickle.load(f)\n",
    "                        model = model_data['model']\n",
    "                    \n",
    "                    pred_val_log = model.predict(X_val)\n",
    "                    pred_test_log = model.predict(X_test)\n",
    "                    \n",
    "                elif 'CatBoost' in model_name:\n",
    "                    model_path = MODELS_DIR / \"catboost_optimized.cbm\"\n",
    "                    model = cb.CatBoostRegressor()\n",
    "                    model.load_model(str(model_path))\n",
    "                    \n",
    "                    pred_val_log = model.predict(X_val)\n",
    "                    pred_test_log = model.predict(X_test)\n",
    "                    \n",
    "                elif 'Extra Trees' in model_name:\n",
    "                    model_path = MODELS_DIR / \"extratrees_optimized.pkl\"\n",
    "                    with open(model_path, 'rb') as f:\n",
    "                        model_data = pickle.load(f)\n",
    "                        model = model_data['model']\n",
    "                    \n",
    "                    pred_val_log = model.predict(X_val)\n",
    "                    pred_test_log = model.predict(X_test)\n",
    "                \n",
    "                # Store models and predictions\n",
    "                final_models[model_name] = model\n",
    "                final_predictions_val[model_name] = np.expm1(pred_val_log)  # Convert to original scale\n",
    "                final_predictions_test[model_name] = np.expm1(pred_test_log)  # Convert to original scale\n",
    "                \n",
    "                # Validate predictions\n",
    "                val_mape = calculate_mape(np.expm1(y_val_log), final_predictions_val[model_name])\n",
    "                print(f\"   ✅ Validation MAPE: {val_mape:.2f}%\")\n",
    "                print(f\"   📊 Test predictions range: ₹{final_predictions_test[model_name].min():,.0f} - ₹{final_predictions_test[model_name].max():,.0f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error loading {model_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n✅ Successfully loaded {len(final_models)} models\")\n",
    "        \n",
    "        # CROSS-VALIDATION ON BEST MODEL\n",
    "        print(\"\\n📊 CROSS-VALIDATION ON BEST MODEL\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        best_model_name = model_names[0]  # Top model\n",
    "        if best_model_name in final_models:\n",
    "            print(f\"🔄 Performing 5-fold CV on {best_model_name}...\")\n",
    "            \n",
    "            cv_scores = []\n",
    "            cv_range_scores = {'low': [], 'mid': [], 'high': []}\n",
    "            \n",
    "            for fold, (train_idx, val_idx) in enumerate(cv_splitter.split(X_train, income_bins), 1):\n",
    "                X_cv_train, X_cv_val = X_train[train_idx], X_train[val_idx]\n",
    "                y_cv_train, y_cv_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "                weights_cv_train = sample_weights[train_idx]\n",
    "                \n",
    "                # Train model on fold\n",
    "                if 'LightGBM' in best_model_name:\n",
    "                    train_data = lgb.Dataset(X_cv_train, label=y_cv_train, weight=weights_cv_train)\n",
    "                    cv_model = lgb.train(\n",
    "                        final_models[best_model_name].params,\n",
    "                        train_data,\n",
    "                        num_boost_round=1000,\n",
    "                        valid_sets=[train_data],\n",
    "                        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
    "                    )\n",
    "                    cv_pred_log = cv_model.predict(X_cv_val, num_iteration=cv_model.best_iteration)\n",
    "                    \n",
    "                elif 'XGBoost' in best_model_name:\n",
    "                    cv_model = xgb.XGBRegressor(**final_models[best_model_name].get_params())\n",
    "                    cv_model.fit(X_cv_train, y_cv_train, sample_weight=weights_cv_train,\n",
    "                               eval_set=[(X_cv_train, y_cv_train)], early_stopping_rounds=100, verbose=False)\n",
    "                    cv_pred_log = cv_model.predict(X_cv_val)\n",
    "                    \n",
    "                elif 'CatBoost' in best_model_name:\n",
    "                    train_pool = cb.Pool(X_cv_train, y_cv_train, weight=weights_cv_train)\n",
    "                    cv_model = cb.CatBoostRegressor(**final_models[best_model_name].get_all_params())\n",
    "                    cv_model.fit(train_pool, verbose=False)\n",
    "                    cv_pred_log = cv_model.predict(X_cv_val)\n",
    "                    \n",
    "                elif 'Extra Trees' in best_model_name:\n",
    "                    cv_model = ExtraTreesRegressor(**final_models[best_model_name].get_params())\n",
    "                    cv_model.fit(X_cv_train, y_cv_train, sample_weight=weights_cv_train)\n",
    "                    cv_pred_log = cv_model.predict(X_cv_val)\n",
    "                \n",
    "                # Evaluate fold\n",
    "                cv_true_orig = np.expm1(y_cv_val)\n",
    "                cv_pred_orig = np.expm1(cv_pred_log)\n",
    "                fold_mape = calculate_mape(cv_true_orig, cv_pred_orig)\n",
    "                cv_scores.append(fold_mape)\n",
    "                \n",
    "                # Range-specific evaluation\n",
    "                for range_key, range_mask_full in income_ranges.items():\n",
    "                    # Map range mask to current fold\n",
    "                    range_mask_fold = range_mask_full[val_idx]\n",
    "                    if range_mask_fold.sum() > 0:\n",
    "                        range_mape = calculate_mape(cv_true_orig[range_mask_fold], cv_pred_orig[range_mask_fold])\n",
    "                        cv_range_scores[range_key].append(range_mape)\n",
    "                \n",
    "                print(f\"   Fold {fold}: {fold_mape:.2f}% MAPE\")\n",
    "            \n",
    "            # CV Results Summary\n",
    "            cv_mean = np.mean(cv_scores)\n",
    "            cv_std = np.std(cv_scores)\n",
    "            \n",
    "            print(f\"\\n📊 Cross-Validation Results:\")\n",
    "            print(f\"   🎯 Mean MAPE: {cv_mean:.2f} ± {cv_std:.2f}%\")\n",
    "            print(f\"   📊 Individual folds: {[f'{score:.2f}%' for score in cv_scores]}\")\n",
    "            \n",
    "            # Range-specific CV results\n",
    "            print(f\"\\n🎯 Range-Specific CV Results:\")\n",
    "            range_labels = {'low': '2-5L', 'mid': '5-10L', 'high': '10L+'}\n",
    "            for range_key, scores in cv_range_scores.items():\n",
    "                if scores:\n",
    "                    range_mean = np.mean(scores)\n",
    "                    range_std = np.std(scores)\n",
    "                    print(f\"   🔹 {range_labels[range_key]:>4}: {range_mean:.2f} ± {range_std:.2f}%\")\n",
    "            \n",
    "            # Statistical significance test\n",
    "            from scipy import stats\n",
    "            if len(cv_scores) >= 3:\n",
    "                # Test if significantly different from 18%\n",
    "                t_stat, p_value = stats.ttest_1samp(cv_scores, 18)\n",
    "                print(f\"\\n📈 Statistical Analysis:\")\n",
    "                print(f\"   🧮 T-test vs 18% target: t={t_stat:.3f}, p={p_value:.3f}\")\n",
    "                if p_value < 0.05:\n",
    "                    if cv_mean < 18:\n",
    "                        print(f\"   ✅ Significantly better than 18% target (p<0.05)\")\n",
    "                    else:\n",
    "                        print(f\"   ⚠️  Significantly worse than 18% target (p<0.05)\")\n",
    "                else:\n",
    "                    print(f\"   📊 No significant difference from 18% target (p≥0.05)\")\n",
    "        \n",
    "        # ENSEMBLE PREDICTIONS\n",
    "        print(\"\\n🤖 SIMPLE ENSEMBLE PREDICTIONS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if len(final_predictions_val) >= 2:\n",
    "            # Simple average ensemble\n",
    "            ensemble_val = np.mean(list(final_predictions_val.values()), axis=0)\n",
    "            ensemble_test = np.mean(list(final_predictions_test.values()), axis=0)\n",
    "            \n",
    "            # Evaluate ensemble\n",
    "            ensemble_mape = calculate_mape(np.expm1(y_val_log), ensemble_val)\n",
    "            \n",
    "            print(f\"📊 Simple Average Ensemble:\")\n",
    "            print(f\"   🎯 Validation MAPE: {ensemble_mape:.2f}%\")\n",
    "            \n",
    "            # Range-specific ensemble evaluation\n",
    "            ensemble_range_metrics = {}\n",
    "            for range_key, mask in income_ranges_val.items():\n",
    "                if mask.sum() > 0:\n",
    "                    range_mape = calculate_mape(np.expm1(y_val_log)[mask], ensemble_val[mask])\n",
    "                    ensemble_range_metrics[range_key] = range_mape\n",
    "                    print(f\"   🔹 {range_labels[range_key]:>4}: {range_mape:.2f}% MAPE\")\n",
    "            \n",
    "            # Store ensemble results\n",
    "            final_predictions_val['Simple Ensemble'] = ensemble_val\n",
    "            final_predictions_test['Simple Ensemble'] = ensemble_test\n",
    "            \n",
    "            # Track ensemble performance\n",
    "            model_tracker.add_result(\"Simple Ensemble\", ensemble_mape, ensemble_range_metrics)\n",
    "        \n",
    "        # GENERATE FINAL SUBMISSION FILES\n",
    "        print(\"\\n💾 GENERATING SUBMISSION FILES\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        submissions_dir = RESULTS_DIR / \"submissions\"\n",
    "        submissions_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        for model_name, test_predictions in final_predictions_test.items():\n",
    "            # Create submission dataframe\n",
    "            submission_df = pd.DataFrame({\n",
    "                'id': range(len(test_predictions)),\n",
    "                'predicted_income': test_predictions.round(2)\n",
    "            })\n",
    "            \n",
    "            # Save submission file\n",
    "            filename = f\"submission_{model_name.lower().replace(' ', '_')}.csv\"\n",
    "            file_path = submissions_dir / filename\n",
    "            submission_df.to_csv(file_path, index=False)\n",
    "            \n",
    "            print(f\"   📄 {model_name}: {file_path}\")\n",
    "            print(f\"      📊 Predictions range: ₹{test_predictions.min():,.0f} - ₹{test_predictions.max():,.0f}\")\n",
    "            print(f\"      📈 Mean prediction: ₹{test_predictions.mean():,.0f}\")\n",
    "        \n",
    "        # FINAL RECOMMENDATIONS\n",
    "        print(\"\\n🏆 FINAL MODEL RECOMMENDATIONS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Get final best model\n",
    "        final_best = model_tracker.get_best_model()\n",
    "        if final_best:\n",
    "            print(f\"🥇 RECOMMENDED MODEL: {final_best['model_name']}\")\n",
    "            print(f\"   🎯 Overall MAPE: {final_best['overall_mape']:.2f}%\")\n",
    "            \n",
    "            if final_best['overall_mape'] < 18:\n",
    "                print(f\"   ✅ MISSION ACCOMPLISHED: Target <18% MAPE achieved!\")\n",
    "            else:\n",
    "                gap = final_best['overall_mape'] - 18\n",
    "                print(f\"   📊 Gap to target: +{gap:.2f}% MAPE\")\n",
    "            \n",
    "            # 2-5L range assessment\n",
    "            if 'low_mape' in final_best and final_best['low_mape'] is not None:\n",
    "                print(f\"   🔴 2-5L range MAPE: {final_best['low_mape']:.2f}%\")\n",
    "                if final_best['low_mape'] < 25:\n",
    "                    print(f\"   ✅ 2-5L range target achieved!\")\n",
    "                else:\n",
    "                    gap_low = final_best['low_mape'] - 25\n",
    "                    print(f\"   📊 2-5L gap to target: +{gap_low:.2f}%\")\n",
    "        \n",
    "        print(f\"\\n📋 SUBMISSION STRATEGY:\")\n",
    "        print(f\"   🎯 Primary: Best individual model ({final_best['model_name'] if final_best else 'Unknown'})\")\n",
    "        if 'Simple Ensemble' in final_predictions_test:\n",
    "            ensemble_performance = [r for r in model_tracker.results if r['model_name'] == 'Simple Ensemble']\n",
    "            if ensemble_performance:\n",
    "                ensemble_mape = ensemble_performance[0]['overall_mape']\n",
    "                print(f\"   🤖 Alternative: Simple ensemble ({ensemble_mape:.2f}% MAPE)\")\n",
    "        print(f\"   📈 Backup: Top 3 individual models available\")\n",
    "        \n",
    "        # FINAL STATISTICS\n",
    "        print(f\"\\n📊 FINAL EXPERIMENT STATISTICS\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"   🔢 Total models trained: {len(model_tracker.results)}\")\n",
    "        print(f\"   ⏱️  Total training time: {sum(r.get('training_time', 0) for r in model_tracker.results):.1f} seconds\")\n",
    "        print(f\"   🎯 Models achieving <18% MAPE: {sum(1 for r in model_tracker.results if r['overall_mape'] < 18)}\")\n",
    "        print(f\"   🔴 Models achieving <25% 2-5L MAPE: {sum(1 for r in model_tracker.results if r.get('low_mape', float('inf')) < 25)}\")\n",
    "        \n",
    "        # Success assessment\n",
    "        success_criteria = {\n",
    "            'target_mape': final_best['overall_mape'] < 18 if final_best else False,\n",
    "            'range_improvement': final_best.get('low_mape', 50) < 35 if final_best else False,  # Significant improvement from 41%\n",
    "            'models_trained': len(model_tracker.results) >= 8,\n",
    "            'ensemble_created': 'Simple Ensemble' in final_predictions_test\n",
    "        }\n",
    "        \n",
    "        success_count = sum(success_criteria.values())\n",
    "        total_criteria = len(success_criteria)\n",
    "        \n",
    "        print(f\"\\n🏆 SUCCESS ASSESSMENT: {success_count}/{total_criteria} criteria met\")\n",
    "        for criterion, achieved in success_criteria.items():\n",
    "            status = \"✅\" if achieved else \"❌\"\n",
    "            print(f\"   {status} {criterion.replace('_', ' ').title()}\")\n",
    "        \n",
    "        if success_count >= 3:\n",
    "            print(f\"\\n🎉 EXPERIMENT SUCCESSFUL!\")\n",
    "            print(f\"   🚀 Ready for production deployment\")\n",
    "        else:\n",
    "            print(f\"\\n📊 EXPERIMENT PARTIALLY SUCCESSFUL\")\n",
    "            print(f\"   🔧 Consider advanced ensemble methods in next notebook\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Insufficient advanced models for final validation\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No model results available for final validation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ MODELING EXPERIMENTS COMPLETE!\")\n",
    "print(\"🎯 All predictions generated and saved\")\n",
    "print(\"📁 Check results/submissions/ for submission files\")\n",
    "print(\"🚀 Proceed to Notebook 5 for advanced ensemble methods\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
