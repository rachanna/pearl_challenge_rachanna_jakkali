{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6932f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 04_MODELING_EXPERIMENTS.IPYNB\n",
    "# L&T Finance Pearl Challenge - Farmer Income Prediction\n",
    "# Objective: Achieve MAPE < 18% with focus on income range balance\n",
    "# =============================================================================\n",
    "\n",
    "# CHUNK 1: INITIAL SETUP & IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"üöÄ L&T Finance Pearl Challenge - Modeling Experiments\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÖ Execution started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üéØ Objective: Achieve MAPE < 18% with income range optimization\")\n",
    "print(f\"üé≤ Random State: {RANDOM_STATE}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Core ML Libraries\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score, StratifiedKFold, KFold, \n",
    "    GridSearchCV, RandomizedSearchCV\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_percentage_error, mean_absolute_error, \n",
    "    mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"‚úÖ Core ML libraries imported successfully\")\n",
    "\n",
    "# Advanced ML Libraries  \n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    print(\"‚úÖ LightGBM imported successfully\")\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ùå LightGBM not available\")\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"‚úÖ XGBoost imported successfully\") \n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ùå XGBoost not available\")\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    print(\"‚úÖ CatBoost imported successfully\")\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ùå CatBoost not available\")\n",
    "    CATBOOST_AVAILABLE = False\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "try:\n",
    "    import optuna\n",
    "    print(\"‚úÖ Optuna imported successfully\")\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    # Suppress optuna logs for cleaner output\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "except ImportError:\n",
    "    print(\"‚ùå Optuna not available - using GridSearchCV fallback\")\n",
    "    OPTUNA_AVAILABLE = False\n",
    "\n",
    "print(\"\\nüìã Library Availability Summary:\")\n",
    "print(f\"   üîπ LightGBM: {'‚úÖ' if LIGHTGBM_AVAILABLE else '‚ùå'}\")\n",
    "print(f\"   üîπ XGBoost: {'‚úÖ' if XGBOOST_AVAILABLE else '‚ùå'}\")\n",
    "print(f\"   üîπ CatBoost: {'‚úÖ' if CATBOOST_AVAILABLE else '‚ùå'}\")\n",
    "print(f\"   üîπ Optuna: {'‚úÖ' if OPTUNA_AVAILABLE else '‚ùå'}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b78f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 2: DIRECTORY SETUP & DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "# Set up project directories\n",
    "BASE_DIR = Path(\"../\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "ENGINEERED_DIR = DATA_DIR / \"feature_engineered\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üìÅ Directory Structure:\")\n",
    "print(f\"   üîπ Base Directory: {BASE_DIR}\")\n",
    "print(f\"   üîπ Data Directory: {ENGINEERED_DIR}\")\n",
    "print(f\"   üîπ Models Directory: {MODELS_DIR}\")\n",
    "print(f\"   üîπ Results Directory: {RESULTS_DIR}\")\n",
    "\n",
    "# Verify required files exist\n",
    "required_files = [\n",
    "    \"X_train_eng.npy\", \"X_val_eng.npy\", \"X_test_eng.npy\",\n",
    "    \"y_train_eng.npy\", \"y_val_eng.npy\", \"feature_metadata.json\"\n",
    "]\n",
    "\n",
    "print(\"\\nüîç Checking required files:\")\n",
    "missing_files = []\n",
    "for file in required_files:\n",
    "    file_path = ENGINEERED_DIR / file\n",
    "    if file_path.exists():\n",
    "        print(f\"   ‚úÖ {file}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {file} - MISSING!\")\n",
    "        missing_files.append(file)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n‚ùå ERROR: Missing files: {missing_files}\")\n",
    "    print(\"Please run preprocessing notebooks first!\")\n",
    "    raise FileNotFoundError(\"Required preprocessed files not found\")\n",
    "\n",
    "print(\"\\nüìä Loading preprocessed datasets...\")\n",
    "\n",
    "# Load training and validation data\n",
    "print(\"   üîÑ Loading X_train...\")\n",
    "X_train = np.load(ENGINEERED_DIR / \"X_train_eng.npy\").astype(np.float32)\n",
    "print(\"   üîÑ Loading X_val...\")\n",
    "X_val = np.load(ENGINEERED_DIR / \"X_val_eng.npy\").astype(np.float32)\n",
    "print(\"   üîÑ Loading X_test...\")\n",
    "X_test = np.load(ENGINEERED_DIR / \"X_test_eng.npy\").astype(np.float32)\n",
    "print(\"   üîÑ Loading y_train...\")\n",
    "y_train = np.load(ENGINEERED_DIR / \"y_train_eng.npy\").astype(np.float32)\n",
    "print(\"   üîÑ Loading y_val...\")\n",
    "y_val = np.load(ENGINEERED_DIR / \"y_val_eng.npy\").astype(np.float32)\n",
    "\n",
    "# Load feature metadata\n",
    "print(\"   üîÑ Loading feature metadata...\")\n",
    "with open(ENGINEERED_DIR / \"feature_metadata.json\", 'r') as f:\n",
    "    feature_metadata = json.load(f)\n",
    "\n",
    "feature_names = feature_metadata['final_feature_names']\n",
    "\n",
    "# Memory usage check\n",
    "def get_memory_usage():\n",
    "    \"\"\"Calculate memory usage of loaded arrays\"\"\"\n",
    "    arrays = [X_train, X_val, X_test, y_train, y_val]\n",
    "    total_memory = sum(arr.nbytes for arr in arrays) / (1024**2)  # MB\n",
    "    return total_memory\n",
    "\n",
    "memory_mb = get_memory_usage()\n",
    "\n",
    "print(f\"\\n‚úÖ Data loading completed successfully!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä DATASET SUMMARY:\")\n",
    "print(f\"   üîπ X_train shape: {X_train.shape}\")\n",
    "print(f\"   üîπ X_val shape: {X_val.shape}\")\n",
    "print(f\"   üîπ X_test shape: {X_test.shape}\")\n",
    "print(f\"   üîπ y_train shape: {y_train.shape}\")\n",
    "print(f\"   üîπ y_val shape: {y_val.shape}\")\n",
    "print(f\"   üîπ Feature count: {len(feature_names)}\")\n",
    "print(f\"   üîπ Total memory usage: {memory_mb:.1f} MB\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff89b04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 3: MANDATORY LOG TRANSFORMATION & INCOME RANGE SETUP\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üéØ CRITICAL: Applying mandatory log1p transformation\")\n",
    "print(\"   üìà Problem: 2-5L income range shows 41.43% MAPE (vs target <18%)\")\n",
    "print(\"   üí° Solution: Log transformation + range-specific modeling\")\n",
    "\n",
    "# MANDATORY: Apply log1p transformation to target variables\n",
    "print(\"\\nüîÑ Applying log1p transformation...\")\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log = np.log1p(y_val)\n",
    "\n",
    "print(f\"   ‚úÖ y_train_log: {y_train_log.shape} | Range: [{y_train_log.min():.3f}, {y_train_log.max():.3f}]\")\n",
    "print(f\"   ‚úÖ y_val_log: {y_val_log.shape} | Range: [{y_val_log.min():.3f}, {y_val_log.max():.3f}]\")\n",
    "\n",
    "# Create income range masks for specialized analysis\n",
    "print(\"\\nüéØ Creating income range masks...\")\n",
    "income_ranges = {\n",
    "    'low': (y_train >= 200000) & (y_train < 500000),    # 2-5 Lakhs\n",
    "    'mid': (y_train >= 500000) & (y_train < 1000000),   # 5-10 Lakhs  \n",
    "    'high': y_train >= 1000000                          # 10+ Lakhs\n",
    "}\n",
    "\n",
    "income_ranges_val = {\n",
    "    'low': (y_val >= 200000) & (y_val < 500000),\n",
    "    'mid': (y_val >= 500000) & (y_val < 1000000),\n",
    "    'high': y_val >= 1000000\n",
    "}\n",
    "\n",
    "# Analyze income distribution\n",
    "print(\"\\nüìä INCOME RANGE ANALYSIS:\")\n",
    "print(\"Training Set:\")\n",
    "for range_name, mask in income_ranges.items():\n",
    "    count = mask.sum()\n",
    "    percentage = (count / len(y_train)) * 100\n",
    "    avg_income = y_train[mask].mean() if count > 0 else 0\n",
    "    print(f\"   üîπ {range_name.upper():>4} (2-5L/5-10L/10L+): {count:>6,} samples ({percentage:>5.1f}%) | Avg: ‚Çπ{avg_income:>8,.0f}\")\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "for range_name, mask in income_ranges_val.items():\n",
    "    count = mask.sum()\n",
    "    percentage = (count / len(y_val)) * 100\n",
    "    avg_income = y_val[mask].mean() if count > 0 else 0\n",
    "    print(f\"   üîπ {range_name.upper():>4} (2-5L/5-10L/10L+): {count:>6,} samples ({percentage:>5.1f}%) | Avg: ‚Çπ{avg_income:>8,.0f}\")\n",
    "\n",
    "# Create sample weights (3x weight for 2-5L range)\n",
    "print(\"\\n‚öñÔ∏è  Creating sample weights for range balancing...\")\n",
    "sample_weights = np.ones(len(y_train_log))\n",
    "sample_weights[income_ranges['low']] = 3.0  # 3x weight for 2-5L range\n",
    "\n",
    "print(f\"   ‚úÖ Sample weights created: {len(sample_weights)} weights\")\n",
    "print(f\"   üîπ Standard weight (5-10L, 10L+): 1.0 ({(sample_weights == 1.0).sum():,} samples)\")\n",
    "print(f\"   üîπ Enhanced weight (2-5L): 3.0 ({(sample_weights == 3.0).sum():,} samples)\")\n",
    "print(f\"   üîπ Total weighted samples equivalent: {sample_weights.sum():,.0f}\")\n",
    "\n",
    "# Target variable statistics in both scales\n",
    "print(\"\\nüìà TARGET VARIABLE STATISTICS:\")\n",
    "print(\"Original Scale (‚Çπ):\")\n",
    "print(f\"   üîπ Training   | Mean: ‚Çπ{y_train.mean():>8,.0f} | Std: ‚Çπ{y_train.std():>8,.0f} | Range: [‚Çπ{y_train.min():>7,.0f}, ‚Çπ{y_train.max():>9,.0f}]\")\n",
    "print(f\"   üîπ Validation | Mean: ‚Çπ{y_val.mean():>8,.0f} | Std: ‚Çπ{y_val.std():>8,.0f} | Range: [‚Çπ{y_val.min():>7,.0f}, ‚Çπ{y_val.max():>9,.0f}]\")\n",
    "\n",
    "print(\"\\nLog-Transformed Scale:\")\n",
    "print(f\"   üîπ Training   | Mean: {y_train_log.mean():>6.3f} | Std: {y_train_log.std():>6.3f} | Range: [{y_train_log.min():>6.3f}, {y_train_log.max():>6.3f}]\")\n",
    "print(f\"   üîπ Validation | Mean: {y_val_log.mean():>6.3f} | Std: {y_val_log.std():>6.3f} | Range: [{y_val_log.min():>6.3f}, {y_val_log.max():>6.3f}]\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed72d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 4: ENHANCED EVALUATION FRAMEWORK\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä Setting up enhanced evaluation framework with income range analysis\")\n",
    "\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Absolute Percentage Error with safety checks\"\"\"\n",
    "    # Avoid division by zero\n",
    "    mask = y_true != 0\n",
    "    if not mask.any():\n",
    "        return np.inf\n",
    "    \n",
    "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "    return mape\n",
    "\n",
    "def evaluate_with_range_analysis(y_true_log, y_pred_log, income_ranges_mask=None, \n",
    "                                dataset_name=\"Dataset\", return_details=False):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation with income range breakdown\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true_log: True values in log space\n",
    "    - y_pred_log: Predicted values in log space  \n",
    "    - income_ranges_mask: Dict with range masks for original scale\n",
    "    - dataset_name: Name for reporting\n",
    "    - return_details: Whether to return detailed metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert from log space to original scale\n",
    "    y_true_orig = np.expm1(y_true_log)\n",
    "    y_pred_orig = np.expm1(y_pred_log)\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_mape = calculate_mape(y_true_orig, y_pred_orig)\n",
    "    overall_mae = mean_absolute_error(y_true_orig, y_pred_orig)\n",
    "    overall_rmse = np.sqrt(mean_squared_error(y_true_orig, y_pred_orig))\n",
    "    overall_r2 = r2_score(y_true_orig, y_pred_orig)\n",
    "    \n",
    "    print(f\"\\nüìä {dataset_name} PERFORMANCE ANALYSIS:\")\n",
    "    print(f\"   üéØ Overall MAPE: {overall_mape:.2f}%\")\n",
    "    print(f\"   üìè Overall MAE:  ‚Çπ{overall_mae:,.0f}\")\n",
    "    print(f\"   üìê Overall RMSE: ‚Çπ{overall_rmse:,.0f}\")\n",
    "    print(f\"   üìà Overall R¬≤:   {overall_r2:.4f}\")\n",
    "    \n",
    "    # Range-specific analysis if masks provided\n",
    "    range_metrics = {}\n",
    "    if income_ranges_mask is not None:\n",
    "        print(f\"\\nüéØ INCOME RANGE BREAKDOWN:\")\n",
    "        \n",
    "        range_names = {'low': '2-5L', 'mid': '5-10L', 'high': '10L+'}\n",
    "        \n",
    "        for range_key, mask in income_ranges_mask.items():\n",
    "            if mask.sum() > 0:\n",
    "                range_mape = calculate_mape(y_true_orig[mask], y_pred_orig[mask])\n",
    "                range_mae = mean_absolute_error(y_true_orig[mask], y_pred_orig[mask])\n",
    "                range_count = mask.sum()\n",
    "                \n",
    "                range_metrics[range_key] = {\n",
    "                    'mape': range_mape,\n",
    "                    'mae': range_mae,\n",
    "                    'count': range_count\n",
    "                }\n",
    "                \n",
    "                status = \"‚úÖ\" if range_mape < 20 else \"‚ö†Ô∏è\" if range_mape < 25 else \"‚ùå\"\n",
    "                print(f\"   {status} {range_names[range_key]:>4}: {range_mape:>6.2f}% MAPE | ‚Çπ{range_mae:>8,.0f} MAE | {range_count:>6,} samples\")\n",
    "            else:\n",
    "                print(f\"   ‚≠ï {range_names[range_key]:>4}: No samples in range\")\n",
    "    \n",
    "    if return_details:\n",
    "        return {\n",
    "            'overall_mape': overall_mape,\n",
    "            'overall_mae': overall_mae, \n",
    "            'overall_rmse': overall_rmse,\n",
    "            'overall_r2': overall_r2,\n",
    "            'range_metrics': range_metrics\n",
    "        }\n",
    "    else:\n",
    "        return overall_mape\n",
    "\n",
    "# Cross-validation setup with stratification\n",
    "def create_stratified_cv(y_train, n_splits=5):\n",
    "    \"\"\"Create stratified CV splits based on income ranges\"\"\"\n",
    "    \n",
    "    # Create income bins for stratification\n",
    "    income_bins = np.digitize(y_train, bins=[200000, 500000, 1000000, 2000000])\n",
    "    \n",
    "    # Use StratifiedKFold with income bins\n",
    "    cv_splitter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    print(f\"‚úÖ Created {n_splits}-fold stratified CV based on income ranges\")\n",
    "    print(f\"   üéØ Bin distribution: {np.bincount(income_bins)}\")\n",
    "    \n",
    "    return cv_splitter, income_bins\n",
    "\n",
    "# Model performance tracking\n",
    "class ModelTracker:\n",
    "    \"\"\"Track model performance across experiments\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "        \n",
    "    def add_result(self, model_name, overall_mape, range_metrics=None, \n",
    "                  training_time=0, memory_usage=0, hyperparams=None):\n",
    "        \"\"\"Add model result to tracking\"\"\"\n",
    "        \n",
    "        result = {\n",
    "            'model_name': model_name,\n",
    "            'overall_mape': overall_mape,\n",
    "            'training_time': training_time,\n",
    "            'memory_usage': memory_usage,\n",
    "            'hyperparams': hyperparams or {},\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "        \n",
    "        if range_metrics:\n",
    "            result.update({\n",
    "                'low_mape': range_metrics.get('low', {}).get('mape', np.nan),\n",
    "                'mid_mape': range_metrics.get('mid', {}).get('mape', np.nan), \n",
    "                'high_mape': range_metrics.get('high', {}).get('mape', np.nan)\n",
    "            })\n",
    "            \n",
    "        self.results.append(result)\n",
    "        \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get performance summary table\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to display\")\n",
    "            return None\n",
    "            \n",
    "        df = pd.DataFrame(self.results)\n",
    "        return df.sort_values('overall_mape')\n",
    "    \n",
    "    def get_best_model(self):\n",
    "        \"\"\"Get best performing model\"\"\"\n",
    "        if not self.results:\n",
    "            return None\n",
    "        return min(self.results, key=lambda x: x['overall_mape'])\n",
    "\n",
    "# Initialize model tracker\n",
    "model_tracker = ModelTracker()\n",
    "\n",
    "# Visualization functions\n",
    "def plot_predictions_vs_actual(y_true, y_pred, title=\"Predictions vs Actual\", \n",
    "                              income_ranges_mask=None):\n",
    "    \"\"\"Plot predictions vs actual values with range highlighting\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Convert to millions for better readability\n",
    "    y_true_millions = y_true / 1000000\n",
    "    y_pred_millions = y_pred / 1000000\n",
    "    \n",
    "    if income_ranges_mask is not None:\n",
    "        # Plot different ranges in different colors\n",
    "        colors = {'low': 'red', 'mid': 'blue', 'high': 'green'}\n",
    "        labels = {'low': '2-5L', 'mid': '5-10L', 'high': '10L+'}\n",
    "        \n",
    "        for range_key, mask in income_ranges_mask.items():\n",
    "            if mask.sum() > 0:\n",
    "                plt.scatter(y_true_millions[mask], y_pred_millions[mask], \n",
    "                          alpha=0.6, c=colors[range_key], label=labels[range_key], s=20)\n",
    "    else:\n",
    "        plt.scatter(y_true_millions, y_pred_millions, alpha=0.6, s=20)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    max_val = max(y_true_millions.max(), y_pred_millions.max())\n",
    "    min_val = min(y_true_millions.min(), y_pred_millions.min())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Actual Income (‚Çπ Millions)')\n",
    "    plt.ylabel('Predicted Income (‚Çπ Millions)')\n",
    "    plt.title(title)\n",
    "    \n",
    "    if income_ranges_mask is not None:\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Initialize CV splitter\n",
    "cv_splitter, income_bins = create_stratified_cv(y_train, n_splits=5)\n",
    "\n",
    "print(\"‚úÖ Enhanced evaluation framework setup complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea90ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 5: BASELINE MODELS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìà Training baseline models to establish performance benchmarks\")\n",
    "print(\"üéØ All models will use log-transformed targets with sample weighting\")\n",
    "\n",
    "# Simple baseline predictions\n",
    "print(\"\\nüîµ SIMPLE BASELINES:\")\n",
    "\n",
    "# Mean baseline\n",
    "mean_baseline_log = np.full_like(y_val_log, y_train_log.mean())\n",
    "mean_mape = evaluate_with_range_analysis(y_val_log, mean_baseline_log, \n",
    "                                        income_ranges_val, \"Mean Baseline\")\n",
    "model_tracker.add_result(\"Mean Baseline\", mean_mape)\n",
    "\n",
    "# Median baseline  \n",
    "median_baseline_log = np.full_like(y_val_log, np.median(y_train_log))\n",
    "median_mape = evaluate_with_range_analysis(y_val_log, median_baseline_log,\n",
    "                                          income_ranges_val, \"Median Baseline\")\n",
    "model_tracker.add_result(\"Median Baseline\", median_mape)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Linear models with regularization\n",
    "print(\"üîµ LINEAR MODEL BASELINES:\")\n",
    "\n",
    "# Ridge Regression\n",
    "print(\"\\nüìä Training Ridge Regression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "ridge_model = Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
    "ridge_model.fit(X_train, y_train_log, sample_weight=sample_weights)\n",
    "\n",
    "ridge_pred_log = ridge_model.predict(X_val)\n",
    "ridge_time = time.time() - start_time\n",
    "\n",
    "ridge_metrics = evaluate_with_range_analysis(y_val_log, ridge_pred_log, \n",
    "                                           income_ranges_val, \"Ridge Regression\", \n",
    "                                           return_details=True)\n",
    "model_tracker.add_result(\"Ridge Regression\", ridge_metrics['overall_mape'], \n",
    "                        ridge_metrics['range_metrics'], ridge_time)\n",
    "\n",
    "# Lasso Regression\n",
    "print(\"\\nüìä Training Lasso Regression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lasso_model = Lasso(alpha=0.1, random_state=RANDOM_STATE, max_iter=2000)\n",
    "lasso_model.fit(X_train, y_train_log, sample_weight=sample_weights)\n",
    "\n",
    "lasso_pred_log = lasso_model.predict(X_val)\n",
    "lasso_time = time.time() - start_time\n",
    "\n",
    "lasso_metrics = evaluate_with_range_analysis(y_val_log, lasso_pred_log,\n",
    "                                           income_ranges_val, \"Lasso Regression\",\n",
    "                                           return_details=True)\n",
    "model_tracker.add_result(\"Lasso Regression\", lasso_metrics['overall_mape'],\n",
    "                        lasso_metrics['range_metrics'], lasso_time)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Tree-based baselines\n",
    "print(\"üîµ TREE-BASED BASELINES:\")\n",
    "\n",
    "# Random Forest baseline\n",
    "print(\"\\nüìä Training Random Forest Baseline...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_baseline = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10, \n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_baseline.fit(X_train, y_train_log, sample_weight=sample_weights)\n",
    "rf_pred_log = rf_baseline.predict(X_val)\n",
    "rf_time = time.time() - start_time\n",
    "\n",
    "rf_metrics = evaluate_with_range_analysis(y_val_log, rf_pred_log,\n",
    "                                        income_ranges_val, \"Random Forest Baseline\",\n",
    "                                        return_details=True)\n",
    "model_tracker.add_result(\"Random Forest Baseline\", rf_metrics['overall_mape'],\n",
    "                        rf_metrics['range_metrics'], rf_time)\n",
    "\n",
    "# Extra Trees baseline\n",
    "print(\"\\nüìä Training Extra Trees Baseline...\")\n",
    "start_time = time.time()\n",
    "\n",
    "et_baseline = ExtraTreesRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5, \n",
    "    min_samples_leaf=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "et_baseline.fit(X_train, y_train_log, sample_weight=sample_weights)\n",
    "et_pred_log = et_baseline.predict(X_val)\n",
    "et_time = time.time() - start_time\n",
    "\n",
    "et_metrics = evaluate_with_range_analysis(y_val_log, et_pred_log,\n",
    "                                        income_ranges_val, \"Extra Trees Baseline\", \n",
    "                                        return_details=True)\n",
    "model_tracker.add_result(\"Extra Trees Baseline\", et_metrics['overall_mape'],\n",
    "                        et_metrics['range_metrics'], et_time)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Gradient boosting baselines (if available)\n",
    "if XGBOOST_AVAILABLE:\n",
    "    print(\"üîµ GRADIENT BOOSTING BASELINE:\")\n",
    "    print(\"\\nüìä Training XGBoost Baseline...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    xgb_baseline = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    xgb_baseline.fit(X_train, y_train_log, sample_weight=sample_weights)\n",
    "    xgb_pred_log = xgb_baseline.predict(X_val)\n",
    "    xgb_time = time.time() - start_time\n",
    "    \n",
    "    xgb_metrics = evaluate_with_range_analysis(y_val_log, xgb_pred_log,\n",
    "                                             income_ranges_val, \"XGBoost Baseline\",\n",
    "                                             return_details=True)\n",
    "    model_tracker.add_result(\"XGBoost Baseline\", xgb_metrics['overall_mape'],\n",
    "                            xgb_metrics['range_metrics'], xgb_time)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Baseline results summary\n",
    "print(\"üìä BASELINE MODELS SUMMARY:\")\n",
    "baseline_summary = model_tracker.get_summary()\n",
    "if baseline_summary is not None:\n",
    "    print(\"\\nTop 5 Baseline Models:\")\n",
    "    display_cols = ['model_name', 'overall_mape', 'low_mape', 'mid_mape', 'high_mape', 'training_time']\n",
    "    available_cols = [col for col in display_cols if col in baseline_summary.columns]\n",
    "    print(baseline_summary[available_cols].head().to_string(index=False, float_format='%.2f'))\n",
    "\n",
    "# Best baseline identification\n",
    "best_baseline = model_tracker.get_best_model()\n",
    "if best_baseline:\n",
    "    print(f\"\\nüèÜ BEST BASELINE MODEL: {best_baseline['model_name']}\")\n",
    "    print(f\"   üéØ Overall MAPE: {best_baseline['overall_mape']:.2f}%\")\n",
    "    if 'low_mape' in best_baseline:\n",
    "        print(f\"   üî¥ 2-5L MAPE: {best_baseline['low_mape']:.2f}%\") \n",
    "        print(f\"   üîµ 5-10L MAPE: {best_baseline['mid_mape']:.2f}%\")\n",
    "        print(f\"   üü¢ 10L+ MAPE: {best_baseline['high_mape']:.2f}%\")\n",
    "\n",
    "print(\"\\n‚úÖ Baseline models training complete!\")\n",
    "print(\"üéØ Target for advanced models: Beat best baseline performance\")\n",
    "print(\"üî• Critical focus: Reduce 2-5L range MAPE significantly\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6980f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 6: ADVANCED MODEL 1 - LIGHTGBM WITH OPTUNA\n",
    "# =============================================================================\n",
    "\n",
    "if LIGHTGBM_AVAILABLE and OPTUNA_AVAILABLE:\n",
    "    print(\"üöÄ Training LightGBM with Optuna hyperparameter optimization\")\n",
    "    print(\"üéØ Focus: Minimize overall MAPE with 2-5L range penalty\")\n",
    "    \n",
    "    def objective_lightgbm(trial):\n",
    "        \"\"\"Optuna objective function for LightGBM optimization\"\"\"\n",
    "        \n",
    "        # Define hyperparameter search space\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mae',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "            'verbosity': -1,\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # Add GPU support if available\n",
    "        try:\n",
    "            params['device'] = 'gpu'\n",
    "            params['gpu_platform_id'] = 0\n",
    "            params['gpu_device_id'] = 0\n",
    "        except:\n",
    "            params['device'] = 'cpu'\n",
    "        \n",
    "        # Cross-validation with income range evaluation\n",
    "        cv_scores = []\n",
    "        cv_range_penalties = []\n",
    "        \n",
    "        for train_idx, val_idx in cv_splitter.split(X_train, income_bins):\n",
    "            X_cv_train, X_cv_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_cv_train, y_cv_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "            weights_cv_train = sample_weights[train_idx]\n",
    "            \n",
    "            # Create LightGBM datasets\n",
    "            train_data = lgb.Dataset(X_cv_train, label=y_cv_train, weight=weights_cv_train)\n",
    "            \n",
    "            # Train model\n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train_data,\n",
    "                num_boost_round=1000,\n",
    "                valid_sets=[train_data],\n",
    "                callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
    "            )\n",
    "            \n",
    "            # Predict and evaluate\n",
    "            y_pred_log = model.predict(X_cv_val, num_iteration=model.best_iteration)\n",
    "            \n",
    "            # Convert to original scale for MAPE calculation\n",
    "            y_true_orig = np.expm1(y_cv_val)\n",
    "            y_pred_orig = np.expm1(y_pred_log)\n",
    "            \n",
    "            # Calculate overall MAPE\n",
    "            overall_mape = calculate_mape(y_true_orig, y_pred_orig)\n",
    "            cv_scores.append(overall_mape)\n",
    "            \n",
    "            # Calculate 2-5L range penalty (if samples exist)\n",
    "            low_mask = (y_true_orig >= 200000) & (y_true_orig < 500000)\n",
    "            if low_mask.sum() > 0:\n",
    "                low_mape = calculate_mape(y_true_orig[low_mask], y_pred_orig[low_mask])\n",
    "                # Penalty: heavily weight 2-5L performance\n",
    "                range_penalty = max(0, low_mape - 25) * 2  # Penalty if >25% MAPE\n",
    "                cv_range_penalties.append(range_penalty)\n",
    "            else:\n",
    "                cv_range_penalties.append(0)\n",
    "        \n",
    "        # Final objective: overall MAPE + range penalty\n",
    "        mean_mape = np.mean(cv_scores)\n",
    "        mean_penalty = np.mean(cv_range_penalties)\n",
    "        objective_value = mean_mape + mean_penalty\n",
    "        \n",
    "        return objective_value\n",
    "    \n",
    "    # Run Optuna optimization\n",
    "    print(\"\\nüîÑ Starting Optuna optimization (Target: 200 trials)...\")\n",
    "    print(\"   üí° Objective: Minimize (Overall MAPE + 2-5L Range Penalty)\")\n",
    "    \n",
    "    study_lgb = optuna.create_study(direction='minimize', study_name='lightgbm_optimization')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    study_lgb.optimize(objective_lightgbm, n_trials=200, timeout=1800)  # 30 min timeout\n",
    "    optimization_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚úÖ Optuna optimization completed in {optimization_time/60:.1f} minutes\")\n",
    "    print(f\"üèÜ Best objective value: {study_lgb.best_value:.3f}\")\n",
    "    print(f\"üéØ Best trial number: {study_lgb.best_trial.number}\")\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    print(\"\\nüîÑ Training final LightGBM model with best parameters...\")\n",
    "    \n",
    "    best_params = study_lgb.best_params.copy()\n",
    "    best_params.update({\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mae',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1\n",
    "    })\n",
    "    \n",
    "    # Add GPU support\n",
    "    try:\n",
    "        best_params['device'] = 'gpu'\n",
    "        best_params['gpu_platform_id'] = 0\n",
    "        best_params['gpu_device_id'] = 0\n",
    "        print(\"   ‚ö° Using GPU acceleration\")\n",
    "    except:\n",
    "        best_params['device'] = 'cpu'\n",
    "        print(\"   üñ•Ô∏è  Using CPU training\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train_log, weight=sample_weights)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val_log, reference=train_data)\n",
    "    \n",
    "    # Train final model\n",
    "    start_time = time.time()\n",
    "    lgb_final = lgb.train(\n",
    "        best_params,\n",
    "        train_data,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'val'],\n",
    "        callbacks=[lgb.early_stopping(150), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Generate predictions\n",
    "    lgb_pred_log = lgb_final.predict(X_val, num_iteration=lgb_final.best_iteration)\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    print(f\"\\nüìä Final LightGBM model training completed in {training_time:.1f} seconds\")\n",
    "    lgb_metrics = evaluate_with_range_analysis(y_val_log, lgb_pred_log,\n",
    "                                             income_ranges_val, \"LightGBM Optimized\",\n",
    "                                             return_details=True)\n",
    "    \n",
    "    # Track results\n",
    "    model_tracker.add_result(\"LightGBM Optimized\", lgb_metrics['overall_mape'],\n",
    "                            lgb_metrics['range_metrics'], training_time,\n",
    "                            hyperparams=best_params)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    print(\"\\nüìä LightGBM Feature Importance (Top 15):\")\n",
    "    feature_importance = lgb_final.feature_importance(importance_type='gain')\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(importance_df.head(15).to_string(index=False))\n",
    "    \n",
    "    # Save model\n",
    "    model_path = MODELS_DIR / \"lightgbm_optimized.pkl\"\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'model': lgb_final,\n",
    "            'params': best_params,\n",
    "            'feature_names': feature_names,\n",
    "            'cv_score': study_lgb.best_value,\n",
    "            'feature_importance': importance_df\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"\\nüíæ LightGBM model saved to: {model_path}\")\n",
    "    \n",
    "    # Visualization of hyperparameter optimization\n",
    "    print(\"\\nüìà Optimization Analysis:\")\n",
    "    print(f\"   üîπ Total trials: {len(study_lgb.trials)}\")\n",
    "    print(f\"   üîπ Best trial: {study_lgb.best_trial.number}\")\n",
    "    print(f\"   üîπ Best parameters:\")\n",
    "    for param, value in study_lgb.best_params.items():\n",
    "        print(f\"      ‚Ä¢ {param}: {value}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ LightGBM optimization and training complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå LightGBM or Optuna not available - skipping LightGBM optimization\")\n",
    "    lgb_metrics = None\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c57eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 7: ADVANCED MODEL 2 - XGBOOST WITH OPTUNA\n",
    "# =============================================================================\n",
    "\n",
    "if XGBOOST_AVAILABLE and OPTUNA_AVAILABLE:\n",
    "    print(\"üöÄ Training XGBoost with Optuna hyperparameter optimization\")\n",
    "    print(\"üéØ Focus: Minimize overall MAPE with enhanced 2-5L range penalty\")\n",
    "    \n",
    "    def objective_xgboost(trial):\n",
    "        \"\"\"Optuna objective function for XGBoost optimization\"\"\"\n",
    "        \n",
    "        # Define hyperparameter search space\n",
    "        params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'mae',\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 2000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.6, 1.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': -1,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        # Add GPU support if available\n",
    "        try:\n",
    "            params['tree_method'] = 'gpu_hist'\n",
    "            params['gpu_id'] = 0\n",
    "        except:\n",
    "            params['tree_method'] = 'hist'\n",
    "        \n",
    "        # Cross-validation with income range evaluation\n",
    "        cv_scores = []\n",
    "        cv_range_penalties = []\n",
    "        \n",
    "        for train_idx, val_idx in cv_splitter.split(X_train, income_bins):\n",
    "            X_cv_train, X_cv_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_cv_train, y_cv_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "            weights_cv_train = sample_weights[train_idx]\n",
    "            \n",
    "            # Train model\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "            model.fit(\n",
    "                X_cv_train, y_cv_train,\n",
    "                sample_weight=weights_cv_train,\n",
    "                eval_set=[(X_cv_train, y_cv_train)],\n",
    "                early_stopping_rounds=100,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Predict and evaluate\n",
    "            y_pred_log = model.predict(X_cv_val)\n",
    "            \n",
    "            # Convert to original scale for MAPE calculation\n",
    "            y_true_orig = np.expm1(y_cv_val)\n",
    "            y_pred_orig = np.expm1(y_pred_log)\n",
    "            \n",
    "            # Calculate overall MAPE\n",
    "            overall_mape = calculate_mape(y_true_orig, y_pred_orig)\n",
    "            cv_scores.append(overall_mape)\n",
    "            \n",
    "            # Calculate 2-5L range penalty (enhanced)\n",
    "            low_mask = (y_true_orig >= 200000) & (y_true_orig < 500000)\n",
    "            if low_mask.sum() > 0:\n",
    "                low_mape = calculate_mape(y_true_orig[low_mask], y_pred_orig[low_mask])\n",
    "                # Enhanced penalty: more aggressive for 2-5L range\n",
    "                range_penalty = max(0, low_mape - 20) * 3  # Penalty if >20% MAPE\n",
    "                cv_range_penalties.append(range_penalty)\n",
    "            else:\n",
    "                cv_range_penalties.append(0)\n",
    "        \n",
    "        # Final objective: overall MAPE + enhanced range penalty\n",
    "        mean_mape = np.mean(cv_scores)\n",
    "        mean_penalty = np.mean(cv_range_penalties)\n",
    "        objective_value = mean_mape + mean_penalty\n",
    "        \n",
    "        return objective_value\n",
    "    \n",
    "    # Run Optuna optimization\n",
    "    print(\"\\nüîÑ Starting Optuna optimization (Target: 200 trials)...\")\n",
    "    print(\"   üí° Objective: Minimize (Overall MAPE + Enhanced 2-5L Range Penalty)\")\n",
    "    \n",
    "    study_xgb = optuna.create_study(direction='minimize', study_name='xgboost_optimization')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    study_xgb.optimize(objective_xgboost, n_trials=200, timeout=1800)  # 30 min timeout\n",
    "    optimization_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚úÖ Optuna optimization completed in {optimization_time/60:.1f} minutes\")\n",
    "    print(f\"üèÜ Best objective value: {study_xgb.best_value:.3f}\")\n",
    "    print(f\"üéØ Best trial number: {study_xgb.best_trial.number}\")\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    print(\"\\nüîÑ Training final XGBoost model with best parameters...\")\n",
    "    \n",
    "    best_params = study_xgb.best_params.copy()\n",
    "    best_params.update({\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'mae',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': 0\n",
    "    })\n",
    "    \n",
    "    # Add GPU support\n",
    "    try:\n",
    "        best_params['tree_method'] = 'gpu_hist'\n",
    "        best_params['gpu_id'] = 0\n",
    "        print(\"   ‚ö° Using GPU acceleration\")\n",
    "    except:\n",
    "        best_params['tree_method'] = 'hist'\n",
    "        print(\"   üñ•Ô∏è  Using CPU training\")\n",
    "    \n",
    "    # Train final model\n",
    "    start_time = time.time()\n",
    "    xgb_final = xgb.XGBRegressor(**best_params)\n",
    "    xgb_final.fit(\n",
    "        X_train, y_train_log,\n",
    "        sample_weight=sample_weights,\n",
    "        eval_set=[(X_train, y_train_log), (X_val, y_val_log)],\n",
    "        early_stopping_rounds=150,\n",
    "        verbose=100\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Generate predictions\n",
    "    xgb_pred_log = xgb_final.predict(X_val)\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    print(f\"\\nüìä Final XGBoost model training completed in {training_time:.1f} seconds\")\n",
    "    xgb_metrics = evaluate_with_range_analysis(y_val_log, xgb_pred_log,\n",
    "                                             income_ranges_val, \"XGBoost Optimized\",\n",
    "                                             return_details=True)\n",
    "    \n",
    "    # Track results\n",
    "    model_tracker.add_result(\"XGBoost Optimized\", xgb_metrics['overall_mape'],\n",
    "                            xgb_metrics['range_metrics'], training_time,\n",
    "                            hyperparams=best_params)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    print(\"\\nüìä XGBoost Feature Importance (Top 15):\")\n",
    "    feature_importance = xgb_final.feature_importances_\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(importance_df.head(15).to_string(index=False))\n",
    "    \n",
    "    # Training progress visualization\n",
    "    if hasattr(xgb_final, 'evals_result_'):\n",
    "        results = xgb_final.evals_result_\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(results['validation_0']['mae'], label='Training MAE')\n",
    "        plt.plot(results['validation_1']['mae'], label='Validation MAE')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.title('XGBoost Training Progress')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.bar(range(len(importance_df.head(10))), importance_df.head(10)['importance'])\n",
    "        plt.xlabel('Feature Rank')\n",
    "        plt.ylabel('Importance')\n",
    "        plt.title('Top 10 Feature Importances')\n",
    "        plt.xticks(range(10), range(1, 11))\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Save model\n",
    "    model_path = MODELS_DIR / \"xgboost_optimized.pkl\"\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'model': xgb_final,\n",
    "            'params': best_params,\n",
    "            'feature_names': feature_names,\n",
    "            'cv_score': study_xgb.best_value,\n",
    "            'feature_importance': importance_df\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"\\nüíæ XGBoost model saved to: {model_path}\")\n",
    "    \n",
    "    # Optimization analysis\n",
    "    print(\"\\nüìà Optimization Analysis:\")\n",
    "    print(f\"   üîπ Total trials: {len(study_xgb.trials)}\")\n",
    "    print(f\"   üîπ Best trial: {study_xgb.best_trial.number}\")\n",
    "    print(f\"   üîπ Best parameters:\")\n",
    "    for param, value in study_xgb.best_params.items():\n",
    "        print(f\"      ‚Ä¢ {param}: {value}\")\n",
    "    \n",
    "    # Compare with previous best\n",
    "    current_best = model_tracker.get_best_model()\n",
    "    if current_best and current_best['model_name'] == 'XGBoost Optimized':\n",
    "        print(f\"\\nüèÜ XGBoost is now the best model!\")\n",
    "    \n",
    "    print(\"\\n‚úÖ XGBoost optimization and training complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå XGBoost or Optuna not available - skipping XGBoost optimization\")\n",
    "    xgb_metrics = None\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf839629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 8: ADVANCED MODEL 3 - CATBOOST WITH OPTUNA\n",
    "# =============================================================================\n",
    "\n",
    "if CATBOOST_AVAILABLE and OPTUNA_AVAILABLE:\n",
    "    print(\"üöÄ Training CatBoost with Optuna hyperparameter optimization\")\n",
    "    print(\"üéØ Focus: Leverage CatBoost's built-in overfitting detection + 2-5L penalty\")\n",
    "    \n",
    "    def objective_catboost(trial):\n",
    "        \"\"\"Optuna objective function for CatBoost optimization\"\"\"\n",
    "        \n",
    "        # Define hyperparameter search space\n",
    "        params = {\n",
    "            'loss_function': 'MAE',\n",
    "            'iterations': trial.suggest_int('iterations', 500, 3000),\n",
    "            'depth': trial.suggest_int('depth', 4, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 30),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.6, 1.0),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 50),\n",
    "            'max_leaves': trial.suggest_int('max_leaves', 16, 512),\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'verbose': False,\n",
    "            'thread_count': -1\n",
    "        }\n",
    "        \n",
    "        # Add GPU support if available\n",
    "        try:\n",
    "            params['task_type'] = 'GPU'\n",
    "            params['gpu_ram_part'] = 0.5\n",
    "        except:\n",
    "            params['task_type'] = 'CPU'\n",
    "        \n",
    "        # Cross-validation with income range evaluation\n",
    "        cv_scores = []\n",
    "        cv_range_penalties = []\n",
    "        \n",
    "        for train_idx, val_idx in cv_splitter.split(X_train, income_bins):\n",
    "            X_cv_train, X_cv_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_cv_train, y_cv_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "            weights_cv_train = sample_weights[train_idx]\n",
    "            \n",
    "            # Create CatBoost pools\n",
    "            train_pool = cb.Pool(X_cv_train, y_cv_train, weight=weights_cv_train)\n",
    "            val_pool = cb.Pool(X_cv_val, y_cv_val)\n",
    "            \n",
    "            # Train model with built-in overfitting detection\n",
    "            model = cb.CatBoostRegressor(**params)\n",
    "            model.fit(\n",
    "                train_pool,\n",
    "                eval_set=val_pool,\n",
    "                early_stopping_rounds=100,\n",
    "                use_best_model=True,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Predict and evaluate\n",
    "            y_pred_log = model.predict(X_cv_val)\n",
    "            \n",
    "            # Convert to original scale for MAPE calculation\n",
    "            y_true_orig = np.expm1(y_cv_val)\n",
    "            y_pred_orig = np.expm1(y_pred_log)\n",
    "            \n",
    "            # Calculate overall MAPE\n",
    "            overall_mape = calculate_mape(y_true_orig, y_pred_orig)\n",
    "            cv_scores.append(overall_mape)\n",
    "            \n",
    "            # Calculate 2-5L range penalty (most aggressive)\n",
    "            low_mask = (y_true_orig >= 200000) & (y_true_orig < 500000)\n",
    "            if low_mask.sum() > 0:\n",
    "                low_mape = calculate_mape(y_true_orig[low_mask], y_pred_orig[low_mask])\n",
    "                # Most aggressive penalty for CatBoost\n",
    "                range_penalty = max(0, low_mape - 18) * 4  # Penalty if >18% MAPE\n",
    "                cv_range_penalties.append(range_penalty)\n",
    "            else:\n",
    "                cv_range_penalties.append(0)\n",
    "        \n",
    "        # Final objective: overall MAPE + most aggressive range penalty\n",
    "        mean_mape = np.mean(cv_scores)\n",
    "        mean_penalty = np.mean(cv_range_penalties)\n",
    "        objective_value = mean_mape + mean_penalty\n",
    "        \n",
    "        return objective_value\n",
    "    \n",
    "    # Run Optuna optimization\n",
    "    print(\"\\nüîÑ Starting Optuna optimization (Target: 150 trials - CatBoost is slower)...\")\n",
    "    print(\"   üí° Objective: Minimize (Overall MAPE + Most Aggressive 2-5L Penalty)\")\n",
    "    print(\"   üéØ Using CatBoost's built-in overfitting detection\")\n",
    "    \n",
    "    study_cb = optuna.create_study(direction='minimize', study_name='catboost_optimization')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    study_cb.optimize(objective_catboost, n_trials=150, timeout=2400)  # 40 min timeout\n",
    "    optimization_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚úÖ Optuna optimization completed in {optimization_time/60:.1f} minutes\")\n",
    "    print(f\"üèÜ Best objective value: {study_cb.best_value:.3f}\")\n",
    "    print(f\"üéØ Best trial number: {study_cb.best_trial.number}\")\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    print(\"\\nüîÑ Training final CatBoost model with best parameters...\")\n",
    "    \n",
    "    best_params = study_cb.best_params.copy()\n",
    "    best_params.update({\n",
    "        'loss_function': 'MAE',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': 100,  # Show progress for final training\n",
    "        'thread_count': -1\n",
    "    })\n",
    "    \n",
    "    # Add GPU support\n",
    "    try:\n",
    "        best_params['task_type'] = 'GPU'\n",
    "        best_params['gpu_ram_part'] = 0.5\n",
    "        print(\"   ‚ö° Using GPU acceleration\")\n",
    "    except:\n",
    "        best_params['task_type'] = 'CPU'\n",
    "        print(\"   üñ•Ô∏è  Using CPU training\")\n",
    "    \n",
    "    # Create pools for training\n",
    "    train_pool = cb.Pool(X_train, y_train_log, weight=sample_weights)\n",
    "    val_pool = cb.Pool(X_val, y_val_log)\n",
    "    \n",
    "    # Train final model\n",
    "    start_time = time.time()\n",
    "    cb_final = cb.CatBoostRegressor(**best_params)\n",
    "    cb_final.fit(\n",
    "        train_pool,\n",
    "        eval_set=val_pool,\n",
    "        early_stopping_rounds=200,\n",
    "        use_best_model=True,\n",
    "        plot=False\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Generate predictions\n",
    "    cb_pred_log = cb_final.predict(X_val)\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    print(f\"\\nüìä Final CatBoost model training completed in {training_time:.1f} seconds\")\n",
    "    cb_metrics = evaluate_with_range_analysis(y_val_log, cb_pred_log,\n",
    "                                            income_ranges_val, \"CatBoost Optimized\",\n",
    "                                            return_details=True)\n",
    "    \n",
    "    # Track results\n",
    "    model_tracker.add_result(\"CatBoost Optimized\", cb_metrics['overall_mape'],\n",
    "                            cb_metrics['range_metrics'], training_time,\n",
    "                            hyperparams=best_params)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    print(\"\\nüìä CatBoost Feature Importance (Top 15):\")\n",
    "    feature_importance = cb_final.get_feature_importance()\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(importance_df.head(15).to_string(index=False))\n",
    "    \n",
    "    # Training metrics visualization\n",
    "    if hasattr(cb_final, 'get_evals_result'):\n",
    "        evals_result = cb_final.get_evals_result()\n",
    "        if evals_result:\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            # Training progress\n",
    "            plt.subplot(1, 3, 1)\n",
    "            train_scores = evals_result['learn']['MAE']\n",
    "            val_scores = evals_result['validation']['MAE']\n",
    "            iterations = range(len(train_scores))\n",
    "            \n",
    "            plt.plot(iterations, train_scores, label='Training MAE', alpha=0.8)\n",
    "            plt.plot(iterations, val_scores, label='Validation MAE', alpha=0.8)\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('MAE')\n",
    "            plt.title('CatBoost Training Progress')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Feature importance\n",
    "            plt.subplot(1, 3, 2)\n",
    "            top_features = importance_df.head(10)\n",
    "            plt.barh(range(len(top_features)), top_features['importance'])\n",
    "            plt.yticks(range(len(top_features)), \n",
    "                      [f[:20] + '...' if len(f) > 20 else f for f in top_features['feature']])\n",
    "            plt.xlabel('Importance')\n",
    "            plt.title('Top 10 Feature Importances')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Learning curve comparison\n",
    "            plt.subplot(1, 3, 3)\n",
    "            min_len = min(len(train_scores), len(val_scores))\n",
    "            diff = np.array(val_scores[:min_len]) - np.array(train_scores[:min_len])\n",
    "            plt.plot(range(min_len), diff, label='Val - Train MAE', color='red', alpha=0.7)\n",
    "            plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('MAE Difference')\n",
    "            plt.title('Overfitting Detection')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    # Model analysis\n",
    "    print(f\"\\nüìà CatBoost Model Analysis:\")\n",
    "    print(f\"   üîπ Total iterations: {cb_final.get_best_iteration()}\")\n",
    "    print(f\"   üîπ Best iteration: {cb_final.get_best_iteration()}\")\n",
    "    print(f\"   üîπ Training score: {cb_final.get_best_score()['learn']['MAE']:.4f}\")\n",
    "    print(f\"   üîπ Validation score: {cb_final.get_best_score()['validation']['MAE']:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_path = MODELS_DIR / \"catboost_optimized.cbm\"\n",
    "    cb_final.save_model(str(model_path))\n",
    "    \n",
    "    # Also save metadata\n",
    "    metadata_path = MODELS_DIR / \"catboost_optimized_metadata.pkl\"\n",
    "    with open(metadata_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'params': best_params,\n",
    "            'feature_names': feature_names,\n",
    "            'cv_score': study_cb.best_value,\n",
    "            'feature_importance': importance_df,\n",
    "            'best_iteration': cb_final.get_best_iteration()\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"\\nüíæ CatBoost model saved to: {model_path}\")\n",
    "    print(f\"üíæ Metadata saved to: {metadata_path}\")\n",
    "    \n",
    "    # Optimization analysis\n",
    "    print(\"\\nüìà Optimization Analysis:\")\n",
    "    print(f\"   üîπ Total trials: {len(study_cb.trials)}\")\n",
    "    print(f\"   üîπ Best trial: {study_cb.best_trial.number}\")\n",
    "    print(f\"   üîπ Best parameters:\")\n",
    "    for param, value in study_cb.best_params.items():\n",
    "        print(f\"      ‚Ä¢ {param}: {value}\")\n",
    "    \n",
    "    # Compare with previous best\n",
    "    current_best = model_tracker.get_best_model()\n",
    "    if current_best and current_best['model_name'] == 'CatBoost Optimized':\n",
    "        print(f\"\\nüèÜ CatBoost is now the best model!\")\n",
    "    \n",
    "    print(\"\\n‚úÖ CatBoost optimization and training complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå CatBoost or Optuna not available - skipping CatBoost optimization\")\n",
    "    cb_metrics = None\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62aed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 9: ADVANCED MODEL 4 - EXTRA TREES WITH OPTUNA\n",
    "# =============================================================================\n",
    "\n",
    "if OPTUNA_AVAILABLE:\n",
    "    print(\"üöÄ Training Extra Trees with Optuna hyperparameter optimization\")\n",
    "    print(\"üéØ Focus: Variance reduction for 2-5L range stability + ensemble diversity\")\n",
    "    \n",
    "    def objective_extratrees(trial):\n",
    "        \"\"\"Optuna objective function for Extra Trees optimization\"\"\"\n",
    "        \n",
    "        # Define hyperparameter search space\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.3, 0.5, 0.7, 1.0]),\n",
    "            'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.01),\n",
    "            'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # Cross-validation with income range evaluation\n",
    "        cv_scores = []\n",
    "        cv_range_penalties = []\n",
    "        cv_range_stds = []  # Track variance for stability\n",
    "        \n",
    "        for train_idx, val_idx in cv_splitter.split(X_train, income_bins):\n",
    "            X_cv_train, X_cv_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_cv_train, y_cv_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "            weights_cv_train = sample_weights[train_idx]\n",
    "            \n",
    "            # Train model\n",
    "            model = ExtraTreesRegressor(**params)\n",
    "            model.fit(X_cv_train, y_cv_train, sample_weight=weights_cv_train)\n",
    "            \n",
    "            # Predict and evaluate\n",
    "            y_pred_log = model.predict(X_cv_val)\n",
    "            \n",
    "            # Convert to original scale for MAPE calculation\n",
    "            y_true_orig = np.expm1(y_cv_val)\n",
    "            y_pred_orig = np.expm1(y_pred_log)\n",
    "            \n",
    "            # Calculate overall MAPE\n",
    "            overall_mape = calculate_mape(y_true_orig, y_pred_orig)\n",
    "            cv_scores.append(overall_mape)\n",
    "            \n",
    "            # Calculate 2-5L range penalty and variance\n",
    "            low_mask = (y_true_orig >= 200000) & (y_true_orig < 500000)\n",
    "            if low_mask.sum() > 0:\n",
    "                low_mape = calculate_mape(y_true_orig[low_mask], y_pred_orig[low_mask])\n",
    "                \n",
    "                # Standard penalty for 2-5L range\n",
    "                range_penalty = max(0, low_mape - 22) * 2.5  # Penalty if >22% MAPE\n",
    "                cv_range_penalties.append(range_penalty)\n",
    "                \n",
    "                # Variance penalty - Extra Trees should reduce prediction variance\n",
    "                low_pred_std = np.std(y_pred_orig[low_mask])\n",
    "                low_true_std = np.std(y_true_orig[low_mask])\n",
    "                variance_ratio = low_pred_std / (low_true_std + 1e-8)\n",
    "                variance_penalty = max(0, variance_ratio - 1.0) * 5  # Penalize high variance\n",
    "                cv_range_stds.append(variance_penalty)\n",
    "            else:\n",
    "                cv_range_penalties.append(0)\n",
    "                cv_range_stds.append(0)\n",
    "        \n",
    "        # Final objective: MAPE + range penalty + variance penalty\n",
    "        mean_mape = np.mean(cv_scores)\n",
    "        mean_range_penalty = np.mean(cv_range_penalties)\n",
    "        mean_variance_penalty = np.mean(cv_range_stds)\n",
    "        objective_value = mean_mape + mean_range_penalty + mean_variance_penalty\n",
    "        \n",
    "        return objective_value\n",
    "    \n",
    "    # Run Optuna optimization\n",
    "    print(\"\\nüîÑ Starting Optuna optimization (Target: 100 trials - faster than boosting)...\")\n",
    "    print(\"   üí° Objective: Minimize (MAPE + 2-5L Range Penalty + Variance Penalty)\")\n",
    "    print(\"   üéØ Focus on variance reduction and ensemble diversity\")\n",
    "    \n",
    "    study_et = optuna.create_study(direction='minimize', study_name='extratrees_optimization')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    study_et.optimize(objective_extratrees, n_trials=100, timeout=1200)  # 20 min timeout\n",
    "    optimization_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚úÖ Optuna optimization completed in {optimization_time/60:.1f} minutes\")\n",
    "    print(f\"üèÜ Best objective value: {study_et.best_value:.3f}\")\n",
    "    print(f\"üéØ Best trial number: {study_et.best_trial.number}\")\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    print(\"\\nüîÑ Training final Extra Trees model with best parameters...\")\n",
    "    \n",
    "    best_params = study_et.best_params.copy()\n",
    "    best_params.update({\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1\n",
    "    })\n",
    "    \n",
    "    # Train final model\n",
    "    start_time = time.time()\n",
    "    et_final = ExtraTreesRegressor(**best_params)\n",
    "    et_final.fit(X_train, y_train_log, sample_weight=sample_weights)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Generate predictions\n",
    "    et_pred_log = et_final.predict(X_val)\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    print(f\"\\nüìä Final Extra Trees model training completed in {training_time:.1f} seconds\")\n",
    "    et_metrics = evaluate_with_range_analysis(y_val_log, et_pred_log,\n",
    "                                            income_ranges_val, \"Extra Trees Optimized\",\n",
    "                                            return_details=True)\n",
    "    \n",
    "    # Track results\n",
    "    model_tracker.add_result(\"Extra Trees Optimized\", et_metrics['overall_mape'],\n",
    "                            et_metrics['range_metrics'], training_time,\n",
    "                            hyperparams=best_params)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    print(\"\\nüìä Extra Trees Feature Importance (Top 15):\")\n",
    "    feature_importance = et_final.feature_importances_\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(importance_df.head(15).to_string(index=False))\n",
    "    \n",
    "    # Prediction variance analysis\n",
    "    print(\"\\nüìä Prediction Variance Analysis:\")\n",
    "    y_val_orig = np.expm1(y_val_log)\n",
    "    et_pred_orig = np.expm1(et_pred_log)\n",
    "    \n",
    "    for range_name, mask in income_ranges_val.items():\n",
    "        if mask.sum() > 0:\n",
    "            true_std = np.std(y_val_orig[mask])\n",
    "            pred_std = np.std(et_pred_orig[mask])\n",
    "            variance_ratio = pred_std / true_std\n",
    "            range_labels = {'low': '2-5L', 'mid': '5-10L', 'high': '10L+'}\n",
    "            print(f\"   üîπ {range_labels[range_name]:>4}: True Std=‚Çπ{true_std:>8,.0f} | Pred Std=‚Çπ{pred_std:>8,.0f} | Ratio={variance_ratio:.3f}\")\n",
    "    \n",
    "    # Ensemble diversity analysis (compare with Random Forest if available)\n",
    "    rf_available = any(result['model_name'] == 'Random Forest Baseline' for result in model_tracker.results)\n",
    "    if rf_available:\n",
    "        print(\"\\nüìä Ensemble Diversity Analysis:\")\n",
    "        # Find Random Forest predictions for comparison\n",
    "        rf_result = next((r for r in model_tracker.results if r['model_name'] == 'Random Forest Baseline'), None)\n",
    "        if rf_result:\n",
    "            print(\"   üéØ Extra Trees vs Random Forest diversity will be valuable for ensemble\")\n",
    "    \n",
    "    # Model visualization\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Feature importance\n",
    "    plt.subplot(1, 3, 1)\n",
    "    top_features = importance_df.head(10)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), \n",
    "              [f[:15] + '...' if len(f) > 15 else f for f in top_features['feature']])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Extra Trees: Top 10 Features')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Predictions vs actual by income range\n",
    "    plt.subplot(1, 3, 2)\n",
    "    colors = {'low': 'red', 'mid': 'blue', 'high': 'green'}\n",
    "    labels = {'low': '2-5L', 'mid': '5-10L', 'high': '10L+'}\n",
    "    \n",
    "    for range_name, mask in income_ranges_val.items():\n",
    "        if mask.sum() > 0:\n",
    "            y_true_millions = y_val_orig[mask] / 1000000\n",
    "            y_pred_millions = et_pred_orig[mask] / 1000000\n",
    "            plt.scatter(y_true_millions, y_pred_millions, \n",
    "                       alpha=0.6, c=colors[range_name], label=labels[range_name], s=20)\n",
    "    \n",
    "    max_val = max(y_val_orig.max(), et_pred_orig.max()) / 1000000\n",
    "    min_val = min(y_val_orig.min(), et_pred_orig.min()) / 1000000\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=2)\n",
    "    plt.xlabel('Actual Income (‚Çπ Millions)')\n",
    "    plt.ylabel('Predicted Income (‚Çπ Millions)')\n",
    "    plt.title('Extra Trees: Predictions vs Actual')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error distribution by range\n",
    "    plt.subplot(1, 3, 3)\n",
    "    errors_by_range = {}\n",
    "    for range_name, mask in income_ranges_val.items():\n",
    "        if mask.sum() > 0:\n",
    "            errors = np.abs(y_val_orig[mask] - et_pred_orig[mask]) / y_val_orig[mask] * 100\n",
    "            errors_by_range[labels[range_name]] = errors\n",
    "    \n",
    "    if errors_by_range:\n",
    "        plt.boxplot(errors_by_range.values(), labels=errors_by_range.keys())\n",
    "        plt.ylabel('Absolute Percentage Error (%)')\n",
    "        plt.title('Error Distribution by Income Range')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save model\n",
    "    model_path = MODELS_DIR / \"extratrees_optimized.pkl\"\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'model': et_final,\n",
    "            'params': best_params,\n",
    "            'feature_names': feature_names,\n",
    "            'cv_score': study_et.best_value,\n",
    "            'feature_importance': importance_df\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"\\nüíæ Extra Trees model saved to: {model_path}\")\n",
    "    \n",
    "    # Optimization analysis\n",
    "    print(\"\\nüìà Optimization Analysis:\")\n",
    "    print(f\"   üîπ Total trials: {len(study_et.trials)}\")\n",
    "    print(f\"   üîπ Best trial: {study_et.best_trial.number}\")\n",
    "    print(f\"   üîπ Best parameters:\")\n",
    "    for param, value in study_et.best_params.items():\n",
    "        print(f\"      ‚Ä¢ {param}: {value}\")\n",
    "    \n",
    "    # Compare with previous best\n",
    "    current_best = model_tracker.get_best_model()\n",
    "    if current_best and current_best['model_name'] == 'Extra Trees Optimized':\n",
    "        print(f\"\\nüèÜ Extra Trees is now the best model!\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Extra Trees optimization and training complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Optuna not available - skipping Extra Trees optimization\")\n",
    "    et_metrics = None\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423455a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 10: MODEL COMPARISON & SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä Comprehensive Model Performance Comparison & Selection\")\n",
    "print(\"üéØ Objective: Identify top 3 models for ensemble and final selection\")\n",
    "\n",
    "# Get comprehensive results summary\n",
    "results_summary = model_tracker.get_summary()\n",
    "\n",
    "if results_summary is not None and len(results_summary) > 0:\n",
    "    \n",
    "    # Display comprehensive comparison table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìã COMPLETE MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Select columns for display\n",
    "    display_columns = ['model_name', 'overall_mape', 'low_mape', 'mid_mape', 'high_mape', 'training_time']\n",
    "    available_columns = [col for col in display_columns if col in results_summary.columns]\n",
    "    \n",
    "    # Format the results for better readability\n",
    "    display_df = results_summary[available_columns].copy()\n",
    "    \n",
    "    # Round numeric columns\n",
    "    numeric_columns = ['overall_mape', 'low_mape', 'mid_mape', 'high_mape', 'training_time']\n",
    "    for col in numeric_columns:\n",
    "        if col in display_df.columns:\n",
    "            display_df[col] = display_df[col].round(2)\n",
    "    \n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    # Statistical analysis of results\n",
    "    print(f\"\\nüìà STATISTICAL ANALYSIS:\")\n",
    "    print(f\"   üîπ Total models trained: {len(results_summary)}\")\n",
    "    \n",
    "    if 'overall_mape' in results_summary.columns:\n",
    "        overall_mapes = results_summary['overall_mape'].dropna()\n",
    "        print(f\"   üîπ Best overall MAPE: {overall_mapes.min():.2f}%\")\n",
    "        print(f\"   üîπ Worst overall MAPE: {overall_mapes.max():.2f}%\")\n",
    "        print(f\"   üîπ Mean overall MAPE: {overall_mapes.mean():.2f}%\")\n",
    "        print(f\"   üîπ MAPE std deviation: {overall_mapes.std():.2f}%\")\n",
    "        \n",
    "        # Target achievement analysis\n",
    "        target_achieved = (overall_mapes < 18).sum()\n",
    "        print(f\"   üéØ Models achieving <18% MAPE: {target_achieved}/{len(overall_mapes)}\")\n",
    "    \n",
    "    # 2-5L range specific analysis\n",
    "    if 'low_mape' in results_summary.columns:\n",
    "        low_mapes = results_summary['low_mape'].dropna()\n",
    "        if len(low_mapes) > 0:\n",
    "            print(f\"\\nüî¥ 2-5L RANGE ANALYSIS:\")\n",
    "            print(f\"   üîπ Best 2-5L MAPE: {low_mapes.min():.2f}%\")\n",
    "            print(f\"   üîπ Worst 2-5L MAPE: {low_mapes.max():.2f}%\")\n",
    "            print(f\"   üîπ Mean 2-5L MAPE: {low_mapes.mean():.2f}%\")\n",
    "            \n",
    "            # Critical improvement tracking\n",
    "            baseline_low_mape = 41.43  # From problem context\n",
    "            best_low_mape = low_mapes.min()\n",
    "            improvement = baseline_low_mape - best_low_mape\n",
    "            improvement_pct = (improvement / baseline_low_mape) * 100\n",
    "            \n",
    "            print(f\"   üöÄ Improvement from baseline: -{improvement:.2f}% ({improvement_pct:.1f}% reduction)\")\n",
    "            \n",
    "            # Target achievement for 2-5L\n",
    "            low_target_achieved = (low_mapes < 25).sum()\n",
    "            print(f\"   üéØ Models achieving <25% MAPE in 2-5L: {low_target_achieved}/{len(low_mapes)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Advanced model identification and ranking\n",
    "    print(\"üèÜ TOP PERFORMING MODELS IDENTIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Filter advanced models only (exclude baselines)\n",
    "    advanced_models = results_summary[\n",
    "        ~results_summary['model_name'].str.contains('Baseline|Mean|Median', case=False, na=False)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(advanced_models) > 0:\n",
    "        print(\"Advanced Models Performance:\")\n",
    "        print(advanced_models[available_columns].to_string(index=False))\n",
    "        \n",
    "        # Multi-criteria ranking system\n",
    "        print(f\"\\nüìä MULTI-CRITERIA RANKING SYSTEM:\")\n",
    "        \n",
    "        # Ranking criteria with weights\n",
    "        ranking_df = advanced_models.copy()\n",
    "        \n",
    "        # Criteria 1: Overall MAPE (40% weight)\n",
    "        if 'overall_mape' in ranking_df.columns:\n",
    "            ranking_df['overall_rank'] = ranking_df['overall_mape'].rank()\n",
    "            print(f\"   üéØ Overall MAPE ranking (40% weight)\")\n",
    "        \n",
    "        # Criteria 2: 2-5L MAPE (35% weight) - Most critical\n",
    "        if 'low_mape' in ranking_df.columns:\n",
    "            ranking_df['low_rank'] = ranking_df['low_mape'].rank()\n",
    "            print(f\"   üî¥ 2-5L MAPE ranking (35% weight) - CRITICAL\")\n",
    "        \n",
    "        # Criteria 3: Training efficiency (15% weight)\n",
    "        if 'training_time' in ranking_df.columns:\n",
    "            ranking_df['time_rank'] = ranking_df['training_time'].rank()\n",
    "            print(f\"   ‚è±Ô∏è  Training time ranking (15% weight)\")\n",
    "        \n",
    "        # Criteria 4: Range consistency (10% weight)\n",
    "        if 'mid_mape' in ranking_df.columns and 'high_mape' in ranking_df.columns:\n",
    "            ranking_df['range_consistency'] = abs(ranking_df['mid_mape'] - ranking_df['high_mape'])\n",
    "            ranking_df['consistency_rank'] = ranking_df['range_consistency'].rank()\n",
    "            print(f\"   üìè Range consistency ranking (10% weight)\")\n",
    "        \n",
    "        # Calculate weighted composite score\n",
    "        composite_scores = []\n",
    "        for idx, row in ranking_df.iterrows():\n",
    "            score = 0\n",
    "            weights_sum = 0\n",
    "            \n",
    "            if 'overall_rank' in ranking_df.columns:\n",
    "                score += row['overall_rank'] * 0.40\n",
    "                weights_sum += 0.40\n",
    "            \n",
    "            if 'low_rank' in ranking_df.columns:\n",
    "                score += row['low_rank'] * 0.35\n",
    "                weights_sum += 0.35\n",
    "            \n",
    "            if 'time_rank' in ranking_df.columns:\n",
    "                score += row['time_rank'] * 0.15\n",
    "                weights_sum += 0.15\n",
    "            \n",
    "            if 'consistency_rank' in ranking_df.columns:\n",
    "                score += row['consistency_rank'] * 0.10\n",
    "                weights_sum += 0.10\n",
    "            \n",
    "            if weights_sum > 0:\n",
    "                composite_scores.append(score / weights_sum)\n",
    "            else:\n",
    "                composite_scores.append(float('inf'))\n",
    "        \n",
    "        ranking_df['composite_score'] = composite_scores\n",
    "        ranking_df = ranking_df.sort_values('composite_score')\n",
    "        \n",
    "        print(f\"\\nüèÜ FINAL WEIGHTED RANKING:\")\n",
    "        rank_display = ranking_df[['model_name', 'overall_mape', 'low_mape', 'composite_score']].copy()\n",
    "        rank_display['rank'] = range(1, len(rank_display) + 1)\n",
    "        rank_display = rank_display[['rank', 'model_name', 'overall_mape', 'low_mape', 'composite_score']]\n",
    "        print(rank_display.to_string(index=False, float_format='%.2f'))\n",
    "        \n",
    "        # Select top 3 models for ensemble\n",
    "        top_3_models = ranking_df.head(3)\n",
    "        \n",
    "        print(f\"\\nüéñÔ∏è  TOP 3 MODELS SELECTED FOR ENSEMBLE:\")\n",
    "        for i, (idx, model) in enumerate(top_3_models.iterrows(), 1):\n",
    "            print(f\"   {i}. {model['model_name']}\")\n",
    "            print(f\"      üìä Overall MAPE: {model['overall_mape']:.2f}%\")\n",
    "            if 'low_mape' in model:\n",
    "                print(f\"      üî¥ 2-5L MAPE: {model['low_mape']:.2f}%\")\n",
    "            if 'training_time' in model:\n",
    "                print(f\"      ‚è±Ô∏è  Training Time: {model['training_time']:.1f}s\")\n",
    "            print(f\"      üèÜ Composite Score: {model['composite_score']:.2f}\")\n",
    "            print()\n",
    "        \n",
    "        # Best individual model analysis\n",
    "        best_model = ranking_df.iloc[0]\n",
    "        print(f\"ü•á BEST INDIVIDUAL MODEL: {best_model['model_name']}\")\n",
    "        print(f\"   üéØ Achieves overall MAPE: {best_model['overall_mape']:.2f}%\")\n",
    "        \n",
    "        if best_model['overall_mape'] < 18:\n",
    "            print(f\"   ‚úÖ SUCCESS: Achieved target <18% MAPE!\")\n",
    "        else:\n",
    "            gap = best_model['overall_mape'] - 18\n",
    "            print(f\"   ‚ö†Ô∏è  Gap to target: +{gap:.2f}% (ensemble may close this gap)\")\n",
    "        \n",
    "        if 'low_mape' in best_model:\n",
    "            if best_model['low_mape'] < 25:\n",
    "                print(f\"   ‚úÖ SUCCESS: 2-5L range improved to {best_model['low_mape']:.2f}%\")\n",
    "            else:\n",
    "                gap_low = best_model['low_mape'] - 25\n",
    "                print(f\"   ‚ö†Ô∏è  2-5L gap to target: +{gap_low:.2f}%\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå No advanced models found for ranking\")\n",
    "    \n",
    "    # Visualization of model comparison\n",
    "    if len(results_summary) >= 3:\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        \n",
    "        # Overall MAPE comparison\n",
    "        plt.subplot(2, 3, 1)\n",
    "        models = results_summary['model_name']\n",
    "        overall_mapes = results_summary['overall_mape']\n",
    "        \n",
    "        colors = ['red' if mape >= 18 else 'green' for mape in overall_mapes]\n",
    "        bars = plt.bar(range(len(models)), overall_mapes, color=colors, alpha=0.7)\n",
    "        plt.axhline(y=18, color='red', linestyle='--', alpha=0.8, label='Target: 18%')\n",
    "        plt.xlabel('Models')\n",
    "        plt.ylabel('Overall MAPE (%)')\n",
    "        plt.title('Overall MAPE Comparison')\n",
    "        plt.xticks(range(len(models)), [m[:10] + '...' if len(m) > 10 else m for m in models], rotation=45)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2-5L MAPE comparison\n",
    "        if 'low_mape' in results_summary.columns:\n",
    "            plt.subplot(2, 3, 2)\n",
    "            low_mapes = results_summary['low_mape'].fillna(0)\n",
    "            colors_low = ['red' if mape >= 25 else 'green' for mape in low_mapes]\n",
    "            plt.bar(range(len(models)), low_mapes, color=colors_low, alpha=0.7)\n",
    "            plt.axhline(y=25, color='red', linestyle='--', alpha=0.8, label='Target: 25%')\n",
    "            plt.xlabel('Models')\n",
    "            plt.ylabel('2-5L MAPE (%)')\n",
    "            plt.title('2-5L Range MAPE Comparison')\n",
    "            plt.xticks(range(len(models)), [m[:10] + '...' if len(m) > 10 else m for m in models], rotation=45)\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Training time comparison\n",
    "        if 'training_time' in results_summary.columns:\n",
    "            plt.subplot(2, 3, 3)\n",
    "            times = results_summary['training_time'].fillna(0)\n",
    "            plt.bar(range(len(models)), times, alpha=0.7, color='blue')\n",
    "            plt.xlabel('Models')\n",
    "            plt.ylabel('Training Time (seconds)')\n",
    "            plt.title('Training Time Comparison')\n",
    "            plt.xticks(range(len(models)), [m[:10] + '...' if len(m) > 10 else m for m in models], rotation=45)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Range performance radar chart for top 3\n",
    "        if len(advanced_models) >= 3 and all(col in advanced_models.columns for col in ['low_mape', 'mid_mape', 'high_mape']):\n",
    "            plt.subplot(2, 3, 4)\n",
    "            top_3 = advanced_models.head(3)\n",
    "            \n",
    "            ranges = ['2-5L', '5-10L', '10L+']\n",
    "            angles = np.linspace(0, 2 * np.pi, len(ranges), endpoint=False)\n",
    "            angles = np.concatenate((angles, [angles[0]]))\n",
    "            \n",
    "            for i, (idx, model) in enumerate(top_3.iterrows()):\n",
    "                values = [model['low_mape'], model['mid_mape'], model['high_mape']]\n",
    "                values = np.concatenate((values, [values[0]]))\n",
    "                plt.polar(angles, values, 'o-', linewidth=2, label=model['model_name'][:15])\n",
    "            \n",
    "            plt.thetagrids(angles[:-1] * 180/np.pi, ranges)\n",
    "            plt.title('Top 3 Models: Range Performance')\n",
    "            plt.legend()\n",
    "        \n",
    "        # Performance improvement timeline\n",
    "        plt.subplot(2, 3, 5)\n",
    "        if 'timestamp' in results_summary.columns:\n",
    "            sorted_results = results_summary.sort_values('timestamp')\n",
    "            cumulative_best = sorted_results['overall_mape'].cummin()\n",
    "            plt.plot(range(len(cumulative_best)), cumulative_best, 'b-o', alpha=0.7)\n",
    "            plt.axhline(y=18, color='red', linestyle='--', alpha=0.8, label='Target: 18%')\n",
    "            plt.xlabel('Model Sequence')\n",
    "            plt.ylabel('Best MAPE So Far (%)')\n",
    "            plt.title('Performance Improvement Timeline')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Model efficiency scatter plot\n",
    "        if 'training_time' in results_summary.columns:\n",
    "            plt.subplot(2, 3, 6)\n",
    "            plt.scatter(results_summary['training_time'], results_summary['overall_mape'], \n",
    "                       alpha=0.7, s=100, c=range(len(results_summary)), cmap='viridis')\n",
    "            \n",
    "            for i, model in enumerate(results_summary['model_name']):\n",
    "                plt.annotate(model[:8], \n",
    "                           (results_summary.iloc[i]['training_time'], results_summary.iloc[i]['overall_mape']),\n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "            \n",
    "            plt.xlabel('Training Time (seconds)')\n",
    "            plt.ylabel('Overall MAPE (%)')\n",
    "            plt.title('Model Efficiency: MAPE vs Training Time')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ Model comparison and selection complete!\")\n",
    "    print(\"üéØ Ready for ensemble methods with top performing models\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No model results available for comparison\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 11: BEST MODEL DEEP ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîç Deep Analysis of Best Performing Model\")\n",
    "print(\"üéØ Objective: Understand model behavior, strengths, and limitations\")\n",
    "\n",
    "# Get the best model\n",
    "best_model_info = model_tracker.get_best_model()\n",
    "\n",
    "if best_model_info is not None:\n",
    "    best_model_name = best_model_info['model_name']\n",
    "    best_overall_mape = best_model_info['overall_mape']\n",
    "    \n",
    "    print(f\"\\nüèÜ ANALYZING BEST MODEL: {best_model_name}\")\n",
    "    print(f\"üéØ Overall MAPE: {best_overall_mape:.2f}%\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load the best model for detailed analysis\n",
    "    best_model = None\n",
    "    best_model_path = None\n",
    "    feature_importance_df = None\n",
    "    \n",
    "    # Determine model path and load\n",
    "    if 'LightGBM' in best_model_name:\n",
    "        best_model_path = MODELS_DIR / \"lightgbm_optimized.pkl\"\n",
    "        if best_model_path.exists():\n",
    "            with open(best_model_path, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "                best_model = model_data['model']\n",
    "                feature_importance_df = model_data.get('feature_importance', None)\n",
    "            \n",
    "            # Generate predictions for analysis\n",
    "            best_pred_log = best_model.predict(X_val, num_iteration=best_model.best_iteration)\n",
    "            \n",
    "    elif 'XGBoost' in best_model_name:\n",
    "        best_model_path = MODELS_DIR / \"xgboost_optimized.pkl\"\n",
    "        if best_model_path.exists():\n",
    "            with open(best_model_path, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "                best_model = model_data['model']\n",
    "                feature_importance_df = model_data.get('feature_importance', None)\n",
    "            \n",
    "            best_pred_log = best_model.predict(X_val)\n",
    "            \n",
    "    elif 'CatBoost' in best_model_name:\n",
    "        best_model_path = MODELS_DIR / \"catboost_optimized.cbm\"\n",
    "        metadata_path = MODELS_DIR / \"catboost_optimized_metadata.pkl\"\n",
    "        \n",
    "        if best_model_path.exists():\n",
    "            best_model = cb.CatBoostRegressor()\n",
    "            best_model.load_model(str(best_model_path))\n",
    "            \n",
    "            if metadata_path.exists():\n",
    "                with open(metadata_path, 'rb') as f:\n",
    "                    metadata = pickle.load(f)\n",
    "                    feature_importance_df = metadata.get('feature_importance', None)\n",
    "            \n",
    "            best_pred_log = best_model.predict(X_val)\n",
    "            \n",
    "    elif 'Extra Trees' in best_model_name:\n",
    "        best_model_path = MODELS_DIR / \"extratrees_optimized.pkl\"\n",
    "        if best_model_path.exists():\n",
    "            with open(best_model_path, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "                best_model = model_data['model']\n",
    "                feature_importance_df = model_data.get('feature_importance', None)\n",
    "            \n",
    "            best_pred_log = best_model.predict(X_val)\n",
    "    \n",
    "    if best_model is not None:\n",
    "        # Convert predictions to original scale\n",
    "        best_pred_orig = np.expm1(best_pred_log)\n",
    "        y_val_orig = np.expm1(y_val_log)\n",
    "        \n",
    "        # 1. DETAILED PERFORMANCE BREAKDOWN\n",
    "        print(\"üìä DETAILED PERFORMANCE BREAKDOWN\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Overall metrics\n",
    "        overall_mae = mean_absolute_error(y_val_orig, best_pred_orig)\n",
    "        overall_rmse = np.sqrt(mean_squared_error(y_val_orig, best_pred_orig))\n",
    "        overall_r2 = r2_score(y_val_orig, best_pred_orig)\n",
    "        \n",
    "        print(f\"Overall Performance:\")\n",
    "        print(f\"   üéØ MAPE: {best_overall_mape:.2f}%\")\n",
    "        print(f\"   üìè MAE:  ‚Çπ{overall_mae:,.0f}\")\n",
    "        print(f\"   üìê RMSE: ‚Çπ{overall_rmse:,.0f}\")\n",
    "        print(f\"   üìà R¬≤:   {overall_r2:.4f}\")\n",
    "        \n",
    "        # Range-specific detailed analysis\n",
    "        print(f\"\\nIncome Range Analysis:\")\n",
    "        range_labels = {'low': '2-5L', 'mid': '5-10L', 'high': '10L+'}\n",
    "        range_details = {}\n",
    "        \n",
    "        for range_key, mask in income_ranges_val.items():\n",
    "            if mask.sum() > 0:\n",
    "                range_mape = calculate_mape(y_val_orig[mask], best_pred_orig[mask])\n",
    "                range_mae = mean_absolute_error(y_val_orig[mask], best_pred_orig[mask])\n",
    "                range_rmse = np.sqrt(mean_squared_error(y_val_orig[mask], best_pred_orig[mask]))\n",
    "                range_r2 = r2_score(y_val_orig[mask], best_pred_orig[mask])\n",
    "                range_count = mask.sum()\n",
    "                \n",
    "                range_details[range_key] = {\n",
    "                    'mape': range_mape, 'mae': range_mae, 'rmse': range_rmse,\n",
    "                    'r2': range_r2, 'count': range_count\n",
    "                }\n",
    "                \n",
    "                status = \"‚úÖ\" if range_mape < 20 else \"‚ö†Ô∏è\" if range_mape < 25 else \"‚ùå\"\n",
    "                print(f\"   {status} {range_labels[range_key]:>4}: {range_mape:>6.2f}% MAPE | ‚Çπ{range_mae:>8,.0f} MAE | R¬≤={range_r2:>6.3f} | {range_count:>6,} samples\")\n",
    "        \n",
    "        # 2. FEATURE IMPORTANCE ANALYSIS\n",
    "        print(f\"\\nüìä FEATURE IMPORTANCE ANALYSIS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if feature_importance_df is not None:\n",
    "            top_20_features = feature_importance_df.head(20)\n",
    "            print(\"Top 20 Most Important Features:\")\n",
    "            \n",
    "            for i, (_, row) in enumerate(top_20_features.iterrows(), 1):\n",
    "                feature_name = row['feature']\n",
    "                importance = row['importance']\n",
    "                \n",
    "                # Categorize feature type\n",
    "                if any(keyword in feature_name.lower() for keyword in ['crop', 'production', 'yield']):\n",
    "                    category = \"üåæ Agricultural\"\n",
    "                elif any(keyword in feature_name.lower() for keyword in ['weather', 'temp', 'rain']):\n",
    "                    category = \"üå§Ô∏è  Weather\"\n",
    "                elif any(keyword in feature_name.lower() for keyword in ['income', 'financial', 'profit']):\n",
    "                    category = \"üí∞ Financial\"\n",
    "                elif any(keyword in feature_name.lower() for keyword in ['geo', 'location', 'distance']):\n",
    "                    category = \"üìç Geographic\"\n",
    "                elif any(keyword in feature_name.lower() for keyword in ['age', 'education', 'family']):\n",
    "                    category = \"üë• Demographic\"\n",
    "                else:\n",
    "                    category = \"üîß Engineered\"\n",
    "                \n",
    "                print(f\"   {i:>2}. {category} | {feature_name[:40]:.<40} {importance:>8.3f}\")\n",
    "            \n",
    "            # Feature category analysis\n",
    "            print(f\"\\nFeature Category Distribution (Top 20):\")\n",
    "            categories = {}\n",
    "            for _, row in top_20_features.iterrows():\n",
    "                feature_name = row['feature']\n",
    "                if any(keyword in feature_name.lower() for keyword in ['crop', 'production', 'yield']):\n",
    "                    cat = \"Agricultural\"\n",
    "                elif any(keyword in feature_name.lower() for keyword in ['weather', 'temp', 'rain']):\n",
    "                    cat = \"Weather\"\n",
    "                elif any(keyword in feature_name.lower() for keyword in ['income', 'financial', 'profit']):\n",
    "                    cat = \"Financial\"\n",
    "                elif any(keyword in feature_name.lower() for keyword in ['geo', 'location', 'distance']):\n",
    "                    cat = \"Geographic\"\n",
    "                elif any(keyword in feature_name.lower() for keyword in ['age', 'education', 'family']):\n",
    "                    cat = \"Demographic\"\n",
    "                else:\n",
    "                    cat = \"Engineered\"\n",
    "                \n",
    "                categories[cat] = categories.get(cat, 0) + 1\n",
    "            \n",
    "            for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"   üîπ {cat}: {count} features\")\n",
    "        \n",
    "        # 3. ERROR ANALYSIS BY SEGMENTS\n",
    "        print(f\"\\nüìä ERROR ANALYSIS BY SEGMENTS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Calculate errors\n",
    "        errors = np.abs(y_val_orig - best_pred_orig)\n",
    "        percentage_errors = errors / y_val_orig * 100\n",
    "        \n",
    "        # Error quantile analysis\n",
    "        print(\"Error Distribution Analysis:\")\n",
    "        percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "        print(\"   üìä Percentage Error Percentiles:\")\n",
    "        for p in percentiles:\n",
    "            value = np.percentile(percentage_errors, p)\n",
    "            print(f\"      P{p:>2}: {value:>6.2f}%\")\n",
    "        \n",
    "        # High error cases analysis\n",
    "        high_error_threshold = np.percentile(percentage_errors, 95)\n",
    "        high_error_mask = percentage_errors > high_error_threshold\n",
    "        \n",
    "        print(f\"\\nüîç High Error Cases Analysis (>{high_error_threshold:.1f}% error):\")\n",
    "        print(f\"   üìä Total high error cases: {high_error_mask.sum():,} ({high_error_mask.sum()/len(y_val_orig)*100:.1f}%)\")\n",
    "        \n",
    "        # Analyze high error cases by income range\n",
    "        for range_key, range_mask in income_ranges_val.items():\n",
    "            if range_mask.sum() > 0:\n",
    "                high_error_in_range = (high_error_mask & range_mask).sum()\n",
    "                range_total = range_mask.sum()\n",
    "                pct_high_error = high_error_in_range / range_total * 100\n",
    "                print(f\"   üîπ {range_labels[range_key]:>4}: {high_error_in_range:>4,}/{range_total:>5,} ({pct_high_error:>5.1f}%) high error cases\")\n",
    "        \n",
    "        # 4. PREDICTION PATTERN ANALYSIS\n",
    "        print(f\"\\nüìä PREDICTION PATTERN ANALYSIS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Bias analysis\n",
    "        bias = np.mean(best_pred_orig - y_val_orig)\n",
    "        abs_bias = np.mean(np.abs(best_pred_orig - y_val_orig))\n",
    "        \n",
    "        print(f\"Prediction Bias Analysis:\")\n",
    "        print(f\"   üéØ Mean Bias: ‚Çπ{bias:,.0f} ({'Over' if bias > 0 else 'Under'}prediction)\")\n",
    "        print(f\"   üìè Mean Absolute Bias: ‚Çπ{abs_bias:,.0f}\")\n",
    "        \n",
    "        # Range-specific bias\n",
    "        print(f\"\\nBias by Income Range:\")\n",
    "        for range_key, mask in income_ranges_val.items():\n",
    "            if mask.sum() > 0:\n",
    "                range_bias = np.mean(best_pred_orig[mask] - y_val_orig[mask])\n",
    "                direction = \"Over\" if range_bias > 0 else \"Under\"\n",
    "                print(f\"   üîπ {range_labels[range_key]:>4}: ‚Çπ{range_bias:>8,.0f} ({direction}prediction)\")\n",
    "        \n",
    "        # 5. MODEL STRENGTHS AND LIMITATIONS\n",
    "        print(f\"\\nüìä MODEL STRENGTHS AND LIMITATIONS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        print(\"üü¢ STRENGTHS:\")\n",
    "        \n",
    "        # Identify strengths based on performance\n",
    "        if best_overall_mape < 18:\n",
    "            print(\"   ‚úÖ Achieves target overall MAPE < 18%\")\n",
    "        \n",
    "        best_range_performance = min([range_details[k]['mape'] for k in range_details.keys()])\n",
    "        if best_range_performance < 20:\n",
    "            print(\"   ‚úÖ Strong performance in at least one income range\")\n",
    "        \n",
    "        if overall_r2 > 0.8:\n",
    "            print(\"   ‚úÖ High R¬≤ indicates good variance explanation\")\n",
    "        \n",
    "        if 'low' in range_details and range_details['low']['mape'] < 30:\n",
    "            print(\"   ‚úÖ Improved 2-5L range performance from baseline\")\n",
    "        \n",
    "        # Analyze prediction consistency\n",
    "        cv_std = np.std([range_details[k]['mape'] for k in range_details.keys()])\n",
    "        if cv_std < 5:\n",
    "            print(\"   ‚úÖ Consistent performance across income ranges\")\n",
    "        \n",
    "        print(\"\\nüî¥ LIMITATIONS:\")\n",
    "        \n",
    "        # Identify limitations\n",
    "        if best_overall_mape >= 18:\n",
    "            gap = best_overall_mape - 18\n",
    "            print(f\"   ‚ö†Ô∏è  Overall MAPE {gap:.2f}% above target\")\n",
    "        \n",
    "        if 'low' in range_details and range_details['low']['mape'] > 25:\n",
    "            gap_low = range_details['low']['mape'] - 25\n",
    "            print(f\"   ‚ö†Ô∏è  2-5L range MAPE {gap_low:.2f}% above target\")\n",
    "        \n",
    "        if bias > 50000:  # Significant bias\n",
    "            print(f\"   ‚ö†Ô∏è  Systematic overprediction bias of ‚Çπ{bias:,.0f}\")\n",
    "        elif bias < -50000:\n",
    "            print(f\"   ‚ö†Ô∏è  Systematic underprediction bias of ‚Çπ{abs(bias):,.0f}\")\n",
    "        \n",
    "        high_error_rate = high_error_mask.sum() / len(y_val_orig) * 100\n",
    "        if high_error_rate > 10:\n",
    "            print(f\"   ‚ö†Ô∏è  High error rate: {high_error_rate:.1f}% of cases have >{high_error_threshold:.1f}% error\")\n",
    "        \n",
    "        # 6. VISUALIZATION\n",
    "        plt.figure(figsize=(20, 12))\n",
    "        \n",
    "        # Predictions vs Actual by range\n",
    "        plt.subplot(2, 4, 1)\n",
    "        colors = {'low': 'red', 'mid': 'blue', 'high': 'green'}\n",
    "        for range_key, mask in income_ranges_val.items():\n",
    "            if mask.sum() > 0:\n",
    "                y_true_millions = y_val_orig[mask] / 1000000\n",
    "                y_pred_millions = best_pred_orig[mask] / 1000000\n",
    "                plt.scatter(y_true_millions, y_pred_millions, \n",
    "                           alpha=0.6, c=colors[range_key], label=range_labels[range_key], s=15)\n",
    "        \n",
    "        max_val = max(y_val_orig.max(), best_pred_orig.max()) / 1000000\n",
    "        min_val = min(y_val_orig.min(), best_pred_orig.min()) / 1000000\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=2)\n",
    "        plt.xlabel('Actual Income (‚Çπ Millions)')\n",
    "        plt.ylabel('Predicted Income (‚Çπ Millions)')\n",
    "        plt.title(f'{best_model_name}: Predictions vs Actual')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Error distribution\n",
    "        plt.subplot(2, 4, 2)\n",
    "        plt.hist(percentage_errors, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        plt.axvline(best_overall_mape, color='red', linestyle='--', label=f'Mean: {best_overall_mape:.1f}%')\n",
    "        plt.axvline(18, color='green', linestyle='--', label='Target: 18%')\n",
    "        plt.xlabel('Absolute Percentage Error (%)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Error Distribution')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Residuals plot\n",
    "        plt.subplot(2, 4, 3)\n",
    "        residuals = best_pred_orig - y_val_orig\n",
    "        plt.scatter(best_pred_orig/1000000, residuals/1000000, alpha=0.6, s=15)\n",
    "        plt.axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "        plt.xlabel('Predicted Income (‚Çπ Millions)')\n",
    "        plt.ylabel('Residuals (‚Çπ Millions)')\n",
    "        plt.title('Residuals Plot')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Feature importance\n",
    "        if feature_importance_df is not None:\n",
    "            plt.subplot(2, 4, 4)\n",
    "            top_10 = feature_importance_df.head(10)\n",
    "            plt.barh(range(len(top_10)), top_10['importance'])\n",
    "            plt.yticks(range(len(top_10)), [f[:20] + '...' if len(f) > 20 else f for f in top_10['feature']])\n",
    "            plt.xlabel('Importance')\n",
    "            plt.title('Top 10 Feature Importance')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Error by income range\n",
    "        plt.subplot(2, 4, 5)\n",
    "        range_mapes = [range_details[k]['mape'] for k in ['low', 'mid', 'high']]\n",
    "        range_names = ['2-5L', '5-10L', '10L+']\n",
    "        colors_bar = ['red' if x > 20 else 'green' for x in range_mapes]\n",
    "        bars = plt.bar(range_names, range_mapes, color=colors_bar, alpha=0.7)\n",
    "        plt.axhline(y=18, color='green', linestyle='--', alpha=0.8, label='Overall Target')\n",
    "        plt.axhline(y=25, color='orange', linestyle='--', alpha=0.8, label='2-5L Target')\n",
    "        plt.ylabel('MAPE (%)')\n",
    "        plt.title('MAPE by Income Range')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Prediction accuracy by actual income\n",
    "        plt.subplot(2, 4, 6)\n",
    "        income_bins_viz = np.linspace(y_val_orig.min(), y_val_orig.max(), 20)\n",
    "        bin_centers = (income_bins_viz[:-1] + income_bins_viz[1:]) / 2\n",
    "        bin_mapes = []\n",
    "        \n",
    "        for i in range(len(income_bins_viz)-1):\n",
    "            mask = (y_val_orig >= income_bins_viz[i]) & (y_val_orig < income_bins_viz[i+1])\n",
    "            if mask.sum() > 10:  # Enough samples\n",
    "                bin_mape = calculate_mape(y_val_orig[mask], best_pred_orig[mask])\n",
    "                bin_mapes.append(bin_mape)\n",
    "            else:\n",
    "                bin_mapes.append(np.nan)\n",
    "        \n",
    "        valid_bins = ~np.isnan(bin_mapes)\n",
    "        plt.plot(bin_centers[valid_bins]/1000000, np.array(bin_mapes)[valid_bins], 'b-o', alpha=0.7)\n",
    "        plt.axhline(y=18, color='red', linestyle='--', alpha=0.8, label='Target: 18%')\n",
    "        plt.xlabel('Actual Income (‚Çπ Millions)')\n",
    "        plt.ylabel('MAPE (%)')\n",
    "        plt.title('Accuracy vs Income Level')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Training time vs performance comparison\n",
    "        plt.subplot(2, 4, 7)\n",
    "        if len(model_tracker.results) > 1:\n",
    "            all_times = [r.get('training_time', 0) for r in model_tracker.results]\n",
    "            all_mapes = [r['overall_mape'] for r in model_tracker.results]\n",
    "            all_names = [r['model_name'] for r in model_tracker.results]\n",
    "            \n",
    "            plt.scatter(all_times, all_mapes, alpha=0.7, s=100)\n",
    "            \n",
    "            # Highlight best model\n",
    "            best_idx = all_names.index(best_model_name)\n",
    "            plt.scatter(all_times[best_idx], all_mapes[best_idx], \n",
    "                       color='red', s=200, marker='*', label='Best Model')\n",
    "            \n",
    "            plt.xlabel('Training Time (seconds)')\n",
    "            plt.ylabel('Overall MAPE (%)')\n",
    "            plt.title('Efficiency: Performance vs Time')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Model confidence intervals (if applicable)\n",
    "        plt.subplot(2, 4, 8)\n",
    "        sorted_indices = np.argsort(y_val_orig)\n",
    "        sorted_actual = y_val_orig[sorted_indices]\n",
    "        sorted_pred = best_pred_orig[sorted_indices]\n",
    "        \n",
    "        # Calculate moving average for smoothing\n",
    "        window = len(sorted_actual) // 50\n",
    "        if window > 1:\n",
    "            smooth_actual = np.convolve(sorted_actual, np.ones(window)/window, mode='valid')\n",
    "            smooth_pred = np.convolve(sorted_pred, np.ones(window)/window, mode='valid')\n",
    "            \n",
    "            plt.plot(smooth_actual/1000000, smooth_pred/1000000, 'b-', alpha=0.8, label='Predicted')\n",
    "            plt.plot(smooth_actual/1000000, smooth_actual/1000000, 'r--', alpha=0.8, label='Perfect')\n",
    "            plt.xlabel('Actual Income (‚Çπ Millions)')\n",
    "            plt.ylabel('Predicted Income (‚Çπ Millions)')\n",
    "            plt.title('Prediction Smoothness')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 7. BUSINESS INSIGHTS\n",
    "        print(f\"\\nüíº BUSINESS INSIGHTS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if feature_importance_df is not None:\n",
    "            top_5_features = feature_importance_df.head(5)['feature'].tolist()\n",
    "            \n",
    "            print(\"Key Predictive Factors:\")\n",
    "            for i, feature in enumerate(top_5_features, 1):\n",
    "                if 'crop' in feature.lower() or 'production' in feature.lower():\n",
    "                    insight = \"Agricultural productivity is a key income driver\"\n",
    "                elif 'weather' in feature.lower() or 'rain' in feature.lower():\n",
    "                    insight = \"Weather patterns significantly impact farmer income\"\n",
    "                elif 'financial' in feature.lower() or 'income' in feature.lower():\n",
    "                    insight = \"Financial diversification affects income stability\"\n",
    "                elif 'geo' in feature.lower() or 'location' in feature.lower():\n",
    "                    insight = \"Geographic location influences income potential\"\n",
    "                else:\n",
    "                    insight = \"This factor contributes to income prediction\"\n",
    "                \n",
    "                print(f\"   {i}. {feature[:30]:<30} ‚Üí {insight}\")\n",
    "        \n",
    "        print(f\"\\nRecommendations for Income Improvement:\")\n",
    "        \n",
    "        if 'low' in range_details and range_details['low']['mape'] > 25:\n",
    "            print(\"   üéØ Focus interventions on 2-5L income farmers (highest prediction error)\")\n",
    "        \n",
    "        print(\"   üåæ Enhance agricultural productivity programs\")\n",
    "        print(\"   üå§Ô∏è  Improve weather resilience and forecasting\")\n",
    "        print(\"   üí∞ Promote income diversification strategies\")\n",
    "        print(\"   üìç Target location-specific interventions\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"‚úÖ Deep analysis of {best_model_name} complete!\")\n",
    "        print(\"üéØ Model ready for ensemble combination and final predictions\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Could not load best model from {best_model_path}\")\n",
    "        print(\"   Please ensure model files are properly saved\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No best model found for analysis\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b2f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNK 12: FINAL VALIDATION & TEST PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üéØ Final Validation & Test Predictions Generation\")\n",
    "print(\"üèÜ Objective: Validate best models and generate submission predictions\")\n",
    "\n",
    "# Get top 3 models for final validation\n",
    "results_summary = model_tracker.get_summary()\n",
    "if results_summary is not None and len(results_summary) > 0:\n",
    "    \n",
    "    # Filter advanced models and get top 3\n",
    "    advanced_models = results_summary[\n",
    "        ~results_summary['model_name'].str.contains('Baseline|Mean|Median', case=False, na=False)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(advanced_models) >= 3:\n",
    "        top_3_models = advanced_models.head(3)\n",
    "        model_names = top_3_models['model_name'].tolist()\n",
    "        \n",
    "        print(f\"\\nüèÜ TOP 3 MODELS FOR FINAL VALIDATION:\")\n",
    "        for i, name in enumerate(model_names, 1):\n",
    "            mape = top_3_models.iloc[i-1]['overall_mape']\n",
    "            print(f\"   {i}. {name}: {mape:.2f}% MAPE\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        \n",
    "        # Load models and generate predictions\n",
    "        final_models = {}\n",
    "        final_predictions_val = {}\n",
    "        final_predictions_test = {}\n",
    "        \n",
    "        print(\"üìä LOADING MODELS AND GENERATING PREDICTIONS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            print(f\"\\nüîÑ Processing {model_name}...\")\n",
    "            \n",
    "            try:\n",
    "                if 'LightGBM' in model_name:\n",
    "                    model_path = MODELS_DIR / \"lightgbm_optimized.pkl\"\n",
    "                    with open(model_path, 'rb') as f:\n",
    "                        model_data = pickle.load(f)\n",
    "                        model = model_data['model']\n",
    "                    \n",
    "                    pred_val_log = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "                    pred_test_log = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "                    \n",
    "                elif 'XGBoost' in model_name:\n",
    "                    model_path = MODELS_DIR / \"xgboost_optimized.pkl\"\n",
    "                    with open(model_path, 'rb') as f:\n",
    "                        model_data = pickle.load(f)\n",
    "                        model = model_data['model']\n",
    "                    \n",
    "                    pred_val_log = model.predict(X_val)\n",
    "                    pred_test_log = model.predict(X_test)\n",
    "                    \n",
    "                elif 'CatBoost' in model_name:\n",
    "                    model_path = MODELS_DIR / \"catboost_optimized.cbm\"\n",
    "                    model = cb.CatBoostRegressor()\n",
    "                    model.load_model(str(model_path))\n",
    "                    \n",
    "                    pred_val_log = model.predict(X_val)\n",
    "                    pred_test_log = model.predict(X_test)\n",
    "                    \n",
    "                elif 'Extra Trees' in model_name:\n",
    "                    model_path = MODELS_DIR / \"extratrees_optimized.pkl\"\n",
    "                    with open(model_path, 'rb') as f:\n",
    "                        model_data = pickle.load(f)\n",
    "                        model = model_data['model']\n",
    "                    \n",
    "                    pred_val_log = model.predict(X_val)\n",
    "                    pred_test_log = model.predict(X_test)\n",
    "                \n",
    "                # Store models and predictions\n",
    "                final_models[model_name] = model\n",
    "                final_predictions_val[model_name] = np.expm1(pred_val_log)  # Convert to original scale\n",
    "                final_predictions_test[model_name] = np.expm1(pred_test_log)  # Convert to original scale\n",
    "                \n",
    "                # Validate predictions\n",
    "                val_mape = calculate_mape(np.expm1(y_val_log), final_predictions_val[model_name])\n",
    "                print(f\"   ‚úÖ Validation MAPE: {val_mape:.2f}%\")\n",
    "                print(f\"   üìä Test predictions range: ‚Çπ{final_predictions_test[model_name].min():,.0f} - ‚Çπ{final_predictions_test[model_name].max():,.0f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error loading {model_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n‚úÖ Successfully loaded {len(final_models)} models\")\n",
    "        \n",
    "        # CROSS-VALIDATION ON BEST MODEL\n",
    "        print(\"\\nüìä CROSS-VALIDATION ON BEST MODEL\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        best_model_name = model_names[0]  # Top model\n",
    "        if best_model_name in final_models:\n",
    "            print(f\"üîÑ Performing 5-fold CV on {best_model_name}...\")\n",
    "            \n",
    "            cv_scores = []\n",
    "            cv_range_scores = {'low': [], 'mid': [], 'high': []}\n",
    "            \n",
    "            for fold, (train_idx, val_idx) in enumerate(cv_splitter.split(X_train, income_bins), 1):\n",
    "                X_cv_train, X_cv_val = X_train[train_idx], X_train[val_idx]\n",
    "                y_cv_train, y_cv_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "                weights_cv_train = sample_weights[train_idx]\n",
    "                \n",
    "                # Train model on fold\n",
    "                if 'LightGBM' in best_model_name:\n",
    "                    train_data = lgb.Dataset(X_cv_train, label=y_cv_train, weight=weights_cv_train)\n",
    "                    cv_model = lgb.train(\n",
    "                        final_models[best_model_name].params,\n",
    "                        train_data,\n",
    "                        num_boost_round=1000,\n",
    "                        valid_sets=[train_data],\n",
    "                        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
    "                    )\n",
    "                    cv_pred_log = cv_model.predict(X_cv_val, num_iteration=cv_model.best_iteration)\n",
    "                    \n",
    "                elif 'XGBoost' in best_model_name:\n",
    "                    cv_model = xgb.XGBRegressor(**final_models[best_model_name].get_params())\n",
    "                    cv_model.fit(X_cv_train, y_cv_train, sample_weight=weights_cv_train,\n",
    "                               eval_set=[(X_cv_train, y_cv_train)], early_stopping_rounds=100, verbose=False)\n",
    "                    cv_pred_log = cv_model.predict(X_cv_val)\n",
    "                    \n",
    "                elif 'CatBoost' in best_model_name:\n",
    "                    train_pool = cb.Pool(X_cv_train, y_cv_train, weight=weights_cv_train)\n",
    "                    cv_model = cb.CatBoostRegressor(**final_models[best_model_name].get_all_params())\n",
    "                    cv_model.fit(train_pool, verbose=False)\n",
    "                    cv_pred_log = cv_model.predict(X_cv_val)\n",
    "                    \n",
    "                elif 'Extra Trees' in best_model_name:\n",
    "                    cv_model = ExtraTreesRegressor(**final_models[best_model_name].get_params())\n",
    "                    cv_model.fit(X_cv_train, y_cv_train, sample_weight=weights_cv_train)\n",
    "                    cv_pred_log = cv_model.predict(X_cv_val)\n",
    "                \n",
    "                # Evaluate fold\n",
    "                cv_true_orig = np.expm1(y_cv_val)\n",
    "                cv_pred_orig = np.expm1(cv_pred_log)\n",
    "                fold_mape = calculate_mape(cv_true_orig, cv_pred_orig)\n",
    "                cv_scores.append(fold_mape)\n",
    "                \n",
    "                # Range-specific evaluation\n",
    "                for range_key, range_mask_full in income_ranges.items():\n",
    "                    # Map range mask to current fold\n",
    "                    range_mask_fold = range_mask_full[val_idx]\n",
    "                    if range_mask_fold.sum() > 0:\n",
    "                        range_mape = calculate_mape(cv_true_orig[range_mask_fold], cv_pred_orig[range_mask_fold])\n",
    "                        cv_range_scores[range_key].append(range_mape)\n",
    "                \n",
    "                print(f\"   Fold {fold}: {fold_mape:.2f}% MAPE\")\n",
    "            \n",
    "            # CV Results Summary\n",
    "            cv_mean = np.mean(cv_scores)\n",
    "            cv_std = np.std(cv_scores)\n",
    "            \n",
    "            print(f\"\\nüìä Cross-Validation Results:\")\n",
    "            print(f\"   üéØ Mean MAPE: {cv_mean:.2f} ¬± {cv_std:.2f}%\")\n",
    "            print(f\"   üìä Individual folds: {[f'{score:.2f}%' for score in cv_scores]}\")\n",
    "            \n",
    "            # Range-specific CV results\n",
    "            print(f\"\\nüéØ Range-Specific CV Results:\")\n",
    "            range_labels = {'low': '2-5L', 'mid': '5-10L', 'high': '10L+'}\n",
    "            for range_key, scores in cv_range_scores.items():\n",
    "                if scores:\n",
    "                    range_mean = np.mean(scores)\n",
    "                    range_std = np.std(scores)\n",
    "                    print(f\"   üîπ {range_labels[range_key]:>4}: {range_mean:.2f} ¬± {range_std:.2f}%\")\n",
    "            \n",
    "            # Statistical significance test\n",
    "            from scipy import stats\n",
    "            if len(cv_scores) >= 3:\n",
    "                # Test if significantly different from 18%\n",
    "                t_stat, p_value = stats.ttest_1samp(cv_scores, 18)\n",
    "                print(f\"\\nüìà Statistical Analysis:\")\n",
    "                print(f\"   üßÆ T-test vs 18% target: t={t_stat:.3f}, p={p_value:.3f}\")\n",
    "                if p_value < 0.05:\n",
    "                    if cv_mean < 18:\n",
    "                        print(f\"   ‚úÖ Significantly better than 18% target (p<0.05)\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ö†Ô∏è  Significantly worse than 18% target (p<0.05)\")\n",
    "                else:\n",
    "                    print(f\"   üìä No significant difference from 18% target (p‚â•0.05)\")\n",
    "        \n",
    "        # ENSEMBLE PREDICTIONS\n",
    "        print(\"\\nü§ñ SIMPLE ENSEMBLE PREDICTIONS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if len(final_predictions_val) >= 2:\n",
    "            # Simple average ensemble\n",
    "            ensemble_val = np.mean(list(final_predictions_val.values()), axis=0)\n",
    "            ensemble_test = np.mean(list(final_predictions_test.values()), axis=0)\n",
    "            \n",
    "            # Evaluate ensemble\n",
    "            ensemble_mape = calculate_mape(np.expm1(y_val_log), ensemble_val)\n",
    "            \n",
    "            print(f\"üìä Simple Average Ensemble:\")\n",
    "            print(f\"   üéØ Validation MAPE: {ensemble_mape:.2f}%\")\n",
    "            \n",
    "            # Range-specific ensemble evaluation\n",
    "            ensemble_range_metrics = {}\n",
    "            for range_key, mask in income_ranges_val.items():\n",
    "                if mask.sum() > 0:\n",
    "                    range_mape = calculate_mape(np.expm1(y_val_log)[mask], ensemble_val[mask])\n",
    "                    ensemble_range_metrics[range_key] = range_mape\n",
    "                    print(f\"   üîπ {range_labels[range_key]:>4}: {range_mape:.2f}% MAPE\")\n",
    "            \n",
    "            # Store ensemble results\n",
    "            final_predictions_val['Simple Ensemble'] = ensemble_val\n",
    "            final_predictions_test['Simple Ensemble'] = ensemble_test\n",
    "            \n",
    "            # Track ensemble performance\n",
    "            model_tracker.add_result(\"Simple Ensemble\", ensemble_mape, ensemble_range_metrics)\n",
    "        \n",
    "        # GENERATE FINAL SUBMISSION FILES\n",
    "        print(\"\\nüíæ GENERATING SUBMISSION FILES\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        submissions_dir = RESULTS_DIR / \"submissions\"\n",
    "        submissions_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        for model_name, test_predictions in final_predictions_test.items():\n",
    "            # Create submission dataframe\n",
    "            submission_df = pd.DataFrame({\n",
    "                'id': range(len(test_predictions)),\n",
    "                'predicted_income': test_predictions.round(2)\n",
    "            })\n",
    "            \n",
    "            # Save submission file\n",
    "            filename = f\"submission_{model_name.lower().replace(' ', '_')}.csv\"\n",
    "            file_path = submissions_dir / filename\n",
    "            submission_df.to_csv(file_path, index=False)\n",
    "            \n",
    "            print(f\"   üìÑ {model_name}: {file_path}\")\n",
    "            print(f\"      üìä Predictions range: ‚Çπ{test_predictions.min():,.0f} - ‚Çπ{test_predictions.max():,.0f}\")\n",
    "            print(f\"      üìà Mean prediction: ‚Çπ{test_predictions.mean():,.0f}\")\n",
    "        \n",
    "        # FINAL RECOMMENDATIONS\n",
    "        print(\"\\nüèÜ FINAL MODEL RECOMMENDATIONS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Get final best model\n",
    "        final_best = model_tracker.get_best_model()\n",
    "        if final_best:\n",
    "            print(f\"ü•á RECOMMENDED MODEL: {final_best['model_name']}\")\n",
    "            print(f\"   üéØ Overall MAPE: {final_best['overall_mape']:.2f}%\")\n",
    "            \n",
    "            if final_best['overall_mape'] < 18:\n",
    "                print(f\"   ‚úÖ MISSION ACCOMPLISHED: Target <18% MAPE achieved!\")\n",
    "            else:\n",
    "                gap = final_best['overall_mape'] - 18\n",
    "                print(f\"   üìä Gap to target: +{gap:.2f}% MAPE\")\n",
    "            \n",
    "            # 2-5L range assessment\n",
    "            if 'low_mape' in final_best and final_best['low_mape'] is not None:\n",
    "                print(f\"   üî¥ 2-5L range MAPE: {final_best['low_mape']:.2f}%\")\n",
    "                if final_best['low_mape'] < 25:\n",
    "                    print(f\"   ‚úÖ 2-5L range target achieved!\")\n",
    "                else:\n",
    "                    gap_low = final_best['low_mape'] - 25\n",
    "                    print(f\"   üìä 2-5L gap to target: +{gap_low:.2f}%\")\n",
    "        \n",
    "        print(f\"\\nüìã SUBMISSION STRATEGY:\")\n",
    "        print(f\"   üéØ Primary: Best individual model ({final_best['model_name'] if final_best else 'Unknown'})\")\n",
    "        if 'Simple Ensemble' in final_predictions_test:\n",
    "            ensemble_performance = [r for r in model_tracker.results if r['model_name'] == 'Simple Ensemble']\n",
    "            if ensemble_performance:\n",
    "                ensemble_mape = ensemble_performance[0]['overall_mape']\n",
    "                print(f\"   ü§ñ Alternative: Simple ensemble ({ensemble_mape:.2f}% MAPE)\")\n",
    "        print(f\"   üìà Backup: Top 3 individual models available\")\n",
    "        \n",
    "        # FINAL STATISTICS\n",
    "        print(f\"\\nüìä FINAL EXPERIMENT STATISTICS\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"   üî¢ Total models trained: {len(model_tracker.results)}\")\n",
    "        print(f\"   ‚è±Ô∏è  Total training time: {sum(r.get('training_time', 0) for r in model_tracker.results):.1f} seconds\")\n",
    "        print(f\"   üéØ Models achieving <18% MAPE: {sum(1 for r in model_tracker.results if r['overall_mape'] < 18)}\")\n",
    "        print(f\"   üî¥ Models achieving <25% 2-5L MAPE: {sum(1 for r in model_tracker.results if r.get('low_mape', float('inf')) < 25)}\")\n",
    "        \n",
    "        # Success assessment\n",
    "        success_criteria = {\n",
    "            'target_mape': final_best['overall_mape'] < 18 if final_best else False,\n",
    "            'range_improvement': final_best.get('low_mape', 50) < 35 if final_best else False,  # Significant improvement from 41%\n",
    "            'models_trained': len(model_tracker.results) >= 8,\n",
    "            'ensemble_created': 'Simple Ensemble' in final_predictions_test\n",
    "        }\n",
    "        \n",
    "        success_count = sum(success_criteria.values())\n",
    "        total_criteria = len(success_criteria)\n",
    "        \n",
    "        print(f\"\\nüèÜ SUCCESS ASSESSMENT: {success_count}/{total_criteria} criteria met\")\n",
    "        for criterion, achieved in success_criteria.items():\n",
    "            status = \"‚úÖ\" if achieved else \"‚ùå\"\n",
    "            print(f\"   {status} {criterion.replace('_', ' ').title()}\")\n",
    "        \n",
    "        if success_count >= 3:\n",
    "            print(f\"\\nüéâ EXPERIMENT SUCCESSFUL!\")\n",
    "            print(f\"   üöÄ Ready for production deployment\")\n",
    "        else:\n",
    "            print(f\"\\nüìä EXPERIMENT PARTIALLY SUCCESSFUL\")\n",
    "            print(f\"   üîß Consider advanced ensemble methods in next notebook\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Insufficient advanced models for final validation\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No model results available for final validation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ MODELING EXPERIMENTS COMPLETE!\")\n",
    "print(\"üéØ All predictions generated and saved\")\n",
    "print(\"üìÅ Check results/submissions/ for submission files\")\n",
    "print(\"üöÄ Proceed to Notebook 5 for advanced ensemble methods\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
