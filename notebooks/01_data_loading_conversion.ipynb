{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78971dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Directory structure created successfully!\n",
      " Base directory: c:\\LnT Project\\lt_finance_farmer_prediction\\notebooks\\..\\data\n",
      "External directory: c:\\LnT Project\\lt_finance_farmer_prediction\\notebooks\\..\\data\\external\\original_data\n"
     ]
    }
   ],
   "source": [
    "# L&T Finance Pearl Challenge - Farmer Income Prediction\n",
    "\n",
    "# ==============================================================================\n",
    "# 01_data_loading_conversion.ipynb\n",
    "# Purpose: Load raw data, verify, convert, and save for further analysis/modeling\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from typing import Dict, List, Tuple, Any, Union\n",
    "import re\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "DATA_URL = \"https://www.ltfraise.com/Pdf/Pearl%20Challenge%20data%20with%20dictionary_For_Share_v4.xlsx\"\n",
    "\n",
    "BASE_DIR = Path('../data')\n",
    "RAW_DATA_DIR = BASE_DIR / \"raw\"\n",
    "EXTERNAL_DATA_DIR = BASE_DIR / \"external\" / \"original_data\"\n",
    "\n",
    "print(\"\\n Directory structure created successfully!\")\n",
    "print(f\" Base directory: {BASE_DIR.absolute()}\")\n",
    "print(f\"External directory: {EXTERNAL_DATA_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6876b168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Checking for existing Excel file...\n",
      " File already exists: ..\\data\\external\\original_data\\Pearl Challenge data with dictionary_For_Share_v4.xlsx\n",
      " File size: 46,097,184 bytes\n",
      " Found sheets: ['TrainData', 'TestData', 'Dictionary']\n",
      " Sheet 'TrainData': 105 columns, sample read successful\n",
      " Sheet 'TestData': 104 columns, sample read successful\n",
      " Sheet 'Dictionary': 3 columns, sample read successful\n",
      " Existing file verification passed!\n",
      "\n",
      " Excel file ready for processing: ..\\data\\external\\original_data\\Pearl Challenge data with dictionary_For_Share_v4.xlsx\n",
      " Final file size: 46,097,184 bytes\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FILE DOWNLOAD AND VERIFICATION WITH ERROR HANDLING\n",
    "# =============================================================================\n",
    "# Purpose: Download Excel file with retry logic and verify integrity\n",
    "# Handles: Network errors, file corruption, path issues\n",
    "# Output: Verified Excel file ready for loading\n",
    "# =============================================================================\n",
    "\n",
    "def download_file_with_retry(url: str, filepath: Path, max_retries: int = 3) -> bool:\n",
    "    \"\"\"\n",
    "    Download file with retry logic and error handling\n",
    "    \n",
    "    Args:\n",
    "        url: Source URL for download\n",
    "        filepath: Destination file path\n",
    "        max_retries: Maximum number of retry attempts\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if download successful, False otherwise\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\" Download attempt {attempt + 1}/{max_retries}...\")\n",
    "            \n",
    "            # Configure request with headers to avoid blocking\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, headers=headers, timeout=30, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Write file in chunks to handle large files\n",
    "            with open(filepath, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            \n",
    "            # Verify file size\n",
    "            file_size = filepath.stat().st_size\n",
    "            if file_size < 1000:  # File too small, likely error page\n",
    "                print(f\" Downloaded file too small ({file_size} bytes), retrying...\")\n",
    "                continue\n",
    "                \n",
    "            print(f\" Download successful! File size: {file_size:,} bytes\")\n",
    "            return True\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\" Download attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\" Waiting 5 seconds before retry...\")\n",
    "                time.sleep(5)\n",
    "        except Exception as e:\n",
    "            print(f\" Unexpected error during download: {e}\")\n",
    "            break\n",
    "    \n",
    "    return False\n",
    "\n",
    "def verify_excel_file(filepath: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Verify Excel file integrity and structure\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to Excel file\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if file is valid Excel with expected sheets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to read Excel file metadata\n",
    "        excel_file = pd.ExcelFile(filepath)\n",
    "        sheet_names = excel_file.sheet_names\n",
    "        \n",
    "        print(f\" Found sheets: {sheet_names}\")\n",
    "        \n",
    "        # Check for expected sheets\n",
    "        expected_sheets = ['TrainData', 'TestData', 'Dictionary']\n",
    "        missing_sheets = [sheet for sheet in expected_sheets if sheet not in sheet_names]\n",
    "        \n",
    "        if missing_sheets:\n",
    "            print(f\" Missing expected sheets: {missing_sheets}\")\n",
    "            return False\n",
    "        \n",
    "        # Quick check - try to read first few rows of each sheet\n",
    "        for sheet in expected_sheets:\n",
    "            sample_df = pd.read_excel(filepath, sheet_name=sheet, nrows=5)\n",
    "            print(f\" Sheet '{sheet}': {sample_df.shape[1]} columns, sample read successful\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Excel file verification failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Main download logic\n",
    "excel_filepath = EXTERNAL_DATA_DIR / \"Pearl Challenge data with dictionary_For_Share_v4.xlsx\"\n",
    "\n",
    "print(\" Checking for existing Excel file...\")\n",
    "if excel_filepath.exists():\n",
    "    print(f\" File already exists: {excel_filepath}\")\n",
    "    print(f\" File size: {excel_filepath.stat().st_size:,} bytes\")\n",
    "    \n",
    "    # Verify existing file\n",
    "    if verify_excel_file(excel_filepath):\n",
    "        print(\" Existing file verification passed!\")\n",
    "        file_ready = True\n",
    "    else:\n",
    "        print(\" Existing file verification failed, will re-download...\")\n",
    "        file_ready = False\n",
    "else:\n",
    "    print(\" File not found, will download...\")\n",
    "    file_ready = False\n",
    "\n",
    "# Download if needed\n",
    "if not file_ready:\n",
    "    print(f\"\\n Starting download from: {DATA_URL}\")\n",
    "    \n",
    "    if download_file_with_retry(DATA_URL, excel_filepath):\n",
    "        if verify_excel_file(excel_filepath):\n",
    "            print(\" Download and verification successful!\")\n",
    "            file_ready = True\n",
    "        else:\n",
    "            print(\" Downloaded file failed verification!\")\n",
    "            file_ready = False\n",
    "    else:\n",
    "        print(\" Download failed after all retries!\")\n",
    "        file_ready = False\n",
    "\n",
    "if not file_ready:\n",
    "    raise FileNotFoundError(\"Unable to obtain valid Excel file. Please check URL and network connection.\")\n",
    "\n",
    "print(f\"\\n Excel file ready for processing: {excel_filepath}\")\n",
    "print(f\" Final file size: {excel_filepath.stat().st_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a49f8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading Excel sheets with type management...\n",
      "\n",
      " Loading sheet: TrainData\n",
      "   Initial shape: (53306, 105)\n",
      "   Columns: 105\n",
      "   Processed shape: (53306, 105)\n",
      "   Memory usage: 339,197,364 bytes\n",
      "   Data types: 1 unique types\n",
      "\n",
      " Loading sheet: TestData\n",
      "   Initial shape: (10000, 104)\n",
      "   Columns: 104\n",
      "   Processed shape: (10000, 104)\n",
      "   Memory usage: 63,102,081 bytes\n",
      "   Data types: 1 unique types\n",
      "\n",
      " LOADING SUMMARY:\n",
      "   Train data: 53,306 rows × 105 columns\n",
      "   Test data: 10,000 rows × 104 columns\n",
      "   Total memory: 402,299,445 bytes\n",
      "\n",
      " DATA TYPE VALIDATION:\n",
      "   Train data types: {'object'}\n",
      "   Test data types: {'object'}\n",
      "   Common types: {'object'}\n",
      "   Data types consistent between train and test\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXCEL DATA LOADING WITH TYPE MANAGEMENT\n",
    "# =============================================================================\n",
    "# Purpose: Load 3 Excel sheets with proper dtype handling and type conversion\n",
    "# Handles: Mixed data types, object columns, categorical variables, encoding\n",
    "# Output: Clean DataFrames with consistent data types\n",
    "# =============================================================================\n",
    "\n",
    "def convert_mixed_dtype_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert mixed data type columns to appropriate types for processing\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with potentially mixed dtypes\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with cleaned data types\n",
    "    \"\"\"\n",
    "    df_converted = df.copy()\n",
    "    \n",
    "    for col in df_converted.columns:\n",
    "        # Handle object columns that might be numeric\n",
    "        if df_converted[col].dtype == 'object':\n",
    "            # Try to convert to numeric, keep as string if fails\n",
    "            try:\n",
    "                # First, handle any string representations of numbers\n",
    "                numeric_series = pd.to_numeric(df_converted[col], errors='coerce')\n",
    "                \n",
    "                # If conversion successful for most values, use it\n",
    "                if numeric_series.notna().sum() > len(df_converted) * 0.8:\n",
    "                    df_converted[col] = numeric_series\n",
    "                    print(f\"   Converted '{col}' from object to numeric\")\n",
    "                else:\n",
    "                    # Keep as string but ensure consistent encoding\n",
    "                    df_converted[col] = df_converted[col].astype(str).replace('nan', np.nan)\n",
    "                    print(f\"   Kept '{col}' as string (object)\")\n",
    "            except:\n",
    "                # Keep as string if conversion fails\n",
    "                df_converted[col] = df_converted[col].astype(str).replace('nan', np.nan)\n",
    "                print(f\"   Kept '{col}' as string (conversion failed)\")\n",
    "    \n",
    "    return df_converted\n",
    "\n",
    "def ensure_json_serializable_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure all DataFrame values are JSON serializable\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to convert\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with JSON-compatible types\n",
    "    \"\"\"\n",
    "    df_json = df.copy()\n",
    "    \n",
    "    for col in df_json.columns:\n",
    "        if df_json[col].dtype == 'int64':\n",
    "            # Convert numpy int64 to int32 for JSON compatibility and memory efficiency\n",
    "            df_json[col] = df_json[col].astype('Int32')  # Nullable integer\n",
    "        elif df_json[col].dtype == 'float64':\n",
    "            # Convert to float32 for memory efficiency\n",
    "            df_json[col] = df_json[col].astype('float32')\n",
    "        elif pd.api.types.is_categorical_dtype(df_json[col]):\n",
    "            # Convert categorical to string\n",
    "            df_json[col] = df_json[col].astype(str)\n",
    "            print(f\"   Converted categorical '{col}' to string\")\n",
    "    \n",
    "    return df_json\n",
    "\n",
    "def load_excel_sheet_with_types(filepath: Path, sheet_name: str) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load Excel sheet with comprehensive type handling\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to Excel file\n",
    "        sheet_name: Name of sheet to load\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (DataFrame, metadata dict)\n",
    "    \"\"\"\n",
    "    print(f\"\\n Loading sheet: {sheet_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Load with minimal dtype inference first\n",
    "        df = pd.read_excel(filepath, sheet_name=sheet_name, dtype=str)\n",
    "        original_shape = df.shape\n",
    "        print(f\"   Initial shape: {original_shape}\")\n",
    "        print(f\"   Columns: {len(df.columns)}\")\n",
    "        \n",
    "        # Create metadata\n",
    "        metadata = {\n",
    "            'sheet_name': sheet_name,\n",
    "            'shape': df.shape,\n",
    "            'columns': len(df.columns),\n",
    "            'rows': len(df),\n",
    "            'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()},\n",
    "            'missing_values': df.isnull().sum().to_dict(),\n",
    "            'memory_usage': df.memory_usage(deep=True).sum()\n",
    "        }\n",
    "        \n",
    "        # Convert numpy types in metadata for JSON serialization\n",
    "        metadata['missing_values'] = {\n",
    "            k: int(v) if isinstance(v, (np.integer, np.int64)) else v \n",
    "            for k, v in metadata['missing_values'].items()\n",
    "        }\n",
    "        metadata['memory_usage'] = int(metadata['memory_usage'])\n",
    "        \n",
    "        print(f\"   Processed shape: {df.shape}\")\n",
    "        print(f\"   Memory usage: {metadata['memory_usage']:,} bytes\")\n",
    "        print(f\"   Data types: {len(set(df.dtypes.astype(str)))} unique types\")\n",
    "        \n",
    "        return df, metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Error loading sheet '{sheet_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "# Load all three sheets\n",
    "print(\" Loading Excel sheets with type management...\")\n",
    "\n",
    "# Load TrainData\n",
    "train_df, train_metadata = load_excel_sheet_with_types(excel_filepath, 'TrainData')\n",
    "\n",
    "# Load TestData  \n",
    "test_df, test_metadata = load_excel_sheet_with_types(excel_filepath, 'TestData')\n",
    "\n",
    "# Create comprehensive loading report\n",
    "loading_report = {\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'source_file': str(excel_filepath),\n",
    "    'sheets_loaded': {\n",
    "        'train': train_metadata,\n",
    "        'test': test_metadata,\n",
    "    },\n",
    "    'total_memory_usage': train_metadata['memory_usage'] + test_metadata['memory_usage']\n",
    "}\n",
    "\n",
    "print(f\"\\n LOADING SUMMARY:\")\n",
    "print(f\"   Train data: {train_df.shape[0]:,} rows × {train_df.shape[1]} columns\")\n",
    "print(f\"   Test data: {test_df.shape[0]:,} rows × {test_df.shape[1]} columns\")  \n",
    "print(f\"   Total memory: {loading_report['total_memory_usage']:,} bytes\")\n",
    "\n",
    "# Quick data type validation\n",
    "print(f\"\\n DATA TYPE VALIDATION:\")\n",
    "train_dtypes = set(train_df.dtypes.astype(str))\n",
    "test_dtypes = set(test_df.dtypes.astype(str))\n",
    "print(f\"   Train data types: {train_dtypes}\")\n",
    "print(f\"   Test data types: {test_dtypes}\")\n",
    "print(f\"   Common types: {train_dtypes.intersection(test_dtypes)}\")\n",
    "\n",
    "if train_dtypes != test_dtypes:\n",
    "    print(f\"   Type differences detected - will address in preprocessing\")\n",
    "else:\n",
    "    print(f\"   Data types consistent between train and test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21505966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting feature name sanitization process...\n",
      "\n",
      " Sanitizing column names for training data...\n",
      "   Original columns: 105\n",
      "   Sanitized columns: 105\n",
      "   Duplicates handled: 0\n",
      "   Target variable found: True\n",
      "   Sample mappings:\n",
      "    'FarmerID' → 'farmerid'\n",
      "    'State' → 'state'\n",
      "    'REGION' → 'region'\n",
      "    'SEX' → 'sex'\n",
      "    'CITY' → 'city'\n",
      "\n",
      " Sanitizing column names for test data...\n",
      "   Original columns: 104\n",
      "   Sanitized columns: 104\n",
      "   Duplicates handled: 0\n",
      "   Target variable found: False\n",
      "   Sample mappings:\n",
      "    'FarmerID' → 'farmerid'\n",
      "    'State' → 'state'\n",
      "    'REGION' → 'region'\n",
      "    'SEX' → 'sex'\n",
      "    'CITY' → 'city'\n",
      "\n",
      " COLUMN CONSISTENCY VALIDATION:\n",
      "   Train columns: 105\n",
      "   Test columns: 104\n",
      "   Common columns: 104\n",
      "   Target in train: True\n",
      "   Target in test: False\n",
      "   Column mismatch detected:\n",
      "     Train-only: ['target_income']\n",
      "\n",
      " Feature name sanitization completed successfully!\n",
      " Comprehensive mapping created with 4 sections\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FEATURE NAME SANITIZATION WITH TYPE SAFETY\n",
    "# =============================================================================\n",
    "# Purpose: Clean column names while preserving data integrity and uniqueness\n",
    "# Handles: Special characters, duplicates, target variable, JSON serialization\n",
    "# Output: Sanitized DataFrames and JSON-compatible feature mapping\n",
    "# =============================================================================\n",
    "\n",
    "def sanitize_column_name(col_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize individual column name following specific rules\n",
    "    \n",
    "    Args:\n",
    "        col_name: Original column name\n",
    "        \n",
    "    Returns:\n",
    "        str: Sanitized column name\n",
    "    \"\"\"\n",
    "    if pd.isna(col_name) or col_name is None:\n",
    "        return \"unnamed_column\"\n",
    "    \n",
    "    # Convert to string and handle special cases\n",
    "    name = str(col_name).strip()\n",
    "    \n",
    "    # Handle target variable specifically\n",
    "    if \"Target_Variable\" in name or \"Total Income\" in name or \"TARGET VARIABLE\" in name:\n",
    "        return \"target_income\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    name = name.lower()\n",
    "    \n",
    "    # Replace special characters with underscores\n",
    "    # Handle: /, (, ), &, spaces, dots, hyphens, etc.\n",
    "    special_chars = r'[/\\(\\)&\\s\\.\\-\\+\\*\\%\\$\\#\\@\\!\\?\\[\\]\\{\\}\\|\\\\\\:\\;\\,\\<\\>\\=\\^\\~\\`\\'\\\"]'\n",
    "    name = re.sub(special_chars, '_', name)\n",
    "    \n",
    "    # Remove multiple consecutive underscores\n",
    "    name = re.sub(r'_+', '_', name)\n",
    "    \n",
    "    # Remove leading/trailing underscores\n",
    "    name = name.strip('_')\n",
    "    \n",
    "    # Handle empty names\n",
    "    if not name:\n",
    "        name = \"unnamed_column\"\n",
    "    \n",
    "    # Ensure doesn't start with number\n",
    "    if name and name[0].isdigit():\n",
    "        name = f\"col_{name}\"\n",
    "    \n",
    "    # Truncate very long names\n",
    "    # if len(name) > 50:\n",
    "    #     name = name[:47] + \"...\"\n",
    "    \n",
    "    return name\n",
    "\n",
    "def create_unique_column_names(columns: List[str]) -> Tuple[List[str], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Create unique column names and mapping dictionary\n",
    "    \n",
    "    Args:\n",
    "        columns: List of original column names\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (unique_names_list, mapping_dict)\n",
    "    \"\"\"\n",
    "    sanitized_names = [sanitize_column_name(col) for col in columns]\n",
    "    unique_names = []\n",
    "    name_counts = {}\n",
    "    mapping = {}\n",
    "    \n",
    "    for i, (original, sanitized) in enumerate(zip(columns, sanitized_names)):\n",
    "        # Handle duplicates by adding suffix\n",
    "        if sanitized in name_counts:\n",
    "            name_counts[sanitized] += 1\n",
    "            unique_name = f\"{sanitized}_{name_counts[sanitized]}\"\n",
    "        else:\n",
    "            name_counts[sanitized] = 0\n",
    "            unique_name = sanitized\n",
    "        \n",
    "        unique_names.append(unique_name)\n",
    "        mapping[str(original)] = unique_name  # Ensure key is string for JSON\n",
    "    \n",
    "    return unique_names, mapping\n",
    "\n",
    "def apply_column_sanitization(df: pd.DataFrame, sheet_type: str) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Apply column name sanitization to DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to sanitize\n",
    "        sheet_type: Type identifier for the sheet\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (sanitized_DataFrame, sanitization_metadata)\n",
    "    \"\"\"\n",
    "    print(f\"\\n Sanitizing column names for {sheet_type} data...\")\n",
    "    \n",
    "    original_columns = df.columns.tolist()\n",
    "    unique_names, column_mapping = create_unique_column_names(original_columns)\n",
    "    \n",
    "    # Create new DataFrame with sanitized names\n",
    "    df_sanitized = df.copy()\n",
    "    df_sanitized.columns = unique_names\n",
    "    \n",
    "    # Create sanitization metadata\n",
    "    sanitization_metadata = {\n",
    "        'sheet_type': sheet_type,\n",
    "        'original_column_count': len(original_columns),\n",
    "        'sanitized_column_count': len(unique_names),\n",
    "        'column_mapping': column_mapping,\n",
    "        'duplicates_found': len(original_columns) - len(set(original_columns)),\n",
    "        'target_variable_found': any('target_income' in name for name in unique_names),\n",
    "        'longest_original_name': max(len(str(col)) for col in original_columns),\n",
    "        'longest_sanitized_name': max(len(name) for name in unique_names)\n",
    "    }\n",
    "    \n",
    "    print(f\"   Original columns: {len(original_columns)}\")\n",
    "    print(f\"   Sanitized columns: {len(unique_names)}\")\n",
    "    print(f\"   Duplicates handled: {sanitization_metadata['duplicates_found']}\")\n",
    "    print(f\"   Target variable found: {sanitization_metadata['target_variable_found']}\")\n",
    "    \n",
    "    # Show sample mappings for verification\n",
    "    print(f\"   Sample mappings:\")\n",
    "    sample_mappings = list(column_mapping.items())[:5]\n",
    "    # for orig, san in sample_mappings:\n",
    "    #     if len(str(orig)) > 30:\n",
    "    #         orig_display = str(orig)[:27] + \"...\"\n",
    "    #     else:\n",
    "    #         orig_display = str(orig)\n",
    "    #     print(f\"    '{orig_display}' → '{san}'\")\n",
    "    \n",
    "    # return df_sanitized, sanitization_metadata\n",
    "\n",
    "    for orig, san in sample_mappings:\n",
    "        orig_display = str(orig)\n",
    "        print(f\"    '{orig_display}' → '{san}'\")\n",
    "    \n",
    "    return df_sanitized, sanitization_metadata\n",
    "\n",
    "def validate_column_consistency(train_cols: List[str], test_cols: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate column consistency between train and test datasets\n",
    "    \n",
    "    Args:\n",
    "        train_cols: List of training column names\n",
    "        test_cols: List of test column names\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Validation results\n",
    "    \"\"\"\n",
    "    train_set = set(train_cols)\n",
    "    test_set = set(test_cols)\n",
    "    \n",
    "    validation_results = {\n",
    "        'train_columns': len(train_cols),\n",
    "        'test_columns': len(test_cols),\n",
    "        'common_columns': len(train_set.intersection(test_set)),\n",
    "        'train_only_columns': list(train_set - test_set),\n",
    "        'test_only_columns': list(test_set - train_set),\n",
    "        'columns_match': train_set == test_set,\n",
    "        'target_in_train': 'target_income' in train_cols,\n",
    "        'target_in_test': 'target_income' in test_cols\n",
    "    }\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Apply sanitization to all datasets\n",
    "print(\" Starting feature name sanitization process...\")\n",
    "\n",
    "# Sanitize training data\n",
    "train_df_clean, train_sanitization = apply_column_sanitization(train_df, 'training')\n",
    "\n",
    "# Sanitize test data\n",
    "test_df_clean, test_sanitization = apply_column_sanitization(test_df, 'test')\n",
    "\n",
    "\n",
    "# Validate column consistency\n",
    "print(f\"\\n COLUMN CONSISTENCY VALIDATION:\")\n",
    "consistency_check = validate_column_consistency(\n",
    "    train_df_clean.columns.tolist(),\n",
    "    test_df_clean.columns.tolist()\n",
    ")\n",
    "\n",
    "print(f\"   Train columns: {consistency_check['train_columns']}\")\n",
    "print(f\"   Test columns: {consistency_check['test_columns']}\")\n",
    "print(f\"   Common columns: {consistency_check['common_columns']}\")\n",
    "print(f\"   Target in train: {consistency_check['target_in_train']}\")\n",
    "print(f\"   Target in test: {consistency_check['target_in_test']}\")\n",
    "\n",
    "if not consistency_check['columns_match']:\n",
    "    print(f\"   Column mismatch detected:\")\n",
    "    if consistency_check['train_only_columns']:\n",
    "        print(f\"     Train-only: {consistency_check['train_only_columns'][:5]}\")\n",
    "    if consistency_check['test_only_columns']:\n",
    "        print(f\"     Test-only: {consistency_check['test_only_columns'][:5]}\")\n",
    "else:\n",
    "    print(f\"   Perfect column alignment!\")\n",
    "\n",
    "# Create comprehensive feature mapping for JSON serialization\n",
    "comprehensive_mapping = {\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'sanitization_summary': {\n",
    "        'train': train_sanitization,\n",
    "        'test': test_sanitization, \n",
    "    },\n",
    "    'consistency_validation': consistency_check,\n",
    "    'master_column_mapping': {\n",
    "        'train': train_sanitization['column_mapping'],\n",
    "        'test': test_sanitization['column_mapping'],\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\n Feature name sanitization completed successfully!\")\n",
    "print(f\" Comprehensive mapping created with {len(comprehensive_mapping)} sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "237979ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting comprehensive data validation and quality assessment...\n",
      "\n",
      " Analyzing data types for training...\n",
      "   Column analysis complete:\n",
      "     Numeric columns: 0\n",
      "     Object columns: 105\n",
      "     Mixed type columns: 0\n",
      "     Potential categorical: 30\n",
      "\n",
      " Analyzing data types for test...\n",
      "   Column analysis complete:\n",
      "     Numeric columns: 0\n",
      "     Object columns: 104\n",
      "     Mixed type columns: 0\n",
      "     Potential categorical: 30\n",
      "\n",
      " Checking train-test data consistency...\n",
      "   Consistency check results:\n",
      "     Common columns: 104\n",
      "     Type mismatches: 0\n",
      "     Range issues: 0\n",
      "\n",
      " Creating data quality report for training...\n",
      "   Quality assessment complete:\n",
      "     Overall completeness: 98.5%\n",
      "     Potential issues: 0\n",
      "     Memory usage: 323.5 MB\n",
      "\n",
      " Creating data quality report for test...\n",
      "   Quality assessment complete:\n",
      "     Overall completeness: 98.5%\n",
      "     Potential issues: 0\n",
      "     Memory usage: 60.2 MB\n",
      "\n",
      " VALIDATION SUMMARY:\n",
      "   Analysis completed for 2 datasets\n",
      "   Consistency score: 100.00%\n",
      "   Issues identified: 0\n",
      "   Ready for processing: True\n",
      "   Proceed with preprocessing - address identified issues during feature engineering\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA VALIDATION WITH TYPE CHECKING AND QUALITY ASSESSMENT\n",
    "# =============================================================================\n",
    "# Purpose: Comprehensive data validation, type consistency, and quality assessment\n",
    "# Handles: Data type issues, missing values, outliers, data quality metrics\n",
    "# Output: Validation reports and data quality assessments with JSON compatibility\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_column_data_types(df: pd.DataFrame, dataset_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze data types and identify potential issues\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "        dataset_name: Name identifier for the dataset\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Comprehensive data type analysis\n",
    "    \"\"\"\n",
    "    print(f\"\\n Analyzing data types for {dataset_name}...\")\n",
    "    \n",
    "    dtype_analysis = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'total_columns': len(df.columns),\n",
    "        'data_types': {},\n",
    "        'type_distribution': {},\n",
    "        'problematic_columns': [],\n",
    "        'mixed_type_columns': [],\n",
    "        'high_cardinality_columns': [],\n",
    "        'potential_categorical_columns': []\n",
    "    }\n",
    "    \n",
    "    # Analyze each column\n",
    "    for col in df.columns:\n",
    "        col_info = {\n",
    "            'dtype': str(df[col].dtype),\n",
    "            'non_null_count': int(df[col].count()),\n",
    "            'null_count': int(df[col].isnull().sum()),\n",
    "            'null_percentage': float(df[col].isnull().sum() / len(df) * 100),\n",
    "            'unique_values': int(df[col].nunique()),\n",
    "            'cardinality_ratio': float(df[col].nunique() / len(df))\n",
    "        }\n",
    "        \n",
    "        # Check for mixed types in object columns\n",
    "        if df[col].dtype == 'object' and col_info['non_null_count'] > 0:\n",
    "            sample_values = df[col].dropna().head(100)\n",
    "            value_types = set(type(val).__name__ for val in sample_values)\n",
    "            col_info['value_types'] = list(value_types)\n",
    "            \n",
    "            if len(value_types) > 1:\n",
    "                dtype_analysis['mixed_type_columns'].append(col)\n",
    "        \n",
    "        # Identify high cardinality columns\n",
    "        if col_info['cardinality_ratio'] > 0.95 and col_info['unique_values'] > 100:\n",
    "            dtype_analysis['high_cardinality_columns'].append(col)\n",
    "        \n",
    "        # Identify potential categorical columns\n",
    "        if (df[col].dtype == 'object' and \n",
    "            col_info['cardinality_ratio'] < 0.1 and \n",
    "            col_info['unique_values'] < 50):\n",
    "            dtype_analysis['potential_categorical_columns'].append(col)\n",
    "        \n",
    "        # Flag problematic columns\n",
    "        if col_info['null_percentage'] > 80:\n",
    "            dtype_analysis['problematic_columns'].append({\n",
    "                'column': col,\n",
    "                'issue': 'high_missing_rate',\n",
    "                'value': col_info['null_percentage']\n",
    "            })\n",
    "        \n",
    "        dtype_analysis['data_types'][col] = col_info\n",
    "    \n",
    "    # Create type distribution summary\n",
    "    type_counts = df.dtypes.value_counts()\n",
    "    dtype_analysis['type_distribution'] = {\n",
    "        str(dtype): int(count) for dtype, count in type_counts.items()\n",
    "    }\n",
    "    \n",
    "    print(f\"   Column analysis complete:\")\n",
    "    print(f\"     Numeric columns: {sum(1 for col in df.columns if pd.api.types.is_numeric_dtype(df[col]))}\")\n",
    "    print(f\"     Object columns: {sum(1 for col in df.columns if df[col].dtype == 'object')}\")\n",
    "    print(f\"     Mixed type columns: {len(dtype_analysis['mixed_type_columns'])}\")\n",
    "    print(f\"     Potential categorical: {len(dtype_analysis['potential_categorical_columns'])}\")\n",
    "    \n",
    "    return dtype_analysis\n",
    "\n",
    "def check_data_consistency(train_df: pd.DataFrame, test_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Check data consistency between train and test datasets\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training DataFrame\n",
    "        test_df: Test DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Consistency check results\n",
    "    \"\"\"\n",
    "    print(f\"\\n Checking train-test data consistency...\")\n",
    "    \n",
    "    consistency_report = {\n",
    "        'timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'shape_comparison': {\n",
    "            'train_shape': list(train_df.shape),\n",
    "            'test_shape': list(test_df.shape),\n",
    "            'same_columns': train_df.shape[1] == test_df.shape[1]\n",
    "        },\n",
    "        'column_consistency': {},\n",
    "        'dtype_consistency': {},\n",
    "        'value_range_consistency': {},\n",
    "        'categorical_consistency': {}\n",
    "    }\n",
    "    \n",
    "    # Compare common columns\n",
    "    common_cols = set(train_df.columns).intersection(set(test_df.columns))\n",
    "    train_only = set(train_df.columns) - set(test_df.columns)\n",
    "    test_only = set(test_df.columns) - set(train_df.columns)\n",
    "    \n",
    "    consistency_report['column_consistency'] = {\n",
    "        'common_columns': len(common_cols),\n",
    "        'train_only': list(train_only),\n",
    "        'test_only': list(test_only),\n",
    "        'perfect_match': len(train_only) == 0 and len(test_only) == 0\n",
    "    }\n",
    "    \n",
    "    # Check data type consistency for common columns\n",
    "    dtype_mismatches = []\n",
    "    for col in common_cols:\n",
    "        train_dtype = str(train_df[col].dtype)\n",
    "        test_dtype = str(test_df[col].dtype)\n",
    "        \n",
    "        if train_dtype != test_dtype:\n",
    "            dtype_mismatches.append({\n",
    "                'column': col,\n",
    "                'train_dtype': train_dtype,\n",
    "                'test_dtype': test_dtype\n",
    "            })\n",
    "    \n",
    "    consistency_report['dtype_consistency'] = {\n",
    "        'matching_dtypes': len(common_cols) - len(dtype_mismatches),\n",
    "        'mismatched_dtypes': len(dtype_mismatches),\n",
    "        'mismatches': dtype_mismatches\n",
    "    }\n",
    "    \n",
    "    # Check value ranges for numeric columns\n",
    "    range_issues = []\n",
    "    for col in common_cols:\n",
    "        if pd.api.types.is_numeric_dtype(train_df[col]) and pd.api.types.is_numeric_dtype(test_df[col]):\n",
    "            train_range = (float(train_df[col].min()), float(train_df[col].max()))\n",
    "            test_range = (float(test_df[col].min()), float(test_df[col].max()))\n",
    "            \n",
    "            # Check if test range is within train range (important for model performance)\n",
    "            if test_range[0] < train_range[0] or test_range[1] > train_range[1]:\n",
    "                range_issues.append({\n",
    "                    'column': col,\n",
    "                    'train_range': train_range,\n",
    "                    'test_range': test_range,\n",
    "                    'test_outside_train_range': True\n",
    "                })\n",
    "    \n",
    "    consistency_report['value_range_consistency'] = {\n",
    "        'columns_checked': len([col for col in common_cols if pd.api.types.is_numeric_dtype(train_df[col])]),\n",
    "        'range_issues': len(range_issues),\n",
    "        'issues_detail': range_issues\n",
    "    }\n",
    "    \n",
    "    print(f\"   Consistency check results:\")\n",
    "    print(f\"     Common columns: {len(common_cols)}\")\n",
    "    print(f\"     Type mismatches: {len(dtype_mismatches)}\")\n",
    "    print(f\"     Range issues: {len(range_issues)}\")\n",
    "    \n",
    "    return consistency_report\n",
    "\n",
    "def create_data_quality_report(df: pd.DataFrame, dataset_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create comprehensive data quality report\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to assess\n",
    "        dataset_name: Name of the dataset\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Data quality metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n Creating data quality report for {dataset_name}...\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    missing_cells = df.isnull().sum().sum()\n",
    "    \n",
    "    quality_report = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'basic_stats': {\n",
    "            'rows': int(df.shape[0]),\n",
    "            'columns': int(df.shape[1]),\n",
    "            'total_cells': int(total_cells),\n",
    "            'missing_cells': int(missing_cells),\n",
    "            'missing_percentage': float(missing_cells / total_cells * 100),\n",
    "            'memory_usage_mb': float(df.memory_usage(deep=True).sum() / 1024 / 1024)\n",
    "        },\n",
    "        'column_quality': {},\n",
    "        'data_completeness': {},\n",
    "        'potential_issues': []\n",
    "    }\n",
    "    \n",
    "    # Per-column quality metrics\n",
    "    for col in df.columns:\n",
    "        col_quality = {\n",
    "            'dtype': str(df[col].dtype),\n",
    "            'missing_count': int(df[col].isnull().sum()),\n",
    "            'missing_percentage': float(df[col].isnull().sum() / len(df) * 100),\n",
    "            'unique_values': int(df[col].nunique()),\n",
    "            'completeness_score': float((len(df) - df[col].isnull().sum()) / len(df))\n",
    "        }\n",
    "        \n",
    "        # Add numeric-specific metrics\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            col_quality.update({\n",
    "                'mean': float(df[col].mean()) if df[col].count() > 0 else None,\n",
    "                'std': float(df[col].std()) if df[col].count() > 0 else None,\n",
    "                'min': float(df[col].min()) if df[col].count() > 0 else None,\n",
    "                'max': float(df[col].max()) if df[col].count() > 0 else None,\n",
    "                'zeros_count': int((df[col] == 0).sum()),\n",
    "                'negative_count': int((df[col] < 0).sum()) if df[col].count() > 0 else 0\n",
    "            })\n",
    "        \n",
    "        quality_report['column_quality'][col] = col_quality\n",
    "        \n",
    "        # Flag potential issues\n",
    "        if col_quality['missing_percentage'] > 50:\n",
    "            quality_report['potential_issues'].append(f\"High missing rate in '{col}': {col_quality['missing_percentage']:.1f}%\")\n",
    "        \n",
    "        if col_quality['unique_values'] == 1:\n",
    "            quality_report['potential_issues'].append(f\"Constant column detected: '{col}'\")\n",
    "    \n",
    "    # Data completeness summary\n",
    "    missing_by_column = df.isnull().sum().sort_values(ascending=False)\n",
    "    quality_report['data_completeness'] = {\n",
    "        'columns_with_missing': int((missing_by_column > 0).sum()),\n",
    "        'complete_columns': int((missing_by_column == 0).sum()),\n",
    "        'worst_columns': {\n",
    "            col: int(count) for col, count in missing_by_column.head(5).items()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"   Quality assessment complete:\")\n",
    "    print(f\"     Overall completeness: {100 - quality_report['basic_stats']['missing_percentage']:.1f}%\")\n",
    "    print(f\"     Potential issues: {len(quality_report['potential_issues'])}\")\n",
    "    print(f\"     Memory usage: {quality_report['basic_stats']['memory_usage_mb']:.1f} MB\")\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "# Perform comprehensive data validation\n",
    "print(\" Starting comprehensive data validation and quality assessment...\")\n",
    "\n",
    "# Analyze data types for each dataset\n",
    "train_dtype_analysis = analyze_column_data_types(train_df_clean, 'training')\n",
    "test_dtype_analysis = analyze_column_data_types(test_df_clean, 'test')\n",
    "\n",
    "# Check consistency between train and test\n",
    "consistency_report = check_data_consistency(train_df_clean, test_df_clean)\n",
    "\n",
    "# Create quality reports\n",
    "train_quality_report = create_data_quality_report(train_df_clean, 'training')\n",
    "test_quality_report = create_data_quality_report(test_df_clean, 'test')\n",
    "\n",
    "# Create comprehensive validation summary\n",
    "validation_summary = {\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'validation_status': 'completed',\n",
    "    'datasets_analyzed': ['training', 'test'],\n",
    "    'dtype_analysis': {\n",
    "        'train': train_dtype_analysis,\n",
    "        'test': test_dtype_analysis\n",
    "    },\n",
    "    'consistency_check': consistency_report,\n",
    "    'quality_reports': {\n",
    "        'train': train_quality_report,\n",
    "        'test': test_quality_report\n",
    "    },\n",
    "    'overall_assessment': {\n",
    "        'data_ready_for_processing': True,\n",
    "        'major_issues_found': len(train_quality_report['potential_issues']) + len(test_quality_report['potential_issues']),\n",
    "        'consistency_score': consistency_report['dtype_consistency']['matching_dtypes'] / max(1, len(set(train_df_clean.columns).intersection(set(test_df_clean.columns)))),\n",
    "        'recommendation': 'Proceed with preprocessing - address identified issues during feature engineering'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\n VALIDATION SUMMARY:\")\n",
    "print(f\"   Analysis completed for {len(validation_summary['datasets_analyzed'])} datasets\")\n",
    "print(f\"   Consistency score: {validation_summary['overall_assessment']['consistency_score']:.2%}\")\n",
    "print(f\"   Issues identified: {validation_summary['overall_assessment']['major_issues_found']}\")\n",
    "print(f\"   Ready for processing: {validation_summary['overall_assessment']['data_ready_for_processing']}\")\n",
    "print(f\"   {validation_summary['overall_assessment']['recommendation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c1278e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting output generation with proper serialization...\n",
      "\n",
      " Saving processed CSV files...\n",
      " Saving training data to train_raw.csv...\n",
      "   Saved successfully: 68.77 MB\n",
      " Saving test data to test_raw.csv...\n",
      "   Saved successfully: 12.86 MB\n",
      "\n",
      " Saving feature mapping...\n",
      " Saving feature_mapping to feature_mapping.pkl...\n",
      "   Pickle saved successfully: 18.5 KB\n",
      "\n",
      " Saving validation summary...\n",
      " Saving validation_summary to validation_summary.json...\n",
      "   JSON saved successfully: 124.1 KB\n",
      "\n",
      " Creating comprehensive loading report...\n",
      " Saving final_loading_report to loading_report.json...\n",
      "   JSON saved successfully: 16.4 KB\n",
      "\n",
      " DATA LOADING AND CONVERSION COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n",
      " PROCESSING SUMMARY:\n",
      "   Training data: 53,306 rows × 105 columns\n",
      "   Test data: 10,000 rows × 104 columns\n",
      "   Target variable:  Found\n",
      "\n",
      " FILES CREATED:\n",
      "  1. train_raw.csv\n",
      "  2. test_raw.csv\n",
      "  3. feature_mapping.pkl\n",
      "  4. validation_summary.json\n",
      "  5. loading_report.json\n",
      "\n",
      " STORAGE SUMMARY:\n",
      "   Total files: 5\n",
      "   Total size: 81.76 MB\n",
      "   Output directory: ..\\data\\raw\n",
      "\n",
      " READY FOR NEXT STEPS:\n",
      "   Data loaded with proper type handling\n",
      "   Feature names sanitized and mapped\n",
      "   Data validation completed\n",
      "   All outputs properly serialized\n",
      "   Consistency score: 100.0%\n",
      "   Next: Run 02_exploratory_data_analysis.ipynb\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# OUTPUT GENERATION WITH PROPER SERIALIZATION\n",
    "# =============================================================================\n",
    "# Purpose: Save all processed data with consistent formatting and serialization\n",
    "# Handles: CSV output, pickle files, JSON reports, type conversion for serialization\n",
    "# Output: Clean datasets ready for preprocessing, comprehensive documentation\n",
    "# =============================================================================\n",
    "\n",
    "def save_dataframe_with_types(df: pd.DataFrame, filepath: Path, dataset_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Save DataFrame with proper type handling and create metadata\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        filepath: Output file path\n",
    "        dataset_name: Name identifier for the dataset\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Save operation metadata\n",
    "    \"\"\"\n",
    "    print(f\" Saving {dataset_name} data to {filepath.name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Ensure directory exists\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save with proper encoding and index handling\n",
    "        df.to_csv(filepath, index=False, encoding='utf-8', float_format='%.6f')\n",
    "        \n",
    "        # Create metadata\n",
    "        file_metadata = {\n",
    "            'filename': filepath.name,\n",
    "            'filepath': str(filepath),\n",
    "            'dataset_name': dataset_name,\n",
    "            'rows': int(df.shape[0]),\n",
    "            'columns': int(df.shape[1]),\n",
    "            'file_size_bytes': int(filepath.stat().st_size),\n",
    "            'file_size_mb': float(filepath.stat().st_size / 1024 / 1024),\n",
    "            'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()},\n",
    "            'save_timestamp': pd.Timestamp.now().isoformat(),\n",
    "            'encoding': 'utf-8',\n",
    "            'index_saved': False\n",
    "        }\n",
    "        \n",
    "        print(f\"   Saved successfully: {file_metadata['file_size_mb']:.2f} MB\")\n",
    "        return file_metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Error saving {dataset_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "def save_json_with_serialization(data: Dict[str, Any], filepath: Path, data_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Save dictionary as JSON with proper type serialization\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary to save\n",
    "        filepath: Output file path\n",
    "        data_name: Name identifier for the data\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Save operation metadata\n",
    "    \"\"\"\n",
    "    print(f\" Saving {data_name} to {filepath.name}...\")\n",
    "    \n",
    "    def make_json_serializable(obj):\n",
    "        \"\"\"Recursively convert objects to JSON serializable types\"\"\"\n",
    "        if isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, pd.Series):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, pd.Timestamp):\n",
    "            return obj.isoformat()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: make_json_serializable(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [make_json_serializable(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    try:\n",
    "        # Ensure directory exists\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Convert data to JSON serializable format\n",
    "        serializable_data = make_json_serializable(data)\n",
    "        \n",
    "        # Save with proper formatting\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(serializable_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Create metadata\n",
    "        save_metadata = {\n",
    "            'filename': filepath.name,\n",
    "            'filepath': str(filepath),\n",
    "            'data_name': data_name,\n",
    "            'file_size_bytes': int(filepath.stat().st_size),\n",
    "            'file_size_kb': float(filepath.stat().st_size / 1024),\n",
    "            'save_timestamp': pd.Timestamp.now().isoformat(),\n",
    "            'encoding': 'utf-8',\n",
    "            'json_formatted': True\n",
    "        }\n",
    "        \n",
    "        print(f\"   JSON saved successfully: {save_metadata['file_size_kb']:.1f} KB\")\n",
    "        return save_metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Error saving JSON {data_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "def save_pickle_with_protocol(data: Any, filepath: Path, data_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Save data as pickle with proper protocol handling\n",
    "    \n",
    "    Args:\n",
    "        data: Data to pickle\n",
    "        filepath: Output file path\n",
    "        data_name: Name identifier for the data\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Save operation metadata\n",
    "    \"\"\"\n",
    "    print(f\" Saving {data_name} to {filepath.name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Ensure directory exists\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save with protocol 4 for Python 3.4+ compatibility\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(data, f, protocol=4)\n",
    "        \n",
    "        # Create metadata\n",
    "        save_metadata = {\n",
    "            'filename': filepath.name,\n",
    "            'filepath': str(filepath),\n",
    "            'data_name': data_name,\n",
    "            'file_size_bytes': int(filepath.stat().st_size),\n",
    "            'file_size_kb': float(filepath.stat().st_size / 1024),\n",
    "            'save_timestamp': pd.Timestamp.now().isoformat(),\n",
    "            'pickle_protocol': 4,\n",
    "            'data_type': str(type(data).__name__)\n",
    "        }\n",
    "        \n",
    "        print(f\"   Pickle saved successfully: {save_metadata['file_size_kb']:.1f} KB\")\n",
    "        return save_metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Error saving pickle {data_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Start output generation process\n",
    "print(\" Starting output generation with proper serialization...\")\n",
    "\n",
    "# Initialize save metadata tracking\n",
    "save_operations = {\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'operations': [],\n",
    "    'files_created': [],\n",
    "    'total_size_mb': 0\n",
    "}\n",
    "\n",
    "# 1. Save cleaned CSV files\n",
    "print(\"\\n Saving processed CSV files...\")\n",
    "\n",
    "# Save training data\n",
    "train_save_meta = save_dataframe_with_types(\n",
    "    train_df_clean, \n",
    "    RAW_DATA_DIR / \"train_raw.csv\", \n",
    "    \"training\"\n",
    ")\n",
    "save_operations['operations'].append(train_save_meta)\n",
    "save_operations['files_created'].append(train_save_meta['filepath'])\n",
    "\n",
    "# Save test data\n",
    "test_save_meta = save_dataframe_with_types(\n",
    "    test_df_clean, \n",
    "    RAW_DATA_DIR / \"test_raw.csv\", \n",
    "    \"test\"\n",
    ")\n",
    "save_operations['operations'].append(test_save_meta)\n",
    "save_operations['files_created'].append(test_save_meta['filepath'])\n",
    "\n",
    "# 2. Save feature mapping as pickle\n",
    "print(\"\\n Saving feature mapping...\")\n",
    "mapping_save_meta = save_pickle_with_protocol(\n",
    "    comprehensive_mapping,\n",
    "    RAW_DATA_DIR / \"feature_mapping.pkl\",\n",
    "    \"feature_mapping\"\n",
    ")\n",
    "save_operations['operations'].append(mapping_save_meta)\n",
    "save_operations['files_created'].append(mapping_save_meta['filepath'])\n",
    "\n",
    "# 3. Save validation summary as JSON\n",
    "print(\"\\n Saving validation summary...\")\n",
    "validation_save_meta = save_json_with_serialization(\n",
    "    validation_summary,\n",
    "    RAW_DATA_DIR / \"validation_summary.json\",\n",
    "    \"validation_summary\"\n",
    ")\n",
    "save_operations['operations'].append(validation_save_meta)\n",
    "save_operations['files_created'].append(validation_save_meta['filepath'])\n",
    "\n",
    "# 4. Create and save comprehensive loading report\n",
    "print(\"\\n Creating comprehensive loading report...\")\n",
    "final_loading_report = {\n",
    "    'project_info': {\n",
    "        'objective': 'L&T Finance Pearl Challenge - Farmer Income Prediction',\n",
    "        'target_metric': 'MAPE < 18%',\n",
    "        'data_source': str(excel_filepath),\n",
    "        'processing_timestamp': pd.Timestamp.now().isoformat()\n",
    "    },\n",
    "    'data_summary': {\n",
    "        'training_data': {\n",
    "            'rows': int(train_df_clean.shape[0]),\n",
    "            'columns': int(train_df_clean.shape[1]),\n",
    "            'target_variable': 'target_income' if 'target_income' in train_df_clean.columns else 'not_found',\n",
    "            'memory_mb': float(train_df_clean.memory_usage(deep=True).sum() / 1024 / 1024)\n",
    "        },\n",
    "        'test_data': {\n",
    "            'rows': int(test_df_clean.shape[0]),\n",
    "            'columns': int(test_df_clean.shape[1]),\n",
    "            'memory_mb': float(test_df_clean.memory_usage(deep=True).sum() / 1024 / 1024)\n",
    "        },\n",
    "    },\n",
    "    'processing_summary': {\n",
    "        'feature_sanitization': {\n",
    "            'train_columns_processed': len(train_sanitization['column_mapping']),\n",
    "            'test_columns_processed': len(test_sanitization['column_mapping']),\n",
    "            'target_variable_found': validation_summary['overall_assessment']['data_ready_for_processing']\n",
    "        },\n",
    "        'data_validation': {\n",
    "            'consistency_score': float(validation_summary['overall_assessment']['consistency_score']),\n",
    "            'issues_found': int(validation_summary['overall_assessment']['major_issues_found']),\n",
    "            'ready_for_processing': validation_summary['overall_assessment']['data_ready_for_processing']\n",
    "        }\n",
    "    },\n",
    "    'file_outputs': save_operations,\n",
    "    'next_steps': [\n",
    "        'Run 02_exploratory_data_analysis.ipynb for comprehensive EDA',\n",
    "        'Proceed with 03_preprocessing_feature_eng.ipynb for data preprocessing',\n",
    "        'Address identified data quality issues during preprocessing',\n",
    "        'Validate target variable distribution and outliers'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Calculate total file size\n",
    "total_size_mb = sum(op.get('file_size_mb', op.get('file_size_kb', 0) / 1024) for op in save_operations['operations'])\n",
    "save_operations['total_size_mb'] = float(total_size_mb)\n",
    "final_loading_report['file_outputs']['total_size_mb'] = float(total_size_mb)\n",
    "\n",
    "# Save final loading report\n",
    "report_save_meta = save_json_with_serialization(\n",
    "    final_loading_report,\n",
    "    RAW_DATA_DIR / \"loading_report.json\",\n",
    "    \"final_loading_report\"\n",
    ")\n",
    "save_operations['operations'].append(report_save_meta)\n",
    "save_operations['files_created'].append(report_save_meta['filepath'])\n",
    "\n",
    "# 5. Display final summary\n",
    "print(f\"\\n DATA LOADING AND CONVERSION COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"=\" * 70)\n",
    "print(f\" PROCESSING SUMMARY:\")\n",
    "print(f\"   Training data: {train_df_clean.shape[0]:,} rows × {train_df_clean.shape[1]} columns\")\n",
    "print(f\"   Test data: {test_df_clean.shape[0]:,} rows × {test_df_clean.shape[1]} columns\")\n",
    "print(f\"   Target variable: {' Found' if 'target_income' in train_df_clean.columns else ' Not found'}\")\n",
    "\n",
    "print(f\"\\n FILES CREATED:\")\n",
    "for i, filepath in enumerate(save_operations['files_created'], 1):\n",
    "    print(f\"  {i}. {Path(filepath).name}\")\n",
    "\n",
    "print(f\"\\n STORAGE SUMMARY:\")\n",
    "print(f\"   Total files: {len(save_operations['files_created'])}\")\n",
    "print(f\"   Total size: {save_operations['total_size_mb']:.2f} MB\")\n",
    "print(f\"   Output directory: {RAW_DATA_DIR}\")\n",
    "\n",
    "print(f\"\\n READY FOR NEXT STEPS:\")\n",
    "print(f\"   Data loaded with proper type handling\")\n",
    "print(f\"   Feature names sanitized and mapped\") \n",
    "print(f\"   Data validation completed\")\n",
    "print(f\"   All outputs properly serialized\")\n",
    "print(f\"   Consistency score: {validation_summary['overall_assessment']['consistency_score']:.1%}\")\n",
    "print(f\"   Next: Run 02_exploratory_data_analysis.ipynb\")\n",
    "\n",
    "print(f\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
