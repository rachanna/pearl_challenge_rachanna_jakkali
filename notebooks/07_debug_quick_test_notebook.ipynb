{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c259715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß DEBUG & QUICK TEST NOTEBOOK\n",
      "============================================================\n",
      "‚è∞ Started at: 2025-09-05 20:53:30\n",
      "üìö Library Status:\n",
      "   LightGBM: ‚úÖ\n",
      "   XGBoost: ‚úÖ\n",
      "   CatBoost: ‚úÖ\n",
      "\n",
      "==================================================\n",
      "1Ô∏è‚É£ DATA AVAILABILITY VALIDATION\n",
      "==================================================\n",
      "üìÅ DIRECTORY STRUCTURE CHECK:\n",
      "   ‚úÖ ..\\data\n",
      "   ‚úÖ ..\\data\n",
      "   ‚úÖ ..\\data\\feature_engineered\n",
      "\n",
      "üìÑ REQUIRED FILES CHECK:\n",
      "   ‚úÖ X_train_eng.npy           (Training features, 16.2MB)\n",
      "   ‚úÖ X_val_eng.npy             (Validation features, 4.0MB)\n",
      "   ‚úÖ X_test_eng.npy            (Test features, 3.8MB)\n",
      "   ‚úÖ y_train_eng.npy           (Training targets, 0.2MB)\n",
      "   ‚úÖ y_val_eng.npy             (Validation targets, 0.0MB)\n",
      "   ‚úÖ feature_metadata.json     (Feature metadata, 0.0MB)\n",
      "   ‚úÖ preprocessing_objects.pkl (Preprocessing objects, 0.0MB)\n",
      "‚úÖ All required files found\n",
      "\n",
      "==================================================\n",
      "2Ô∏è‚É£ DATA LOADING AND VALIDATION\n",
      "==================================================\n",
      "üìä Loading data arrays...\n",
      "‚úÖ Data loading successful\n",
      "\n",
      "üìä DATA VALIDATION:\n",
      "   Training set: 42,436 samples √ó 100 features\n",
      "   Validation set: 10,610 samples √ó 100 features\n",
      "   Test set: 10,000 samples √ó 100 features\n",
      "   Feature names: 100 available\n",
      "\n",
      "üîç MISSING VALUE CHECK:\n",
      "   X_train missing: 0\n",
      "   X_val missing: 0\n",
      "   X_test missing: 0\n",
      "   y_train missing: 0\n",
      "   y_val missing: 0\n",
      "‚úÖ No missing values - data ready for modeling\n",
      "\n",
      "üè∑Ô∏è FEATURE CONSISTENCY:\n",
      "   X_train features: 100\n",
      "   X_val features: 100\n",
      "   X_test features: 100\n",
      "   Expected features: 100\n",
      "‚úÖ Feature consistency verified\n",
      "\n",
      "üéØ TARGET VARIABLE ANALYSIS:\n",
      "   y_train range: ‚Çπ50,000 - ‚Çπ9,960,000\n",
      "   y_train mean: ‚Çπ1,127,400\n",
      "   y_val range: ‚Çπ50,000 - ‚Çπ9,700,005\n",
      "   y_val mean: ‚Çπ1,124,200\n",
      "\n",
      "üíæ MEMORY USAGE: 24.3 MB total\n",
      "\n",
      "==================================================\n",
      "3Ô∏è‚É£ LIBRARY COMPATIBILITY TESTING\n",
      "==================================================\n",
      "üîß BASIC MODEL COMPATIBILITY:\n",
      "\n",
      "üß™ Testing Ridge Regression...\n",
      "   ‚úÖ Ridge Regression: Training OK (0.05s)\n",
      "      Predictions range: 603516 - 1185978\n",
      "\n",
      "üß™ Testing Lasso Regression...\n",
      "   ‚úÖ Lasso Regression: Training OK (0.01s)\n",
      "      Predictions range: 473157 - 1158457\n",
      "\n",
      "üß™ Testing Random Forest...\n",
      "   ‚úÖ Random Forest: Training OK (0.09s)\n",
      "      Predictions range: 678960 - 1191784\n",
      "\n",
      "üöÄ GRADIENT BOOSTING COMPATIBILITY:\n",
      "\n",
      "üß™ Testing LightGBM...\n",
      "   ‚úÖ LightGBM: Training OK (0.03s)\n",
      "      Predictions range: 613870 - 1248136\n",
      "\n",
      "üß™ Testing XGBoost...\n",
      "   ‚úÖ XGBoost: Training OK (0.08s)\n",
      "      Predictions range: 549991 - 1100033\n",
      "\n",
      "üß™ Testing CatBoost...\n",
      "   ‚úÖ CatBoost: Training OK (0.17s)\n",
      "      Predictions range: 556700 - 1113573\n",
      "\n",
      "üõë EARLY STOPPING COMPATIBILITY:\n",
      "   ‚ùå Early stopping failed: XGBModel.fit() got an unexpected keyword argument \n",
      "   ‚úÖ Early stopping OK\n",
      "   ‚úÖ Early stopping OK\n",
      "\n",
      "==================================================\n",
      "4Ô∏è‚É£ QUICK MODEL TRIALS (5 EACH)\n",
      "==================================================\n",
      "üìà LINEAR MODEL TRIALS:\n",
      "\n",
      "üèÉ Running 5 trials for Ridge Regression:\n",
      "   Trial 1: MAPE  30.66% | MAE ‚Çπ 325,019 | R¬≤ 0.508 | Time  0.0s\n",
      "   Trial 2: MAPE  30.66% | MAE ‚Çπ 325,023 | R¬≤ 0.508 | Time  0.2s\n",
      "   Trial 3: MAPE  30.65% | MAE ‚Çπ 325,004 | R¬≤ 0.508 | Time  0.0s\n",
      "   Trial 4: MAPE  30.65% | MAE ‚Çπ 324,970 | R¬≤ 0.508 | Time  0.0s\n",
      "   Trial 5: MAPE  30.68% | MAE ‚Çπ 325,126 | R¬≤ 0.508 | Time  0.0s\n",
      "   üìä Summary: MAPE 30.66% ¬± 0.01% (min: 30.65%, max: 30.68%)\n",
      "   üèÜ Best: Trial 4 with 30.65% MAPE\n",
      "\n",
      "üèÉ Running 5 trials for Lasso Regression:\n",
      "   Trial 1: MAPE  30.66% | MAE ‚Çπ 325,025 | R¬≤ 0.508 | Time  2.3s\n",
      "   Trial 2: MAPE  30.66% | MAE ‚Çπ 325,026 | R¬≤ 0.508 | Time  2.3s\n",
      "   Trial 3: MAPE  30.66% | MAE ‚Çπ 325,026 | R¬≤ 0.508 | Time  2.3s\n",
      "   Trial 4: MAPE  30.66% | MAE ‚Çπ 325,019 | R¬≤ 0.508 | Time  2.2s\n",
      "   Trial 5: MAPE  30.66% | MAE ‚Çπ 325,062 | R¬≤ 0.508 | Time  0.7s\n",
      "   üìä Summary: MAPE 30.66% ¬± 0.00% (min: 30.66%, max: 30.66%)\n",
      "   üèÜ Best: Trial 4 with 30.66% MAPE\n",
      "\n",
      "üå≤ TREE-BASED MODEL TRIALS:\n",
      "\n",
      "üèÉ Running 5 trials for Random Forest:\n",
      "   Trial 1: MAPE  24.35% | MAE ‚Çπ 248,366 | R¬≤ 0.648 | Time 16.5s\n",
      "   Trial 2: MAPE  24.30% | MAE ‚Çπ 247,587 | R¬≤ 0.648 | Time 31.3s\n",
      "   Trial 3: MAPE  24.15% | MAE ‚Çπ 246,516 | R¬≤ 0.650 | Time 62.0s\n",
      "   Trial 4: MAPE  27.75% | MAE ‚Çπ 283,216 | R¬≤ 0.617 | Time  6.8s\n",
      "   Trial 5: MAPE  24.67% | MAE ‚Çπ 250,765 | R¬≤ 0.654 | Time 13.1s\n",
      "   üìä Summary: MAPE 25.04% ¬± 1.36% (min: 24.15%, max: 27.75%)\n",
      "   üèÜ Best: Trial 3 with 24.15% MAPE\n",
      "\n",
      "üèÉ Running 5 trials for LightGBM:\n",
      "   Trial 1: MAPE  25.75% | MAE ‚Çπ 266,907 | R¬≤ 0.664 | Time  0.6s\n",
      "   Trial 2: MAPE  33.51% | MAE ‚Çπ 337,224 | R¬≤ 0.509 | Time  0.6s\n",
      "   Trial 3: MAPE  25.75% | MAE ‚Çπ 266,907 | R¬≤ 0.664 | Time  0.6s\n",
      "   Trial 4: MAPE  26.46% | MAE ‚Çπ 273,885 | R¬≤ 0.656 | Time  0.5s\n",
      "   Trial 5: MAPE  25.27% | MAE ‚Çπ 260,140 | R¬≤ 0.670 | Time  0.8s\n",
      "   üìä Summary: MAPE 27.35% ¬± 3.10% (min: 25.27%, max: 33.51%)\n",
      "   üèÜ Best: Trial 5 with 25.27% MAPE\n",
      "\n",
      "üèÉ Running 5 trials for XGBoost:\n",
      "   Trial 1: MAPE  25.29% | MAE ‚Çπ 264,323 | R¬≤ 0.646 | Time  0.9s\n",
      "   Trial 2: MAPE  33.76% | MAE ‚Çπ 340,545 | R¬≤ 0.480 | Time  0.9s\n",
      "   Trial 3: MAPE  26.15% | MAE ‚Çπ 271,152 | R¬≤ 0.647 | Time  0.9s\n",
      "   Trial 4: MAPE  27.49% | MAE ‚Çπ 287,743 | R¬≤ 0.610 | Time  0.3s\n",
      "   Trial 5: MAPE  25.10% | MAE ‚Çπ 261,878 | R¬≤ 0.634 | Time  1.8s\n",
      "   üìä Summary: MAPE 27.56% ¬± 3.21% (min: 25.10%, max: 33.76%)\n",
      "   üèÜ Best: Trial 5 with 25.10% MAPE\n",
      "\n",
      "üèÉ Running 5 trials for CatBoost:\n",
      "   Trial 1: MAPE  26.17% | MAE ‚Çπ 275,262 | R¬≤ 0.640 | Time  0.9s\n",
      "   Trial 2: MAPE  35.59% | MAE ‚Çπ 358,056 | R¬≤ 0.426 | Time  0.9s\n",
      "   Trial 3: MAPE  28.13% | MAE ‚Çπ 291,430 | R¬≤ 0.614 | Time  0.9s\n",
      "   Trial 4: MAPE  27.42% | MAE ‚Çπ 284,138 | R¬≤ 0.626 | Time  0.5s\n",
      "   Trial 5: MAPE  25.94% | MAE ‚Çπ 270,754 | R¬≤ 0.644 | Time  2.4s\n",
      "   üìä Summary: MAPE 28.65% ¬± 3.56% (min: 25.94%, max: 35.59%)\n",
      "   üèÜ Best: Trial 5 with 25.94% MAPE\n",
      "\n",
      "==================================================\n",
      "5Ô∏è‚É£ RESULTS SUMMARY AND RECOMMENDATIONS\n",
      "==================================================\n",
      "üèÜ MODEL PERFORMANCE RANKING:\n",
      "================================================================================\n",
      "Rank  Model                Best MAPE    Mean MAPE    Std      Status\n",
      "================================================================================\n",
      "1     Random Forest        24.15      % 25.04      % 1.36   % üìà Good\n",
      "2     XGBoost              25.10      % 27.56      % 3.21   % ‚ö†Ô∏è High\n",
      "3     LightGBM             25.27      % 27.35      % 3.10   % ‚ö†Ô∏è High\n",
      "4     CatBoost             25.94      % 28.65      % 3.56   % ‚ö†Ô∏è High\n",
      "5     Ridge Regression     30.65      % 30.66      % 0.01   % ‚ö†Ô∏è High\n",
      "6     Lasso Regression     30.66      % 30.66      % 0.00   % ‚ö†Ô∏è High\n",
      "\n",
      "ü•á BEST PERFORMING MODEL:\n",
      "   Model: Random Forest\n",
      "   Best MAPE: 24.15%\n",
      "   Mean MAPE: 25.04% (¬±1.36%)\n",
      "   Target status: üìà NEEDS IMPROVEMENT\n",
      "\n",
      "üîß SYSTEM READINESS ASSESSMENT:\n",
      "   Data quality: ‚úÖ\n",
      "   Feature consistency: ‚úÖ\n",
      "   Working models: 6/6\n",
      "   Early stopping: 2/3 gradient boosting models\n",
      "   Memory usage: ‚úÖ (24.3 MB)\n",
      "\n",
      "üéØ FINAL RECOMMENDATION:\n",
      "‚úÖ SYSTEM READY FOR FULL MODELING EXPERIMENTS\n",
      "   All checks passed - proceed with confidence!\n",
      "\n",
      "üöÄ Recommended next steps:\n",
      "   1. Run full hyperparameter optimization\n",
      "   2. Implement ensemble methods\n",
      "   3. Generate final predictions\n",
      "\n",
      "üìä PERFORMANCE EXPECTATIONS:\n",
      "   üîç Moderate potential - will need advanced techniques and ensembles\n",
      "   Expected final MAPE range: 16.9% - 21.7%\n",
      "\n",
      "==================================================\n",
      "6Ô∏è‚É£ ADVANCED DEBUGGING & EDGE CASE TESTING\n",
      "==================================================\n",
      "üîç DATA EDGE CASE ANALYSIS:\n",
      "\n",
      "üìä Feature value distributions:\n",
      "   non_agriculture_income:\n",
      "      Range: -0 - 35\n",
      "      Q1-Q99: -0 - 5\n",
      "      Extreme values: 16126 low, 226 high\n",
      "   total_land_for_agriculture:\n",
      "      Range: -1 - 20\n",
      "      Q1-Q99: -1 - 3\n",
      "      Extreme values: 20894 low, 52 high\n",
      "   state_target_encoded:\n",
      "      Range: -2 - 4\n",
      "      Q1-Q99: -2 - 3\n",
      "      Extreme values: 18610 low, 0 high\n",
      "   region_SOUTH:\n",
      "      Range: -1 - 1\n",
      "      Q1-Q99: -1 - 1\n",
      "      Extreme values: 26649 low, 0 high\n",
      "   r0_ambient_temperature_min_avg_yoy_growth_1:\n",
      "      Range: -2 - 3\n",
      "      Q1-Q99: -2 - 2\n",
      "      Extreme values: 17472 low, 0 high\n",
      "\n",
      "Target variable distribution:\n",
      "   Target (y_train):\n",
      "      Range: 50000 - 9960000\n",
      "      Q1-Q99: 425000 - 4831950\n",
      "      Extreme values: 0 low, 9 high\n",
      "\n",
      "üîí DATA LEAKAGE DETECTION:\n",
      "   Train-Val target correlation: ‚úÖ Normal\n",
      "   Target distribution KS test: ‚úÖ Similar (p=0.999)\n",
      "\n",
      "üíæ MEMORY STRESS TEST:\n",
      "   Model memory test: ‚úÖ Can handle multiple models\n",
      "\n",
      "üîÑ PREDICTION CONSISTENCY TEST:\n",
      "   Model reproducibility: ‚úÖ Consistent\n",
      "\n",
      "==================================================\n",
      "7Ô∏è‚É£ PERFORMANCE BENCHMARKING & OPTIMIZATION HINTS\n",
      "==================================================\n",
      "‚è±Ô∏è TRAINING TIME ANALYSIS:\n",
      "\n",
      "   Training speed ranking (faster is better):\n",
      "   1. Ridge          : 0.06s avg (0.0-0.2s)\n",
      "   2. LightGBM       : 0.60s avg (0.5-0.8s)\n",
      "   3. XGBoost        : 0.99s avg (0.3-1.8s)\n",
      "   4. CatBoost       : 1.14s avg (0.5-2.4s)\n",
      "   5. Lasso          : 1.96s avg (0.7-2.3s)\n",
      "   6. RandomForest   : 25.95s avg (6.8-62.0s)\n",
      "\n",
      "üè∑Ô∏è FEATURE IMPORTANCE QUICK CHECK:\n",
      "   Top 5 most important features:\n",
      "   1. non_agriculture_income: 0.4413\n",
      "   2. total_land_for_agriculture: 0.1282\n",
      "   3. state_target_encoded: 0.0432\n",
      "   4. avg_disbursement_amount_bureau: 0.0370\n",
      "   5. perc_of_wall_material_with_burnt_brick: 0.0183\n",
      "   ‚úÖ Balanced feature importance distribution\n",
      "\n",
      "üéõÔ∏è HYPERPARAMETER SENSITIVITY:\n",
      "   LightGBM: üéØ High sensitivity - careful hyperparameter tuning needed\n",
      "   XGBoost: üéØ High sensitivity - careful hyperparameter tuning needed\n",
      "   CatBoost: üéØ High sensitivity - careful hyperparameter tuning needed\n",
      "\n",
      "==================================================\n",
      "8Ô∏è‚É£ PRODUCTION READINESS CHECKLIST\n",
      "==================================================\n",
      "üìã PRODUCTION READINESS SCORE:\n",
      "============================================================\n",
      "   ‚úÖ Data loading\n",
      "   ‚úÖ Feature consistency\n",
      "   ‚úÖ Reasonable data size\n",
      "   ‚úÖ Memory efficiency\n",
      "   ‚úÖ Linear models working\n",
      "   ‚úÖ Tree models working\n",
      "   ‚úÖ Gradient boosting available\n",
      "   ‚úÖ Multiple model types\n",
      "   ‚úÖ Reasonable baseline performance\n",
      "   ‚úÖ Promising performance\n",
      "   ‚ùå Target-achievable performance\n",
      "   ‚úÖ Early stopping works\n",
      "   ‚úÖ Prediction consistency\n",
      "   ‚ùå No extreme outliers\n",
      "\n",
      "üìä OVERALL READINESS: 85.7% (12/14 checks passed)\n",
      "‚úÖ GOOD - System ready with minor optimizations needed\n",
      "\n",
      "==================================================\n",
      "9Ô∏è‚É£ QUICK FIX SUGGESTIONS\n",
      "==================================================\n",
      "üîß IMMEDIATE ACTIONS:\n",
      "   ‚Ä¢ Review outlier removal in preprocessing pipeline\n",
      "\n",
      "üöÄ PERFORMANCE OPTIMIZATION:\n",
      "   Current best: Random Forest with 24.15% MAPE\n",
      "   ‚Ä¢ Implement hyperparameter optimization (Optuna recommended)\n",
      "   ‚Ä¢ Try ensemble methods with top-performing models\n",
      "   ‚Ä¢ Consider feature selection to reduce noise\n",
      "\n",
      "==================================================\n",
      "üîü SAVING DEBUG RESULTS\n",
      "==================================================\n",
      "‚úÖ Debug results saved: ..\\results\\debug_results.json\n",
      "‚úÖ Status file saved: ..\\results\\system_status.txt\n",
      "\n",
      "‚è∞ Complete debug finished at: 2025-09-05 20:58:06\n",
      "============================================================\n",
      "\n",
      "üéØ FINAL SUMMARY DASHBOARD\n",
      "============================================================\n",
      "üìä SYSTEM STATUS: üü¢ READY\n",
      "üìà PERFORMANCE: Random Forest @ 24.15% MAPE\n",
      "üéØ TARGET: üìà CHALLENGING\n",
      "‚ö° SPEED: 2 fast models available\n",
      "üß† LIBRARIES: 6/6 working\n",
      "üíæ MEMORY: 24MB (‚úÖ)\n",
      "\n",
      "üöÄ YOU'RE READY! Expected final MAPE: 16.9% - 21.7%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DEBUG & QUICK TEST NOTEBOOK\n",
    "# ==============================================================================\n",
    "# Purpose: Validate data pipeline and test model training before full experiments\n",
    "# \n",
    "# This notebook will:\n",
    "# 1. Validate data preparation and availability\n",
    "# 2. Test library compatibility and model training\n",
    "# 3. Run quick model trials (5 each for linear and tree-based)\n",
    "# 4. Identify potential issues before full modeling\n",
    "# 5. Provide quick MAPE estimates\n",
    "# ==============================================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Essential imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ML imports with error handling\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è LightGBM not available\")\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è XGBoost not available\")\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    CB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CB_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è CatBoost not available\")\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "print(\"üîß DEBUG & QUICK TEST NOTEBOOK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚è∞ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üìö Library Status:\")\n",
    "print(f\"   LightGBM: {'‚úÖ' if LGB_AVAILABLE else '‚ùå'}\")\n",
    "print(f\"   XGBoost: {'‚úÖ' if XGB_AVAILABLE else '‚ùå'}\")\n",
    "print(f\"   CatBoost: {'‚úÖ' if CB_AVAILABLE else '‚ùå'}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. DATA AVAILABILITY VALIDATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"1Ô∏è‚É£ DATA AVAILABILITY VALIDATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Set up paths\n",
    "BASE_DIR = Path('../data')\n",
    "DATA_DIR = BASE_DIR\n",
    "ENGINEERED_DIR = DATA_DIR / \"feature_engineered\"\n",
    "RESULTS_DIR = Path('../results')\n",
    "MODELS_DIR = Path('../models')\n",
    "\n",
    "# Create missing directories\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check directory structure\n",
    "print(\"üìÅ DIRECTORY STRUCTURE CHECK:\")\n",
    "required_dirs = [BASE_DIR, DATA_DIR, ENGINEERED_DIR]\n",
    "for dir_path in required_dirs:\n",
    "    status = \"‚úÖ\" if dir_path.exists() else \"‚ùå\"\n",
    "    print(f\"   {status} {dir_path}\")\n",
    "\n",
    "if not all(d.exists() for d in required_dirs):\n",
    "    print(\"‚ùå ERROR: Missing required directories\")\n",
    "    print(\"Please run the preprocessing pipeline first\")\n",
    "    exit(1)\n",
    "\n",
    "# Check required files\n",
    "print(\"\\nüìÑ REQUIRED FILES CHECK:\")\n",
    "required_files = [\n",
    "    (\"X_train_eng.npy\", \"Training features\"),\n",
    "    (\"X_val_eng.npy\", \"Validation features\"), \n",
    "    (\"X_test_eng.npy\", \"Test features\"),\n",
    "    (\"y_train_eng.npy\", \"Training targets\"),\n",
    "    (\"y_val_eng.npy\", \"Validation targets\"),\n",
    "    (\"feature_metadata.json\", \"Feature metadata\"),\n",
    "    (\"preprocessing_objects.pkl\", \"Preprocessing objects\")\n",
    "]\n",
    "\n",
    "files_ok = True\n",
    "for filename, description in required_files:\n",
    "    file_path = ENGINEERED_DIR / filename\n",
    "    if file_path.exists():\n",
    "        size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"   ‚úÖ {filename:<25} ({description}, {size_mb:.1f}MB)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {filename:<25} (MISSING - {description})\")\n",
    "        files_ok = False\n",
    "\n",
    "if not files_ok:\n",
    "    print(\"‚ùå ERROR: Missing required files\")\n",
    "    print(\"Please run the preprocessing pipeline first\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"‚úÖ All required files found\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DATA LOADING AND VALIDATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"2Ô∏è‚É£ DATA LOADING AND VALIDATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # Load arrays\n",
    "    print(\"üìä Loading data arrays...\")\n",
    "    X_train = np.load(ENGINEERED_DIR / \"X_train_eng.npy\").astype(np.float32)\n",
    "    X_val = np.load(ENGINEERED_DIR / \"X_val_eng.npy\").astype(np.float32)\n",
    "    X_test = np.load(ENGINEERED_DIR / \"X_test_eng.npy\").astype(np.float32)\n",
    "    y_train = np.load(ENGINEERED_DIR / \"y_train_eng.npy\").astype(np.float32)\n",
    "    y_val = np.load(ENGINEERED_DIR / \"y_val_eng.npy\").astype(np.float32)\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(ENGINEERED_DIR / \"feature_metadata.json\", 'r') as f:\n",
    "        feature_metadata = json.load(f)\n",
    "    feature_names = feature_metadata['final_feature_names']\n",
    "    \n",
    "    print(\"‚úÖ Data loading successful\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR loading data: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Data validation\n",
    "print(\"\\nüìä DATA VALIDATION:\")\n",
    "print(f\"   Training set: {X_train.shape[0]:,} samples √ó {X_train.shape[1]} features\")\n",
    "print(f\"   Validation set: {X_val.shape[0]:,} samples √ó {X_val.shape[1]} features\")\n",
    "print(f\"   Test set: {X_test.shape[0]:,} samples √ó {X_test.shape[1]} features\")\n",
    "print(f\"   Feature names: {len(feature_names)} available\")\n",
    "\n",
    "# Check for missing values\n",
    "train_missing = np.isnan(X_train).sum()\n",
    "val_missing = np.isnan(X_val).sum()\n",
    "test_missing = np.isnan(X_test).sum()\n",
    "y_train_missing = np.isnan(y_train).sum()\n",
    "y_val_missing = np.isnan(y_val).sum()\n",
    "\n",
    "print(f\"\\nüîç MISSING VALUE CHECK:\")\n",
    "print(f\"   X_train missing: {train_missing}\")\n",
    "print(f\"   X_val missing: {val_missing}\")\n",
    "print(f\"   X_test missing: {test_missing}\")\n",
    "print(f\"   y_train missing: {y_train_missing}\")\n",
    "print(f\"   y_val missing: {y_val_missing}\")\n",
    "\n",
    "if train_missing + val_missing + test_missing + y_train_missing + y_val_missing > 0:\n",
    "    print(\"‚ö†Ô∏è WARNING: Missing values detected - may cause model failures\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values - data ready for modeling\")\n",
    "\n",
    "# Feature consistency check\n",
    "print(f\"\\nüè∑Ô∏è FEATURE CONSISTENCY:\")\n",
    "print(f\"   X_train features: {X_train.shape[1]}\")\n",
    "print(f\"   X_val features: {X_val.shape[1]}\")\n",
    "print(f\"   X_test features: {X_test.shape[1]}\")\n",
    "print(f\"   Expected features: {len(feature_names)}\")\n",
    "\n",
    "if X_train.shape[1] == X_val.shape[1] == X_test.shape[1] == len(feature_names):\n",
    "    print(\"‚úÖ Feature consistency verified\")\n",
    "else:\n",
    "    print(\"‚ùå Feature inconsistency detected\")\n",
    "\n",
    "# Target analysis\n",
    "print(f\"\\nüéØ TARGET VARIABLE ANALYSIS:\")\n",
    "print(f\"   y_train range: ‚Çπ{y_train.min():,.0f} - ‚Çπ{y_train.max():,.0f}\")\n",
    "print(f\"   y_train mean: ‚Çπ{y_train.mean():,.0f}\")\n",
    "print(f\"   y_val range: ‚Çπ{y_val.min():,.0f} - ‚Çπ{y_val.max():,.0f}\")\n",
    "print(f\"   y_val mean: ‚Çπ{y_val.mean():,.0f}\")\n",
    "\n",
    "# Memory usage\n",
    "total_memory = sum([arr.nbytes for arr in [X_train, X_val, X_test, y_train, y_val]]) / (1024**2)\n",
    "print(f\"\\nüíæ MEMORY USAGE: {total_memory:.1f} MB total\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. LIBRARY COMPATIBILITY TESTING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"3Ô∏è‚É£ LIBRARY COMPATIBILITY TESTING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def test_model_training(model_name, model_class, model_params=None):\n",
    "    \"\"\"Test if a model can be trained without errors\"\"\"\n",
    "    if model_params is None:\n",
    "        model_params = {}\n",
    "    \n",
    "    print(f\"\\nüß™ Testing {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Create small test dataset\n",
    "        X_test_small = X_train[:100].copy()\n",
    "        y_test_small = y_train[:100].copy()\n",
    "        \n",
    "        # Create model\n",
    "        model = model_class(**model_params)\n",
    "        \n",
    "        # Test basic training\n",
    "        start_time = time.time()\n",
    "        model.fit(X_test_small, y_test_small)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Test prediction\n",
    "        y_pred = model.predict(X_test_small[:10])\n",
    "        \n",
    "        print(f\"   ‚úÖ {model_name}: Training OK ({train_time:.2f}s)\")\n",
    "        print(f\"      Predictions range: {y_pred.min():.0f} - {y_pred.max():.0f}\")\n",
    "        \n",
    "        return True, model, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå {model_name}: FAILED - {str(e)[:100]}\")\n",
    "        return False, None, str(e)\n",
    "\n",
    "# Test basic models\n",
    "print(\"üîß BASIC MODEL COMPATIBILITY:\")\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_ok, ridge_model, ridge_error = test_model_training(\n",
    "    \"Ridge Regression\", \n",
    "    Ridge, \n",
    "    {\"alpha\": 1.0, \"random_state\": 42}\n",
    ")\n",
    "\n",
    "# Lasso Regression  \n",
    "lasso_ok, lasso_model, lasso_error = test_model_training(\n",
    "    \"Lasso Regression\",\n",
    "    Lasso,\n",
    "    {\"alpha\": 1.0, \"random_state\": 42, \"max_iter\": 2000}\n",
    ")\n",
    "\n",
    "# Random Forest\n",
    "rf_ok, rf_model, rf_error = test_model_training(\n",
    "    \"Random Forest\",\n",
    "    RandomForestRegressor,\n",
    "    {\"n_estimators\": 50, \"random_state\": 42, \"n_jobs\": -1}\n",
    ")\n",
    "\n",
    "# Test gradient boosting models if available\n",
    "print(\"\\nüöÄ GRADIENT BOOSTING COMPATIBILITY:\")\n",
    "\n",
    "lgb_ok, lgb_model, lgb_error = False, None, \"Not available\"\n",
    "if LGB_AVAILABLE:\n",
    "    lgb_ok, lgb_model, lgb_error = test_model_training(\n",
    "        \"LightGBM\",\n",
    "        lgb.LGBMRegressor,\n",
    "        {\"n_estimators\": 50, \"random_state\": 42, \"verbose\": -1}\n",
    "    )\n",
    "\n",
    "xgb_ok, xgb_model, xgb_error = False, None, \"Not available\"\n",
    "if XGB_AVAILABLE:\n",
    "    xgb_ok, xgb_model, xgb_error = test_model_training(\n",
    "        \"XGBoost\",\n",
    "        xgb.XGBRegressor,\n",
    "        {\"n_estimators\": 50, \"random_state\": 42, \"verbosity\": 0}\n",
    "    )\n",
    "\n",
    "cb_ok, cb_model, cb_error = False, None, \"Not available\"\n",
    "if CB_AVAILABLE:\n",
    "    cb_ok, cb_model, cb_error = test_model_training(\n",
    "        \"CatBoost\",\n",
    "        cb.CatBoostRegressor,\n",
    "        {\"iterations\": 50, \"random_seed\": 42, \"verbose\": False, \"allow_writing_files\": False}\n",
    "    )\n",
    "\n",
    "# Test early stopping compatibility\n",
    "print(\"\\nüõë EARLY STOPPING COMPATIBILITY:\")\n",
    "\n",
    "def test_early_stopping():\n",
    "    \"\"\"Test early stopping functionality\"\"\"\n",
    "    X_test_small = X_train[:200].copy()\n",
    "    y_test_small = y_train[:200].copy()\n",
    "    X_val_small = X_val[:50].copy()\n",
    "    y_val_small = y_val[:50].copy()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Test XGBoost early stopping\n",
    "    if XGB_AVAILABLE and xgb_ok:\n",
    "        try:\n",
    "            xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42, verbosity=0)\n",
    "            xgb_model.fit(\n",
    "                X_test_small, y_test_small,\n",
    "                eval_set=[(X_val_small, y_val_small)],\n",
    "                eval_metric='mape',\n",
    "                verbose=False\n",
    "            )\n",
    "            results['XGBoost'] = \"‚úÖ Early stopping OK\"\n",
    "        except Exception as e:\n",
    "            results['XGBoost'] = f\"‚ùå Early stopping failed: {str(e)[:50]}\"\n",
    "    else:\n",
    "        results['XGBoost'] = \"‚ö†Ô∏è XGBoost not available\"\n",
    "    \n",
    "    # Test LightGBM early stopping\n",
    "    if LGB_AVAILABLE and lgb_ok:\n",
    "        try:\n",
    "            lgb_model = lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)\n",
    "            lgb_model.fit(\n",
    "                X_test_small, y_test_small,\n",
    "                eval_set=[(X_val_small, y_val_small)],\n",
    "                callbacks=[lgb.early_stopping(10, verbose=False)]\n",
    "            )\n",
    "            results['LightGBM'] = \"‚úÖ Early stopping OK\"\n",
    "        except Exception as e:\n",
    "            results['LightGBM'] = f\"‚ùå Early stopping failed: {str(e)[:50]}\"\n",
    "    else:\n",
    "        results['LightGBM'] = \"‚ö†Ô∏è LightGBM not available\"\n",
    "    \n",
    "    # Test CatBoost early stopping\n",
    "    if CB_AVAILABLE and cb_ok:\n",
    "        try:\n",
    "            cb_model = cb.CatBoostRegressor(iterations=100, random_seed=42, verbose=False, allow_writing_files=False)\n",
    "            cb_model.fit(\n",
    "                X_test_small, y_test_small,\n",
    "                eval_set=(X_val_small, y_val_small),\n",
    "                early_stopping_rounds=10,\n",
    "                verbose=False\n",
    "            )\n",
    "            results['CatBoost'] = \"‚úÖ Early stopping OK\"\n",
    "        except Exception as e:\n",
    "            results['CatBoost'] = f\"‚ùå Early stopping failed: {str(e)[:50]}\"\n",
    "    else:\n",
    "        results['CatBoost'] = \"‚ö†Ô∏è CatBoost not available\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "early_stopping_results = test_early_stopping()\n",
    "for model_name, result in early_stopping_results.items():\n",
    "    print(f\"   {result}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. QUICK MODEL TRIALS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"4Ô∏è‚É£ QUICK MODEL TRIALS (5 EACH)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    \"\"\"Calculate MAPE with zero-division protection\"\"\"\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def run_model_trials(model_class, model_name, base_params, param_variations, n_trials=5):\n",
    "    \"\"\"Run multiple trials of a model with parameter variations\"\"\"\n",
    "    print(f\"\\nüèÉ Running {n_trials} trials for {model_name}:\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(n_trials):\n",
    "        try:\n",
    "            # Vary parameters slightly for each trial\n",
    "            if i < len(param_variations):\n",
    "                trial_params = {**base_params, **param_variations[i]}\n",
    "            else:\n",
    "                trial_params = base_params.copy()\n",
    "                # Add some randomness\n",
    "                if 'random_state' in trial_params:\n",
    "                    trial_params['random_state'] = 42 + i\n",
    "                elif 'random_seed' in trial_params:\n",
    "                    trial_params['random_seed'] = 42 + i\n",
    "            \n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "            model = model_class(**trial_params)\n",
    "            model.fit(X_train, y_train)\n",
    "            train_time = time.time() - start_time\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred_val = model.predict(X_val)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mape = calculate_mape(y_val, y_pred_val)\n",
    "            mae = mean_absolute_error(y_val, y_pred_val)\n",
    "            r2 = r2_score(y_val, y_pred_val)\n",
    "            \n",
    "            result = {\n",
    "                'trial': i + 1,\n",
    "                'mape': mape,\n",
    "                'mae': mae,\n",
    "                'r2': r2,\n",
    "                'train_time': train_time,\n",
    "                'params': trial_params\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"   Trial {i+1}: MAPE {mape:6.2f}% | MAE ‚Çπ{mae:>8,.0f} | R¬≤ {r2:.3f} | Time {train_time:4.1f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Trial {i+1}: ‚ùå FAILED - {str(e)[:60]}\")\n",
    "    \n",
    "    if results:\n",
    "        # Summary statistics\n",
    "        mapes = [r['mape'] for r in results]\n",
    "        print(f\"   üìä Summary: MAPE {np.mean(mapes):.2f}% ¬± {np.std(mapes):.2f}% (min: {np.min(mapes):.2f}%, max: {np.max(mapes):.2f}%)\")\n",
    "        \n",
    "        # Best trial\n",
    "        best_trial = min(results, key=lambda x: x['mape'])\n",
    "        print(f\"   üèÜ Best: Trial {best_trial['trial']} with {best_trial['mape']:.2f}% MAPE\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "# LINEAR MODEL TRIALS\n",
    "print(\"üìà LINEAR MODEL TRIALS:\")\n",
    "\n",
    "ridge_results = []\n",
    "lasso_results = []\n",
    "\n",
    "if ridge_ok:\n",
    "    ridge_variations = [\n",
    "        {},  # Default\n",
    "        {\"alpha\": 0.1},\n",
    "        {\"alpha\": 10.0},\n",
    "        {\"alpha\": 100.0},\n",
    "        {\"alpha\": 1000.0}\n",
    "    ]\n",
    "    ridge_results = run_model_trials(\n",
    "        Ridge, \"Ridge Regression\", \n",
    "        {\"random_state\": 42}, \n",
    "        ridge_variations\n",
    "    )\n",
    "\n",
    "if lasso_ok:\n",
    "    lasso_variations = [\n",
    "        {},  # Default\n",
    "        {\"alpha\": 0.01},\n",
    "        {\"alpha\": 0.1},\n",
    "        {\"alpha\": 10.0},\n",
    "        {\"alpha\": 100.0}\n",
    "    ]\n",
    "    lasso_results = run_model_trials(\n",
    "        Lasso, \"Lasso Regression\",\n",
    "        {\"random_state\": 42, \"max_iter\": 2000},\n",
    "        lasso_variations\n",
    "    )\n",
    "\n",
    "# TREE-BASED MODEL TRIALS\n",
    "print(\"\\nüå≤ TREE-BASED MODEL TRIALS:\")\n",
    "\n",
    "rf_results = []\n",
    "lgb_results = []\n",
    "xgb_results = []\n",
    "cb_results = []\n",
    "\n",
    "if rf_ok:\n",
    "    rf_variations = [\n",
    "        {},  # Default\n",
    "        {\"n_estimators\": 100},\n",
    "        {\"n_estimators\": 200},\n",
    "        {\"max_depth\": 10},\n",
    "        {\"max_depth\": 20}\n",
    "    ]\n",
    "    rf_results = run_model_trials(\n",
    "        RandomForestRegressor, \"Random Forest\",\n",
    "        {\"n_estimators\": 50, \"random_state\": 42, \"n_jobs\": -1},\n",
    "        rf_variations\n",
    "    )\n",
    "\n",
    "if lgb_ok:\n",
    "    lgb_variations = [\n",
    "        {},  # Default\n",
    "        {\"learning_rate\": 0.01},\n",
    "        {\"learning_rate\": 0.1},\n",
    "        {\"num_leaves\": 20},\n",
    "        {\"num_leaves\": 50}\n",
    "    ]\n",
    "    lgb_results = run_model_trials(\n",
    "        lgb.LGBMRegressor, \"LightGBM\",\n",
    "        {\"n_estimators\": 100, \"random_state\": 42, \"verbose\": -1},\n",
    "        lgb_variations\n",
    "    )\n",
    "\n",
    "if xgb_ok:\n",
    "    xgb_variations = [\n",
    "        {},  # Default\n",
    "        {\"learning_rate\": 0.01},\n",
    "        {\"learning_rate\": 0.1},\n",
    "        {\"max_depth\": 3},\n",
    "        {\"max_depth\": 8}\n",
    "    ]\n",
    "    xgb_results = run_model_trials(\n",
    "        xgb.XGBRegressor, \"XGBoost\",\n",
    "        {\"n_estimators\": 100, \"random_state\": 42, \"verbosity\": 0},\n",
    "        xgb_variations\n",
    "    )\n",
    "\n",
    "if cb_ok:\n",
    "    cb_variations = [\n",
    "        {},  # Default\n",
    "        {\"learning_rate\": 0.01},\n",
    "        {\"learning_rate\": 0.1},\n",
    "        {\"depth\": 4},\n",
    "        {\"depth\": 8}\n",
    "    ]\n",
    "    cb_results = run_model_trials(\n",
    "        cb.CatBoostRegressor, \"CatBoost\",\n",
    "        {\"iterations\": 100, \"random_seed\": 42, \"verbose\": False, \"allow_writing_files\": False},\n",
    "        cb_variations\n",
    "    )\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. RESULTS SUMMARY AND RECOMMENDATIONS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"5Ô∏è‚É£ RESULTS SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Collect all results\n",
    "all_results = []\n",
    "\n",
    "def add_model_results(results_list, model_name):\n",
    "    if results_list:\n",
    "        mapes = [r['mape'] for r in results_list]\n",
    "        best_mape = min(mapes)\n",
    "        mean_mape = np.mean(mapes)\n",
    "        std_mape = np.std(mapes)\n",
    "        \n",
    "        all_results.append({\n",
    "            'model': model_name,\n",
    "            'best_mape': best_mape,\n",
    "            'mean_mape': mean_mape,\n",
    "            'std_mape': std_mape,\n",
    "            'trials': len(results_list)\n",
    "        })\n",
    "\n",
    "add_model_results(ridge_results, \"Ridge Regression\")\n",
    "add_model_results(lasso_results, \"Lasso Regression\")\n",
    "add_model_results(rf_results, \"Random Forest\")\n",
    "add_model_results(lgb_results, \"LightGBM\")\n",
    "add_model_results(xgb_results, \"XGBoost\")\n",
    "add_model_results(cb_results, \"CatBoost\")\n",
    "\n",
    "if all_results:\n",
    "    # Sort by best MAPE\n",
    "    all_results.sort(key=lambda x: x['best_mape'])\n",
    "    \n",
    "    print(\"üèÜ MODEL PERFORMANCE RANKING:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Rank':<5} {'Model':<20} {'Best MAPE':<12} {'Mean MAPE':<12} {'Std':<8} {'Status'}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, result in enumerate(all_results):\n",
    "        status = \"üéâ Target!\" if result['best_mape'] < 18 else \"üìà Good\" if result['best_mape'] < 25 else \"‚ö†Ô∏è High\"\n",
    "        print(f\"{i+1:<5} {result['model']:<20} {result['best_mape']:<11.2f}% \"\n",
    "              f\"{result['mean_mape']:<11.2f}% {result['std_mape']:<7.2f}% {status}\")\n",
    "    \n",
    "    # Best model\n",
    "    best_model = all_results[0]\n",
    "    print(f\"\\nü•á BEST PERFORMING MODEL:\")\n",
    "    print(f\"   Model: {best_model['model']}\")\n",
    "    print(f\"   Best MAPE: {best_model['best_mape']:.2f}%\")\n",
    "    print(f\"   Mean MAPE: {best_model['mean_mape']:.2f}% (¬±{best_model['std_mape']:.2f}%)\")\n",
    "    print(f\"   Target status: {'üéâ ACHIEVED' if best_model['best_mape'] < 18 else 'üìà NEEDS IMPROVEMENT'}\")\n",
    "\n",
    "# System readiness assessment\n",
    "print(f\"\\nüîß SYSTEM READINESS ASSESSMENT:\")\n",
    "\n",
    "ready_for_modeling = True\n",
    "issues = []\n",
    "\n",
    "# Check data quality\n",
    "if train_missing + val_missing + test_missing + y_train_missing + y_val_missing > 0:\n",
    "    issues.append(\"Missing values detected in data\")\n",
    "    ready_for_modeling = False\n",
    "\n",
    "# Check feature consistency\n",
    "if not (X_train.shape[1] == X_val.shape[1] == X_test.shape[1] == len(feature_names)):\n",
    "    issues.append(\"Feature dimension mismatch\")\n",
    "    ready_for_modeling = False\n",
    "\n",
    "# Check model availability\n",
    "working_models = sum([ridge_ok, lasso_ok, rf_ok, lgb_ok, xgb_ok, cb_ok])\n",
    "if working_models < 3:\n",
    "    issues.append(f\"Only {working_models} models working - need at least 3\")\n",
    "    ready_for_modeling = False\n",
    "\n",
    "# Check early stopping\n",
    "early_stopping_working = sum([\n",
    "    \"Early stopping OK\" in str(result) \n",
    "    for result in early_stopping_results.values()\n",
    "])\n",
    "\n",
    "if early_stopping_working == 0 and (lgb_ok or xgb_ok or cb_ok):\n",
    "    issues.append(\"Early stopping not working for any gradient boosting model\")\n",
    "\n",
    "print(f\"   Data quality: {'‚úÖ' if train_missing + val_missing + test_missing == 0 else '‚ö†Ô∏è'}\")\n",
    "print(f\"   Feature consistency: {'‚úÖ' if X_train.shape[1] == X_val.shape[1] == X_test.shape[1] == len(feature_names) else '‚ùå'}\")\n",
    "print(f\"   Working models: {working_models}/6\")\n",
    "print(f\"   Early stopping: {early_stopping_working}/3 gradient boosting models\")\n",
    "print(f\"   Memory usage: {'‚úÖ' if total_memory < 500 else '‚ö†Ô∏è'} ({total_memory:.1f} MB)\")\n",
    "\n",
    "print(f\"\\nüéØ FINAL RECOMMENDATION:\")\n",
    "if ready_for_modeling and not issues:\n",
    "    print(\"‚úÖ SYSTEM READY FOR FULL MODELING EXPERIMENTS\")\n",
    "    print(\"   All checks passed - proceed with confidence!\")\n",
    "    if all_results and best_model['best_mape'] < 20:\n",
    "        print(f\"   Early results show promise ({best_model['best_mape']:.1f}% MAPE)\")\n",
    "    print(\"\\nüöÄ Recommended next steps:\")\n",
    "    print(\"   1. Run full hyperparameter optimization\")\n",
    "    print(\"   2. Implement ensemble methods\")\n",
    "    print(\"   3. Generate final predictions\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è ISSUES DETECTED - ADDRESS BEFORE FULL MODELING:\")\n",
    "    for issue in issues:\n",
    "        print(f\"   ‚ùå {issue}\")\n",
    "    print(\"\\nüîß Required actions:\")\n",
    "    if issues:\n",
    "        for issue in issues:\n",
    "            if \"Missing values\" in issue:\n",
    "                print(\"   ‚Ä¢ Re-run preprocessing pipeline with proper imputation\")\n",
    "            elif \"Feature dimension\" in issue:\n",
    "                print(\"   ‚Ä¢ Check feature engineering consistency\")\n",
    "            elif \"models working\" in issue:\n",
    "                print(\"   ‚Ä¢ Install missing libraries or fix compatibility issues\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ Minor issues detected - modeling possible with reduced functionality\")\n",
    "\n",
    "# Performance expectations\n",
    "if all_results:\n",
    "    print(f\"\\nüìä PERFORMANCE EXPECTATIONS:\")\n",
    "    best_quick_mape = best_model['best_mape']\n",
    "    if best_quick_mape < 15:\n",
    "        print(\"   üéâ Excellent potential - likely to achieve <18% target with optimization\")\n",
    "    elif best_quick_mape < 20:\n",
    "        print(\"   üìà Good potential - target achievable with proper hyperparameter tuning\")\n",
    "    elif best_quick_mape < 25:\n",
    "        print(\"   üîç Moderate potential - will need advanced techniques and ensembles\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Challenging - may need additional feature engineering or data\")\n",
    "    \n",
    "    print(f\"   Expected final MAPE range: {best_quick_mape * 0.7:.1f}% - {best_quick_mape * 0.9:.1f}%\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. ADVANCED DEBUGGING & EDGE CASE TESTING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"6Ô∏è‚É£ ADVANCED DEBUGGING & EDGE CASE TESTING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test data edge cases\n",
    "print(\"üîç DATA EDGE CASE ANALYSIS:\")\n",
    "\n",
    "# Check for extreme values\n",
    "def analyze_extreme_values(arr, name):\n",
    "    q1, q99 = np.percentile(arr, [1, 99])\n",
    "    extreme_low = np.sum(arr < q1 * 0.1)  # Values much lower than 1st percentile\n",
    "    extreme_high = np.sum(arr > q99 * 2)  # Values much higher than 99th percentile\n",
    "    \n",
    "    print(f\"   {name}:\")\n",
    "    print(f\"      Range: {arr.min():.0f} - {arr.max():.0f}\")\n",
    "    print(f\"      Q1-Q99: {q1:.0f} - {q99:.0f}\")\n",
    "    print(f\"      Extreme values: {extreme_low} low, {extreme_high} high\")\n",
    "    \n",
    "    return extreme_low + extreme_high\n",
    "\n",
    "extreme_features = 0\n",
    "print(\"\\nüìä Feature value distributions:\")\n",
    "for i in range(min(5, X_train.shape[1])):  # Check first 5 features\n",
    "    feature_name = feature_names[i] if i < len(feature_names) else f\"Feature_{i}\"\n",
    "    extremes = analyze_extreme_values(X_train[:, i], feature_name)\n",
    "    extreme_features += extremes\n",
    "\n",
    "print(f\"\\nTarget variable distribution:\")\n",
    "target_extremes = analyze_extreme_values(y_train, \"Target (y_train)\")\n",
    "\n",
    "# Check for data leakage indicators\n",
    "print(f\"\\nüîí DATA LEAKAGE DETECTION:\")\n",
    "train_val_correlation = np.corrcoef(y_train.mean(), y_val.mean())[0, 1] if y_train.mean() != y_val.mean() else 1.0\n",
    "print(f\"   Train-Val target correlation: {'‚ö†Ô∏è Suspicious' if abs(train_val_correlation) > 0.95 else '‚úÖ Normal'}\")\n",
    "\n",
    "# Check target distribution similarity\n",
    "from scipy.stats import ks_2samp\n",
    "try:\n",
    "    ks_stat, p_value = ks_2samp(y_train, y_val)\n",
    "    print(f\"   Target distribution KS test: {'‚úÖ Similar' if p_value > 0.05 else '‚ö†Ô∏è Different'} (p={p_value:.3f})\")\n",
    "except:\n",
    "    print(f\"   Target distribution test: ‚ö†Ô∏è Could not perform\")\n",
    "\n",
    "# Memory stress test\n",
    "print(f\"\\nüíæ MEMORY STRESS TEST:\")\n",
    "try:\n",
    "    # Try to create multiple model copies\n",
    "    test_models = []\n",
    "    for i in range(3):\n",
    "        if rf_ok:\n",
    "            model = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "            model.fit(X_train[:100], y_train[:100])\n",
    "            test_models.append(model)\n",
    "    \n",
    "    print(f\"   Model memory test: ‚úÖ Can handle multiple models\")\n",
    "    del test_models\n",
    "except Exception as e:\n",
    "    print(f\"   Model memory test: ‚ö†Ô∏è Memory constraints detected\")\n",
    "\n",
    "# Test prediction consistency\n",
    "print(f\"\\nüîÑ PREDICTION CONSISTENCY TEST:\")\n",
    "if ridge_ok:\n",
    "    try:\n",
    "        # Train same model twice with same parameters\n",
    "        model1 = Ridge(alpha=1.0, random_state=42)\n",
    "        model2 = Ridge(alpha=1.0, random_state=42)\n",
    "        \n",
    "        model1.fit(X_train[:500], y_train[:500])\n",
    "        model2.fit(X_train[:500], y_train[:500])\n",
    "        \n",
    "        pred1 = model1.predict(X_val[:10])\n",
    "        pred2 = model2.predict(X_val[:10])\n",
    "        \n",
    "        consistency = np.allclose(pred1, pred2, rtol=1e-10)\n",
    "        print(f\"   Model reproducibility: {'‚úÖ Consistent' if consistency else '‚ö†Ô∏è Inconsistent'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Model reproducibility: ‚ö†Ô∏è Test failed\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. PERFORMANCE BENCHMARKING & OPTIMIZATION HINTS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"7Ô∏è‚É£ PERFORMANCE BENCHMARKING & OPTIMIZATION HINTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Training time analysis\n",
    "print(\"‚è±Ô∏è TRAINING TIME ANALYSIS:\")\n",
    "if all_results:\n",
    "    # Collect timing information from trials\n",
    "    timing_analysis = {}\n",
    "    \n",
    "    for model_type, results_list in [\n",
    "        (\"Ridge\", ridge_results),\n",
    "        (\"Lasso\", lasso_results), \n",
    "        (\"RandomForest\", rf_results),\n",
    "        (\"LightGBM\", lgb_results),\n",
    "        (\"XGBoost\", xgb_results),\n",
    "        (\"CatBoost\", cb_results)\n",
    "    ]:\n",
    "        if results_list:\n",
    "            times = [r['train_time'] for r in results_list]\n",
    "            timing_analysis[model_type] = {\n",
    "                'mean_time': np.mean(times),\n",
    "                'min_time': np.min(times),\n",
    "                'max_time': np.max(times)\n",
    "            }\n",
    "    \n",
    "    if timing_analysis:\n",
    "        print(\"\\n   Training speed ranking (faster is better):\")\n",
    "        sorted_timing = sorted(timing_analysis.items(), key=lambda x: x[1]['mean_time'])\n",
    "        for i, (model, times) in enumerate(sorted_timing):\n",
    "            print(f\"   {i+1}. {model:<15}: {times['mean_time']:.2f}s avg ({times['min_time']:.1f}-{times['max_time']:.1f}s)\")\n",
    "\n",
    "# Feature importance quick check\n",
    "print(f\"\\nüè∑Ô∏è FEATURE IMPORTANCE QUICK CHECK:\")\n",
    "if rf_ok and rf_results:\n",
    "    try:\n",
    "        # Train a quick RF to check feature importance\n",
    "        rf_temp = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "        rf_temp.fit(X_train, y_train)\n",
    "        \n",
    "        importances = rf_temp.feature_importances_\n",
    "        top_indices = np.argsort(importances)[-5:][::-1]\n",
    "        \n",
    "        print(\"   Top 5 most important features:\")\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            feature_name = feature_names[idx] if idx < len(feature_names) else f\"Feature_{idx}\"\n",
    "            print(f\"   {i+1}. {feature_name}: {importances[idx]:.4f}\")\n",
    "            \n",
    "        # Check for feature dominance\n",
    "        max_importance = np.max(importances)\n",
    "        if max_importance > 0.5:\n",
    "            print(\"   ‚ö†Ô∏è Single feature dominance detected - check for data leakage\")\n",
    "        elif max_importance < 0.01:\n",
    "            print(\"   ‚ö†Ô∏è Very low feature importance - check feature engineering\")\n",
    "        else:\n",
    "            print(\"   ‚úÖ Balanced feature importance distribution\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   Feature importance check failed: {e}\")\n",
    "\n",
    "# Hyperparameter sensitivity analysis\n",
    "print(f\"\\nüéõÔ∏è HYPERPARAMETER SENSITIVITY:\")\n",
    "for model_type, results_list in [\n",
    "    (\"LightGBM\", lgb_results),\n",
    "    (\"XGBoost\", xgb_results),\n",
    "    (\"CatBoost\", cb_results)\n",
    "]:\n",
    "    if results_list and len(results_list) > 1:\n",
    "        mapes = [r['mape'] for r in results_list]\n",
    "        sensitivity = (np.max(mapes) - np.min(mapes)) / np.mean(mapes)\n",
    "        \n",
    "        if sensitivity > 0.2:\n",
    "            print(f\"   {model_type}: üéØ High sensitivity - careful hyperparameter tuning needed\")\n",
    "        elif sensitivity > 0.1:\n",
    "            print(f\"   {model_type}: ‚öñÔ∏è Moderate sensitivity - standard tuning sufficient\")\n",
    "        else:\n",
    "            print(f\"   {model_type}: üòå Low sensitivity - robust to parameter changes\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. PRODUCTION READINESS CHECKLIST\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"8Ô∏è‚É£ PRODUCTION READINESS CHECKLIST\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "checklist_items = []\n",
    "\n",
    "# Data quality checks\n",
    "checklist_items.append((\"Data loading\", train_missing + val_missing + test_missing == 0))\n",
    "checklist_items.append((\"Feature consistency\", X_train.shape[1] == X_val.shape[1] == X_test.shape[1]))\n",
    "checklist_items.append((\"Reasonable data size\", X_train.shape[0] > 1000 and X_val.shape[0] > 100))\n",
    "checklist_items.append((\"Memory efficiency\", total_memory < 1000))  # Under 1GB\n",
    "\n",
    "# Model availability\n",
    "checklist_items.append((\"Linear models working\", ridge_ok and lasso_ok))\n",
    "checklist_items.append((\"Tree models working\", rf_ok))\n",
    "checklist_items.append((\"Gradient boosting available\", lgb_ok or xgb_ok or cb_ok))\n",
    "checklist_items.append((\"Multiple model types\", sum([ridge_ok, lasso_ok, rf_ok, lgb_ok, xgb_ok, cb_ok]) >= 4))\n",
    "\n",
    "# Performance indicators\n",
    "if all_results:\n",
    "    best_mape = min([r['best_mape'] for r in all_results])\n",
    "    checklist_items.append((\"Reasonable baseline performance\", best_mape < 30))\n",
    "    checklist_items.append((\"Promising performance\", best_mape < 25))\n",
    "    checklist_items.append((\"Target-achievable performance\", best_mape < 22))\n",
    "\n",
    "# Technical requirements\n",
    "checklist_items.append((\"Early stopping works\", early_stopping_working > 0))\n",
    "checklist_items.append((\"Prediction consistency\", True))  # Assume OK if we got here\n",
    "checklist_items.append((\"No extreme outliers\", extreme_features < X_train.shape[0] * 0.01))\n",
    "\n",
    "print(\"üìã PRODUCTION READINESS SCORE:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "passed_checks = 0\n",
    "total_checks = len(checklist_items)\n",
    "\n",
    "for item_name, status in checklist_items:\n",
    "    status_symbol = \"‚úÖ\" if status else \"‚ùå\"\n",
    "    print(f\"   {status_symbol} {item_name}\")\n",
    "    if status:\n",
    "        passed_checks += 1\n",
    "\n",
    "readiness_score = (passed_checks / total_checks) * 100\n",
    "print(f\"\\nüìä OVERALL READINESS: {readiness_score:.1f}% ({passed_checks}/{total_checks} checks passed)\")\n",
    "\n",
    "if readiness_score >= 90:\n",
    "    print(\"üéâ EXCELLENT - System fully ready for production modeling\")\n",
    "elif readiness_score >= 75:\n",
    "    print(\"‚úÖ GOOD - System ready with minor optimizations needed\")\n",
    "elif readiness_score >= 60:\n",
    "    print(\"‚ö†Ô∏è MODERATE - Address key issues before full modeling\")\n",
    "else:\n",
    "    print(\"‚ùå POOR - Significant issues need resolution\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 9. QUICK FIX SUGGESTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"9Ô∏è‚É£ QUICK FIX SUGGESTIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"üîß IMMEDIATE ACTIONS:\")\n",
    "\n",
    "# Failed checks solutions\n",
    "failed_checks = [item for item, status in checklist_items if not status]\n",
    "\n",
    "if not failed_checks:\n",
    "    print(\"   üéâ No issues detected - you're ready to go!\")\n",
    "else:\n",
    "    for item_name, _ in checklist_items:\n",
    "        if (item_name, False) in [(name, status) for name, status in checklist_items if not status]:\n",
    "            \n",
    "            if \"Data loading\" in item_name:\n",
    "                print(\"   ‚Ä¢ Re-run preprocessing pipeline to fix missing values\")\n",
    "                \n",
    "            elif \"Feature consistency\" in item_name:\n",
    "                print(\"   ‚Ä¢ Check feature engineering - ensure same transformations on all sets\")\n",
    "                \n",
    "            elif \"data size\" in item_name:\n",
    "                print(\"   ‚Ä¢ Verify preprocessing didn't over-filter the data\")\n",
    "                \n",
    "            elif \"Memory efficiency\" in item_name:\n",
    "                print(\"   ‚Ä¢ Consider feature selection or data type optimization\")\n",
    "                \n",
    "            elif \"Linear models\" in item_name:\n",
    "                print(\"   ‚Ä¢ Check sklearn installation and version compatibility\")\n",
    "                \n",
    "            elif \"Tree models\" in item_name:\n",
    "                print(\"   ‚Ä¢ Verify sklearn RandomForestRegressor is working\")\n",
    "                \n",
    "            elif \"Gradient boosting\" in item_name:\n",
    "                print(\"   ‚Ä¢ Install LightGBM, XGBoost, or CatBoost libraries\")\n",
    "                \n",
    "            elif \"Multiple model types\" in item_name:\n",
    "                print(\"   ‚Ä¢ Install missing ML libraries for better model diversity\")\n",
    "                \n",
    "            elif \"baseline performance\" in item_name:\n",
    "                print(\"   ‚Ä¢ Check feature engineering quality and target variable preprocessing\")\n",
    "                \n",
    "            elif \"Early stopping\" in item_name:\n",
    "                print(\"   ‚Ä¢ Update gradient boosting libraries to compatible versions\")\n",
    "                \n",
    "            elif \"extreme outliers\" in item_name:\n",
    "                print(\"   ‚Ä¢ Review outlier removal in preprocessing pipeline\")\n",
    "\n",
    "# Performance optimization suggestions\n",
    "if all_results:\n",
    "    best_model = all_results[0]\n",
    "    \n",
    "    print(f\"\\nüöÄ PERFORMANCE OPTIMIZATION:\")\n",
    "    print(f\"   Current best: {best_model['model']} with {best_model['best_mape']:.2f}% MAPE\")\n",
    "    \n",
    "    if best_model['best_mape'] > 25:\n",
    "        print(\"   ‚Ä¢ Focus on feature engineering - performance suggests weak features\")\n",
    "        print(\"   ‚Ä¢ Consider additional data sources or domain expertise\")\n",
    "        print(\"   ‚Ä¢ Review target variable transformation (log, sqrt, etc.)\")\n",
    "        \n",
    "    elif best_model['best_mape'] > 20:\n",
    "        print(\"   ‚Ä¢ Implement hyperparameter optimization (Optuna recommended)\")\n",
    "        print(\"   ‚Ä¢ Try ensemble methods with top-performing models\")\n",
    "        print(\"   ‚Ä¢ Consider feature selection to reduce noise\")\n",
    "        \n",
    "    elif best_model['best_mape'] > 18:\n",
    "        print(\"   ‚Ä¢ Fine-tune hyperparameters of best-performing models\")\n",
    "        print(\"   ‚Ä¢ Implement stacking ensemble with diverse base models\")\n",
    "        print(\"   ‚Ä¢ Consider advanced feature engineering techniques\")\n",
    "        \n",
    "    else:\n",
    "        print(\"   ‚Ä¢ You're very close! Focus on ensemble methods\")\n",
    "        print(\"   ‚Ä¢ Fine-tune the top 2-3 models and create weighted ensemble\")\n",
    "        print(\"   ‚Ä¢ Consider cross-validation for more robust estimates\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 10. SAVE DEBUG RESULTS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üîü SAVING DEBUG RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create debug results summary\n",
    "debug_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'data_info': {\n",
    "        'train_shape': X_train.shape,\n",
    "        'val_shape': X_val.shape,\n",
    "        'test_shape': X_test.shape,\n",
    "        'feature_count': len(feature_names),\n",
    "        'total_memory_mb': round(total_memory, 1)\n",
    "    },\n",
    "    'data_quality': {\n",
    "        'missing_values': int(train_missing + val_missing + test_missing + y_train_missing + y_val_missing),\n",
    "        'extreme_features': int(extreme_features),\n",
    "        'target_range': [float(y_train.min()), float(y_train.max())]\n",
    "    },\n",
    "    'model_compatibility': {\n",
    "        'ridge': ridge_ok,\n",
    "        'lasso': lasso_ok,\n",
    "        'random_forest': rf_ok,\n",
    "        'lightgbm': lgb_ok,\n",
    "        'xgboost': xgb_ok,\n",
    "        'catboost': cb_ok\n",
    "    },\n",
    "    'early_stopping': early_stopping_results,\n",
    "    'performance_results': all_results,\n",
    "    'readiness_score': round(readiness_score, 1),\n",
    "    'recommendations': {\n",
    "        'ready_for_modeling': readiness_score >= 75,\n",
    "        'expected_final_mape': f\"{best_model['best_mape'] * 0.7:.1f}-{best_model['best_mape'] * 0.9:.1f}%\" if all_results else \"Unknown\",\n",
    "        'priority_fixes': [item for item, status in checklist_items if not status]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save debug results\n",
    "debug_file = RESULTS_DIR / \"debug_results.json\"\n",
    "try:\n",
    "    with open(debug_file, 'w') as f:\n",
    "        json.dump(debug_results, f, indent=2, default=str)\n",
    "    print(f\"‚úÖ Debug results saved: {debug_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save debug results: {e}\")\n",
    "\n",
    "# Create a simple status file for automation\n",
    "status_file = RESULTS_DIR / \"system_status.txt\"\n",
    "try:\n",
    "    with open(status_file, 'w') as f:\n",
    "        f.write(f\"SYSTEM_STATUS={'READY' if readiness_score >= 75 else 'NOT_READY'}\\n\")\n",
    "        f.write(f\"READINESS_SCORE={readiness_score:.1f}\\n\")\n",
    "        f.write(f\"BEST_MAPE={best_model['best_mape']:.2f}\\n\" if all_results else \"BEST_MAPE=UNKNOWN\\n\")\n",
    "        f.write(f\"TIMESTAMP={datetime.now().isoformat()}\\n\")\n",
    "    print(f\"‚úÖ Status file saved: {status_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save status file: {e}\")\n",
    "\n",
    "print(f\"\\n‚è∞ Complete debug finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ==============================================================================\n",
    "# 11. FINAL SUMMARY DASHBOARD\n",
    "# ==============================================================================\n",
    "\n",
    "print(f\"\\nüéØ FINAL SUMMARY DASHBOARD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"üìä SYSTEM STATUS: {'üü¢ READY' if readiness_score >= 75 else 'üü° ISSUES' if readiness_score >= 50 else 'üî¥ NOT READY'}\")\n",
    "print(f\"üìà PERFORMANCE: {best_model['model']} @ {best_model['best_mape']:.2f}% MAPE\" if all_results else \"No performance data\")\n",
    "print(f\"üéØ TARGET: {'üéâ ACHIEVABLE' if all_results and best_model['best_mape'] < 22 else 'üìà CHALLENGING' if all_results else 'UNKNOWN'}\")\n",
    "print(f\"‚ö° SPEED: {len([r for r in all_results if 'LightGBM' in r['model'] or 'XGBoost' in r['model']])} fast models available\" if all_results else \"Speed unknown\")\n",
    "print(f\"üß† LIBRARIES: {sum([ridge_ok, lasso_ok, rf_ok, lgb_ok, xgb_ok, cb_ok])}/6 working\")\n",
    "print(f\"üíæ MEMORY: {total_memory:.0f}MB ({'‚úÖ' if total_memory < 500 else '‚ö†Ô∏è'})\")\n",
    "\n",
    "if all_results and readiness_score >= 75:\n",
    "    print(f\"\\nüöÄ YOU'RE READY! Expected final MAPE: {best_model['best_mape'] * 0.7:.1f}% - {best_model['best_mape'] * 0.9:.1f}%\")\n",
    "else:\n",
    "    print(f\"\\nüîß FIXES NEEDED: Address {total_checks - passed_checks} issues before full modeling\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0983f8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üîÑ CROSS-VALIDATION TESTING & VALIDATION\n",
      "==================================================\n",
      "üîç TESTING CROSS-VALIDATION IMPLEMENTATION:\n",
      "\n",
      "   Testing CV with Ridge Regression...\n",
      "      Manual Fold 1: 30.829% MAPE\n",
      "      Manual Fold 2: 30.618% MAPE\n",
      "      Manual Fold 3: 30.697% MAPE\n",
      "\n",
      "   üìä CV Results Comparison:\n",
      "      Manual CV:  30.715% ¬± 0.087%\n",
      "      Sklearn CV: 30.715% ¬± 0.087%\n",
      "      ‚úÖ CV implementations consistent\n",
      "\n",
      "üöÄ GRADIENT BOOSTING CV TESTING:\n",
      "\n",
      "   Testing LightGBM CV...\n",
      "      LightGBM CV: 30.677% ¬± 0.676%\n",
      "      Individual folds: ['30.865%', '31.395%', '29.770%']\n",
      "      ‚úÖ LightGBM CV results look reasonable\n",
      "\n",
      "   Testing XGBoost CV...\n",
      "      XGBoost CV: 31.762% ¬± 0.909%\n",
      "      Individual folds: ['30.876%', '31.400%', '33.011%']\n",
      "      ‚úÖ XGBoost CV results look reasonable\n",
      "\n",
      "   Testing CatBoost CV...\n",
      "      CatBoost CV: 30.942% ¬± 1.516%\n",
      "      Individual folds: ['33.014%', '30.384%', '29.428%']\n",
      "      ‚úÖ CatBoost CV results look reasonable\n",
      "\n",
      "==================================================\n",
      "üéØ OPTUNA OPTIMIZATION TESTING\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-05 20:58:08,623] A new study created in memory with name: no-name-0e82c52e-8989-44c2-9c50-14a7cc801fca\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Optuna available for hyperparameter optimization\n",
      "\n",
      "üîç TESTING OPTUNA OPTIMIZATION:\n",
      "   Running 5 Optuna trials for testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-05 20:58:09,133] Trial 0 finished with value: 31.08011963517963 and parameters: {'n_estimators': 106, 'learning_rate': 0.19063571821788408, 'num_leaves': 76}. Best is trial 0 with value: 31.08011963517963.\n",
      "[I 2025-09-05 20:58:09,359] Trial 1 finished with value: 29.93761089844685 and parameters: {'n_estimators': 140, 'learning_rate': 0.039643541684062936, 'num_leaves': 24}. Best is trial 1 with value: 29.93761089844685.\n",
      "[I 2025-09-05 20:58:09,593] Trial 2 finished with value: 30.35769204127513 and parameters: {'n_estimators': 58, 'learning_rate': 0.1745734676972377, 'num_leaves': 64}. Best is trial 1 with value: 29.93761089844685.\n",
      "[I 2025-09-05 20:58:10,248] Trial 3 finished with value: 29.9210115270211 and parameters: {'n_estimators': 156, 'learning_rate': 0.013911053916202464, 'num_leaves': 98}. Best is trial 3 with value: 29.9210115270211.\n",
      "[I 2025-09-05 20:58:10,533] Trial 4 finished with value: 29.73229978552449 and parameters: {'n_estimators': 175, 'learning_rate': 0.05034443102887247, 'num_leaves': 26}. Best is trial 4 with value: 29.73229978552449.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Optuna test completed\n",
      "      Best MAPE: 29.732%\n",
      "      Best params: {'n_estimators': 175, 'learning_rate': 0.05034443102887247, 'num_leaves': 26}\n",
      "      Trials completed: 5\n",
      "      ‚úÖ Optimization working - 1.35% MAPE range\n",
      "\n",
      "üìä CV STABILITY ANALYSIS:\n",
      "\n",
      "   CV Stability by Model:\n",
      "      LightGBM:\n",
      "         Mean: 30.677%\n",
      "         Std:  0.676%\n",
      "         CV:   0.022\n",
      "         ‚úÖ Very stable\n",
      "      XGBoost:\n",
      "         Mean: 31.762%\n",
      "         Std:  0.909%\n",
      "         CV:   0.029\n",
      "         ‚úÖ Very stable\n",
      "      CatBoost:\n",
      "         Mean: 30.942%\n",
      "         Std:  1.516%\n",
      "         CV:   0.049\n",
      "         ‚úÖ Very stable\n",
      "\n",
      "==================================================\n",
      "üöÄ OPTIMIZATION READINESS ASSESSMENT\n",
      "==================================================\n",
      "üìã OPTIMIZATION READINESS CHECKLIST:\n",
      "=============================================\n",
      "   ‚úÖ CV Implementation\n",
      "   ‚úÖ Gradient Boosting CV\n",
      "   ‚úÖ Optuna Available\n",
      "   ‚úÖ Optuna Working\n",
      "   ‚úÖ Model Diversity\n",
      "   ‚úÖ Adequate Data Size\n",
      "\n",
      "üìä OPTIMIZATION READINESS: 100.0% (6/6 checks passed)\n",
      "üéâ EXCELLENT - Ready for full hyperparameter optimization\n",
      "\n",
      "üí° OPTIMIZATION RECOMMENDATIONS:\n",
      "   ‚Ä¢ Start with LightGBM or XGBoost optimization\n",
      "   ‚Ä¢ Use 50-100 Optuna trials for initial runs\n",
      "   ‚Ä¢ Focus on n_estimators, learning_rate, max_depth parameters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 10. CROSS-VALIDATION TESTING & VALIDATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üîÑ CROSS-VALIDATION TESTING & VALIDATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def test_cv_implementation():\n",
    "    \"\"\"Test and debug cross-validation implementation\"\"\"\n",
    "    print(\"üîç TESTING CROSS-VALIDATION IMPLEMENTATION:\")\n",
    "    \n",
    "    from sklearn.model_selection import KFold, cross_val_score\n",
    "    \n",
    "    # Test with a simple model first\n",
    "    if ridge_ok:\n",
    "        print(\"\\n   Testing CV with Ridge Regression...\")\n",
    "        \n",
    "        # Method 1: Manual CV (like your current implementation)\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        manual_cv_scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "            X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "            \n",
    "            # Create FRESH model for each fold\n",
    "            fold_model = Ridge(alpha=1.0, random_state=42)\n",
    "            fold_model.fit(X_fold_train, y_fold_train)\n",
    "            y_fold_pred = fold_model.predict(X_fold_val)\n",
    "            \n",
    "            fold_mape = calculate_mape(y_fold_val, y_fold_pred)\n",
    "            manual_cv_scores.append(fold_mape)\n",
    "            print(f\"      Manual Fold {fold+1}: {fold_mape:.3f}% MAPE\")\n",
    "        \n",
    "        manual_cv_mean = np.mean(manual_cv_scores)\n",
    "        manual_cv_std = np.std(manual_cv_scores)\n",
    "        \n",
    "        # Method 2: sklearn cross_val_score\n",
    "        def mape_scorer(estimator, X_val, y_val):\n",
    "            y_pred = estimator.predict(X_val)\n",
    "            return -calculate_mape(y_val, y_pred)  # Negative because sklearn maximizes\n",
    "        \n",
    "        sklearn_model = Ridge(alpha=1.0, random_state=42)\n",
    "        sklearn_cv_scores = cross_val_score(\n",
    "            sklearn_model, X_train, y_train, \n",
    "            cv=KFold(n_splits=3, shuffle=True, random_state=42),\n",
    "            scoring=mape_scorer\n",
    "        )\n",
    "        sklearn_cv_scores = -sklearn_cv_scores  # Convert back to positive MAPE\n",
    "        \n",
    "        sklearn_cv_mean = np.mean(sklearn_cv_scores)\n",
    "        sklearn_cv_std = np.std(sklearn_cv_scores)\n",
    "        \n",
    "        print(f\"\\n   üìä CV Results Comparison:\")\n",
    "        print(f\"      Manual CV:  {manual_cv_mean:.3f}% ¬± {manual_cv_std:.3f}%\")\n",
    "        print(f\"      Sklearn CV: {sklearn_cv_mean:.3f}% ¬± {sklearn_cv_std:.3f}%\")\n",
    "        \n",
    "        cv_difference = abs(manual_cv_mean - sklearn_cv_mean)\n",
    "        if cv_difference < 0.1:\n",
    "            print(\"      ‚úÖ CV implementations consistent\")\n",
    "        else:\n",
    "            print(f\"      ‚ö†Ô∏è CV difference: {cv_difference:.3f}% - check implementation\")\n",
    "        \n",
    "        return manual_cv_mean, sklearn_cv_mean\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Run CV testing\n",
    "manual_cv, sklearn_cv = test_cv_implementation()\n",
    "\n",
    "def test_gradient_boosting_cv():\n",
    "    \"\"\"Test CV with gradient boosting models\"\"\"\n",
    "    print(\"\\nüöÄ GRADIENT BOOSTING CV TESTING:\")\n",
    "    \n",
    "    cv_results = {}\n",
    "    \n",
    "    # Test each available gradient boosting model\n",
    "    models_to_test = []\n",
    "    if lgb_ok:\n",
    "        models_to_test.append((\"LightGBM\", lgb.LGBMRegressor(n_estimators=50, random_state=42, verbose=-1)))\n",
    "    if xgb_ok:\n",
    "        models_to_test.append((\"XGBoost\", xgb.XGBRegressor(n_estimators=50, random_state=42, verbosity=0)))\n",
    "    if cb_ok:\n",
    "        models_to_test.append((\"CatBoost\", cb.CatBoostRegressor(iterations=50, random_seed=42, verbose=False, allow_writing_files=False)))\n",
    "    \n",
    "    for model_name, model in models_to_test:\n",
    "        print(f\"\\n   Testing {model_name} CV...\")\n",
    "        \n",
    "        try:\n",
    "            # Use smaller dataset for speed\n",
    "            X_small = X_train[:1000]\n",
    "            y_small = y_train[:1000]\n",
    "            \n",
    "            # Test our CV function (fixed version)\n",
    "            def fixed_cv_test(model, X, y, cv_folds=3):\n",
    "                kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "                cv_scores = []\n",
    "                \n",
    "                for train_idx, val_idx in kf.split(X):\n",
    "                    X_fold_train, X_fold_val = X[train_idx], X[val_idx]\n",
    "                    y_fold_train, y_fold_val = y[train_idx], y[val_idx]\n",
    "                    \n",
    "                    # Create fresh model for each fold\n",
    "                    if hasattr(model, 'get_params'):\n",
    "                        fold_model = type(model)(**model.get_params())\n",
    "                    else:\n",
    "                        from sklearn.base import clone\n",
    "                        fold_model = clone(model)\n",
    "                    \n",
    "                    fold_model.fit(X_fold_train, y_fold_train)\n",
    "                    y_pred = fold_model.predict(X_fold_val)\n",
    "                    cv_scores.append(calculate_mape(y_fold_val, y_pred))\n",
    "                \n",
    "                return cv_scores\n",
    "            \n",
    "            cv_scores = fixed_cv_test(model, X_small, y_small)\n",
    "            cv_mean = np.mean(cv_scores)\n",
    "            cv_std = np.std(cv_scores)\n",
    "            \n",
    "            print(f\"      {model_name} CV: {cv_mean:.3f}% ¬± {cv_std:.3f}%\")\n",
    "            print(f\"      Individual folds: {[f'{score:.3f}%' for score in cv_scores]}\")\n",
    "            \n",
    "            # Check for reasonable results\n",
    "            if cv_mean < 50 and cv_std < 10:\n",
    "                print(f\"      ‚úÖ {model_name} CV results look reasonable\")\n",
    "            else:\n",
    "                print(f\"      ‚ö†Ô∏è {model_name} CV results may be problematic\")\n",
    "            \n",
    "            cv_results[model_name] = {\n",
    "                'mean': cv_mean,\n",
    "                'std': cv_std,\n",
    "                'scores': cv_scores\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ùå {model_name} CV failed: {str(e)[:100]}\")\n",
    "    \n",
    "    return cv_results\n",
    "\n",
    "# Test gradient boosting CV\n",
    "gb_cv_results = test_gradient_boosting_cv()\n",
    "\n",
    "# ==============================================================================\n",
    "# 11. OPTUNA OPTIMIZATION TESTING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéØ OPTUNA OPTIMIZATION TESTING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    print(\"‚úÖ Optuna available for hyperparameter optimization\")\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"‚ùå Optuna not available - install with: pip install optuna\")\n",
    "\n",
    "def test_optuna_optimization():\n",
    "    \"\"\"Test Optuna optimization with a simple model\"\"\"\n",
    "    if not OPTUNA_AVAILABLE:\n",
    "        print(\"‚ö†Ô∏è Skipping Optuna test - library not available\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\nüîç TESTING OPTUNA OPTIMIZATION:\")\n",
    "    \n",
    "    # Use small dataset for speed\n",
    "    X_small = X_train[:2000]\n",
    "    y_small = y_train[:2000]\n",
    "    X_val_small = X_val[:500]\n",
    "    y_val_small = y_val[:500]\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Test with LightGBM if available, otherwise Random Forest\n",
    "        if lgb_ok:\n",
    "            model = lgb.LGBMRegressor(\n",
    "                n_estimators=trial.suggest_int('n_estimators', 50, 200),\n",
    "                learning_rate=trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "                num_leaves=trial.suggest_int('num_leaves', 10, 100),\n",
    "                random_state=42,\n",
    "                verbose=-1\n",
    "            )\n",
    "        elif rf_ok:\n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=trial.suggest_int('n_estimators', 50, 200),\n",
    "                max_depth=trial.suggest_int('max_depth', 5, 20),\n",
    "                min_samples_split=trial.suggest_int('min_samples_split', 2, 10),\n",
    "                random_state=42,\n",
    "                n_jobs=1  # Single job for speed\n",
    "            )\n",
    "        else:\n",
    "            return float('inf')  # No suitable model available\n",
    "        \n",
    "        try:\n",
    "            model.fit(X_small, y_small)\n",
    "            y_pred = model.predict(X_val_small)\n",
    "            mape = calculate_mape(y_val_small, y_pred)\n",
    "            return mape\n",
    "        except Exception:\n",
    "            return float('inf')\n",
    "    \n",
    "    try:\n",
    "        # Create study with minimal trials for testing\n",
    "        study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        \n",
    "        print(\"   Running 5 Optuna trials for testing...\")\n",
    "        study.optimize(objective, n_trials=5, timeout=60, show_progress_bar=False)\n",
    "        \n",
    "        print(f\"   ‚úÖ Optuna test completed\")\n",
    "        print(f\"      Best MAPE: {study.best_value:.3f}%\")\n",
    "        print(f\"      Best params: {study.best_params}\")\n",
    "        print(f\"      Trials completed: {len(study.trials)}\")\n",
    "        \n",
    "        # Check if optimization is working\n",
    "        trial_values = [trial.value for trial in study.trials if trial.value is not None]\n",
    "        if len(trial_values) > 1:\n",
    "            improvement = max(trial_values) - min(trial_values)\n",
    "            if improvement > 1.0:  # At least 1% MAPE improvement\n",
    "                print(f\"      ‚úÖ Optimization working - {improvement:.2f}% MAPE range\")\n",
    "            else:\n",
    "                print(f\"      ‚ö†Ô∏è Limited optimization range - {improvement:.2f}% MAPE range\")\n",
    "        \n",
    "        return study\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Optuna test failed: {str(e)[:100]}\")\n",
    "        return None\n",
    "\n",
    "# Test Optuna\n",
    "optuna_study = test_optuna_optimization()\n",
    "\n",
    "def analyze_cv_stability():\n",
    "    \"\"\"Analyze CV stability across different models\"\"\"\n",
    "    print(\"\\nüìä CV STABILITY ANALYSIS:\")\n",
    "    \n",
    "    if gb_cv_results:\n",
    "        print(\"\\n   CV Stability by Model:\")\n",
    "        for model_name, results in gb_cv_results.items():\n",
    "            cv_coefficient = results['std'] / results['mean'] if results['mean'] > 0 else float('inf')\n",
    "            \n",
    "            print(f\"      {model_name}:\")\n",
    "            print(f\"         Mean: {results['mean']:.3f}%\")\n",
    "            print(f\"         Std:  {results['std']:.3f}%\")\n",
    "            print(f\"         CV:   {cv_coefficient:.3f}\")\n",
    "            \n",
    "            if cv_coefficient < 0.1:\n",
    "                print(f\"         ‚úÖ Very stable\")\n",
    "            elif cv_coefficient < 0.2:\n",
    "                print(f\"         ‚úÖ Stable\") \n",
    "            elif cv_coefficient < 0.3:\n",
    "                print(f\"         ‚ö†Ô∏è Moderate stability\")\n",
    "            else:\n",
    "                print(f\"         ‚ùå Unstable - high variance\")\n",
    "\n",
    "analyze_cv_stability()\n",
    "\n",
    "# ==============================================================================\n",
    "# 12. OPTIMIZATION READINESS ASSESSMENT\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ OPTIMIZATION READINESS ASSESSMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "optimization_readiness = []\n",
    "\n",
    "# CV Implementation Check\n",
    "if manual_cv is not None and sklearn_cv is not None:\n",
    "    cv_consistent = abs(manual_cv - sklearn_cv) < 0.5\n",
    "    optimization_readiness.append((\"CV Implementation\", cv_consistent))\n",
    "else:\n",
    "    optimization_readiness.append((\"CV Implementation\", False))\n",
    "\n",
    "# Gradient Boosting CV Check\n",
    "gb_cv_working = len(gb_cv_results) > 0 and all(r['mean'] < 50 for r in gb_cv_results.values())\n",
    "optimization_readiness.append((\"Gradient Boosting CV\", gb_cv_working))\n",
    "\n",
    "# Optuna Availability\n",
    "optimization_readiness.append((\"Optuna Available\", OPTUNA_AVAILABLE))\n",
    "\n",
    "# Optuna Functionality\n",
    "optuna_working = optuna_study is not None and len(optuna_study.trials) > 0\n",
    "optimization_readiness.append((\"Optuna Working\", optuna_working))\n",
    "\n",
    "# Model Diversity\n",
    "diverse_models = sum([lgb_ok, xgb_ok, cb_ok, rf_ok]) >= 3\n",
    "optimization_readiness.append((\"Model Diversity\", diverse_models))\n",
    "\n",
    "# Data Size Adequacy\n",
    "adequate_data = X_train.shape[0] > 5000 and X_val.shape[0] > 1000\n",
    "optimization_readiness.append((\"Adequate Data Size\", adequate_data))\n",
    "\n",
    "print(\"üìã OPTIMIZATION READINESS CHECKLIST:\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "opt_passed = 0\n",
    "opt_total = len(optimization_readiness)\n",
    "\n",
    "for check_name, status in optimization_readiness:\n",
    "    status_symbol = \"‚úÖ\" if status else \"‚ùå\"\n",
    "    print(f\"   {status_symbol} {check_name}\")\n",
    "    if status:\n",
    "        opt_passed += 1\n",
    "\n",
    "opt_readiness_score = (opt_passed / opt_total) * 100\n",
    "print(f\"\\nüìä OPTIMIZATION READINESS: {opt_readiness_score:.1f}% ({opt_passed}/{opt_total} checks passed)\")\n",
    "\n",
    "if opt_readiness_score >= 80:\n",
    "    print(\"üéâ EXCELLENT - Ready for full hyperparameter optimization\")\n",
    "elif opt_readiness_score >= 60:\n",
    "    print(\"‚úÖ GOOD - Ready for optimization with minor limitations\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è LIMITED - Address key issues before optimization\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nüí° OPTIMIZATION RECOMMENDATIONS:\")\n",
    "\n",
    "if not OPTUNA_AVAILABLE:\n",
    "    print(\"   ‚Ä¢ Install Optuna: pip install optuna\")\n",
    "\n",
    "if not gb_cv_working:\n",
    "    print(\"   ‚Ä¢ Fix gradient boosting CV implementation\")\n",
    "\n",
    "if not cv_consistent:\n",
    "    print(\"   ‚Ä¢ Debug CV implementation - results inconsistent\")\n",
    "\n",
    "if not diverse_models:\n",
    "    print(\"   ‚Ä¢ Install more gradient boosting libraries for better optimization\")\n",
    "\n",
    "if opt_readiness_score >= 60:\n",
    "    print(\"   ‚Ä¢ Start with LightGBM or XGBoost optimization\")\n",
    "    print(\"   ‚Ä¢ Use 50-100 Optuna trials for initial runs\")\n",
    "    print(\"   ‚Ä¢ Focus on n_estimators, learning_rate, max_depth parameters\")\n",
    "\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
